% !TEX root = ../thesis.tex
\label[ch1]
\chap Introduction
%==============================================================

Section \ref[ch2] Research contains information about similar tasks as well as usage history of Convolutional Neural Networks.

Section \ref[ch3] Task describes what we are trying to achieve as well as the dataset available for this task. We also comment on the possible ways of enhancing the initial dataset either with imagery data from Google Street View, or with vector representation of neighborhood collected from Open Street Map database.

Section \ref[ch4] Method goes deeper into the methodology of Convolutional Neural Networks and discusses the architecture decisions we have made in designing our models. We also cover the topics of model training and evaluation schemes.

Section \ref[ch5] Implementation explores the actual code solutions and tries to illustrate chosen approaches with pseudocode. We also explore the syntax necessary for model building in framework Keras.

Section \ref[ch6] Results presents measurements and evaluation of each individual approach we employed to enhance the performance of our models. It also shows the final assessment of our model capabilities.

Section \ref[ch7] Discussion comments upon the results of taken experiments.

Section \ref[ch8] Conclusion closes the topic with final words.

\label[ch2]
\chap Research
%==============================================================

\label[ch2_planning]
\sec Planning
%===================


There are many applications for the task to search for the shortest route on graph representation of map. We can abstract many real world scenario problems into this representation and then use many already existing algorithms commonly used for this class of tasks.

\midinsert
\clabel[1_abstracted_map]{Abstracted representation of map}
\picw=13cm \cinspic figures/2.1._abstracted_representation_of_map.pdf 
\caption/f Abstracted representation of map, description of real world location via set of nodes and edges. This representation allows us to run generic algorithms over it.
\endinsert

We have a set of nodes N, which can be understood as places on map and edges E, which are the possible paths from one node to another, In order to have measurement of quality of traveling between two nodes, we also need a cost function. This cost function will assign a positive value c $\in$ \realnumbers + to each edge. Typically we are facing the task of finding the shortest path, in which we are minimizing the aggregated cost. See illustration of Figure \ref[1_abstracted_map].

In real life scenario, road segments exhibit many different parameters which influence how fast we can traverse them. More or less objective criteria such as surface material, size of the road, time of the day, or criteria which depends solely on the preference of driver. What is the surrounding environment, what is the comfort level of the road. 

This tasks gets more interesting, when we look at more complicated examples, where the cost function is multi-criterial. In such case we need more data at our disposal and we can also  expect the model to be more computationally difficult. For practical use, we need to use effective algorithm with speed up heuristics (see ~\cite[hrnvcivr2016practical]). Great part of research is also in the area of hardware efficient algorithms, which would work on maps containing continent-sized datasets of nodes and edges, and yet coming up with solution in realistic time. We can expect such task on hand-held devices of car gps.

\label[ch2_data_collection]
\secc Data collection
%------------------

Cost function can be very simple, but in order that it works on real life scenarios, we usually need more complicated one with lots of data recorded. Estimation of how much “cost” we associate with one street (represented by edge e $\in$ E connecting two crossroads represented by nodes) should reflect how much time we spend in crossing it.

In case of planning for cars we generally just want to get across as fast as possible, or to cover minimizing distance. When the user is driving a bicycle, more factors become relevant. We need to know the quality of terrain, steepness of the road, amount of traffic in the area and overall pleasantness of the road. In many cases the bikers will not follow the strictly shortest path, choosing their own criteria, such as for example stopping for a rest in a park.

Some of these these criteria are measurable and objectively visible in the real world. For these we need to have highly detailed data available with parameters such as the quality of road and others. Other criteria are based on subjective, personal preference of some routes over other and for these we might need a long period of traces recording which routes have users selected in past. For example the work of [Navigation made personal] makes heavy use of user recorded traces.

See ~\cite[hrnvcivr2016practical] and Figure \ref[2_slowdown_features] for examples of types of measurements we would likely need to estimate cost of each edge considering the slowdown effect of these features.

\midinsert
\clabel[2_slowdown_features]{Measurable features of real world road segment}
\picw=13cm \cinspic figures/2.1.1._example_slowdown_features.pdf 
\caption/f Example of the categories of highly detailed data we would require for multi-criteria cost function formulation as presented by ~\cite[hrnvcivr2016practical]. List of features contributing to a slowdown effect on route segment.
\endinsert

In any case highly qualitative, detailed and annotated dataset is required to begin with and a carefully fitted cost function which would take all these parameters into weighted account is also needed. Large companies are usually protective of their proprietary formulas of evaluating costs for route planners. For example ~\cite[navigation-made-personal] makes use of the road network data of Bing Maps with many parameters related to categories such as speed, delay on segment and turning to another street, however the exact representation remains unpublished.

As we will touch upon this topic in later chapters, its useful to realize that this highly qualitative dataset is not always available. We would like to carry information we can infer from small annotated dataset into different areas, where we lack detailed measurements. We are using visual information of Google Street View images, which is more readily available in certain areas than the highly qualitative dataset.

\label[ch2_history_of_cnn]
\sec History of Convolutional Neural Networks 
%===================

Initial idea to use Convolutional Neural Networks (CNNs) as model was introduced in ~\cite[lecun1998gradient] by LeCun with his LeNet network design trained on the task of handwritten digits recognition.

In this section we try to trace the important steps in the field of Computer Vision which lead to the widespread use of CNNs in current state of the art research.

\secc ImageNet dataset
%------------------

Computer Vision research has experienced a great boost in the work of ~\cite[deng2009imagenet] in the form of image database ImageNet. ImageNet contains full resolution images built into the hierarchical structure of WordNet, database of synsets, “synonym sets” linked into a hierarchy.

WordNet is often used as a resource in the tasks of natural language processing, such as word sense disambiguation, spellcheck and other. ImageNet is a project which tries to populate the entries of WordNet with imagery representation of given synset with accurate and diverse enough images illustrating the object in various poses, viewing points and with changing occlusion.

As the work suggests, with internet and social media, the available data is plenty, but qualitative annotation is not, which is why such hierarchical dataset like ImageNet is needed. The argument for choosing WordNet is that the resulting structure of ImageNet is more diverse than any other related database. The goal is to populate the whole structure of WordNet with 500-1000 high quality images per synset, which would roughly total to 50 million images. Even till today, the ImageNet project is not yet finished, however many following articles already take advantage of this database with great benefits.

Effectively ImageNet became the huge qualitative dataset which was needed to properly teach large CNN models.

\label[ch2_competition]
\secc ImageNet Large Scale Visual Recognition Competition
%------------------

The ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) ~\cite[Russakovsky2014] fueled the path of progress in the field of Image Recognition. While the tasks for each year slightly differs to encourage new novel approaches, it is considered the most prestigious  competition in this field. The victorious strategies, methods and trends of each years of this competition offer a reliable looking glass into the state of art techniques. The success of these works and fast rate of progress has lead to popularization of CNNs into more practical implementations as is the case of Japanese farmer automatizing the sorting procedure of cucumbers ~\cite[japanese-cucumber].

\secc AlexNet, CNN using huge datasets
%------------------

The task of object recognition has been waiting for larger databases with high quality of annotation to move from easier tasks done in relatively controlled environment, such as the MNIST task. In the work of ~\cite[krizhevsky2012imagenet] the ImageNet database was used to teach a Deep Convolutional Neural Network (CNN) in a model later referred to as AlexNet. At the time this method has achieved more than 10\% improvement over its competitors in an ILSVRC 2012 competition.

Given hardware limitations and limited time available for learning, this work made use of only subsets of the ImageNet database used in ILSVRC-2010 and ILSVRC-2012 competition. Choice of CNN as a machine learning model has been made with the reasoning that it behaves almost as well as fully connected neural networks, but the amount of parameters of connections is much smaller and the learning is then more efficient.

For the competition an architecture of five convolutional and three fully-connected layers composed of Rectified Linear Units (ReLUs) as neuron models was used. Other alterations on the CNN architecture were also employed to combat overfitting, to fine-tune and increase score and reflect the limitations of hardware. Output of last fully-connected layer feeds to a softmax layer which produces a distribution over 1000 classes as a result of CNN.

The stochastic gradient descent was used for training the model for roughly 90 cycles through training set composed of 1.2 million of images, which took five to six days to train on two NVIDIA GTX 580 3GB GPUs.

\secc VGG16, VGG19, going deeper
%------------------

Following the successful use of CNNs in ILSVRC2012, the submissions of following years tried to tweak the parameters of CNN architecture. Approach chosen by ~\cite[vgg_simonyan2014very] stands out because of its success. It focused on increasing the depth of CNN while altering the structure of network.  In their convolutional layers they chose to use very small convolution filter (with 3x3 receptive field), which leads to large decrease of amount of parameters generated by each layer.

This allowed them to build much deeper architectures and acquire second place in the classification task and first place in localization task of ILSVRC 2014.

\secc ResNet, recurrent connections and residual learning
%------------------

The work of ~\cite[resnet_He_2016_CVPR] introduced a new framework of deep residual learning which allowed them to go even deeper with their CNN models. They encountered the problem of degradation, where accuracy was in fact decreasing with deeper networks. This issue is not caused by overfitting as the error was increased both in the training and validation dataset.

Alternative architecture of model, where a identity shortcut  connection was introduced between building blocks of the model, allowing it to combat this degradation issue and in fact gain better results with increasing CNN depth.

Their model ResNet 152 using 152 layers achieved a first place in the classification task of ILSVRC 2015.

\secc Ensemble models
%------------------

The state of the art models as of ILSVRC 2016 made use of the ensemble approach. Multiple models are used for the task and final ensemble model weights their contribution into an aggregated score.

The widespread of using the ensemble technique reflects the democratization and emergence of more platforms and public cloud computing solutions giving more processing power to the competing teams of ILSVRC.

\label[ch2_feature_transfer_research]
\secc Feature transfer
%------------------

The success of large CNNs with many parameters trained on large datasets like ImageNet has not only been positive, it also poses a question – will we always need huge datasets like ImageNet to properly teach CNN models? ImageNet has millions of annotated images and it has been gradually growing over time.

Article ~\cite[Oquab_2014_CVPR] talks about this issue and proposes a strategy called feature transfer or model finetuning, which composes of using CNN models trained at one task to be retrained to a different task.

They offer a solution, where similar architecture of CNNs can be taught upon one task and then several of its layers can be reused, effectively transferring the mid-level feature representations to different tasks.

\midinsert
\clabel[3_feature_transfer]{Feature transfer illustrated}
\picw=13cm \cinspic figures/2.2.3._feature_transform.pdf 
\caption/f Illustration of the basic idea of feature transfer between source and target task. Imagine the source task source task being classification task on ImageNet and the target task as a new problem without large dataset at its disposal.
\endinsert

This can be used, when the source task has a rich annotated dataset available (for example ImageNet), whereas the target one doesn’t. When talking about two tasks, the source and target classes might differ, when for example the labeling is different. Furthermore the whole domain of class labels can be also different – for example two datasets of images, where first mostly exhibits single objects and the second one rather contains more objects composed into scenes. This issue is referred to as a “dataset capture bias”.

The issue of different class labels is combated by adding two new adaptation layers which are retrained on the new set of classes. The problems of different positions and distributions of objects in image is addressed by employing a strategy of sliding window decomposition of the original image, continuing with sensible subsamples and finally having the result classifying all objects in the source image separately.

The article also works with a special target dataset Pascal VOC 2012 containing difficult class categories of activities described like “taking photos” or “playing instrument”. In this case the source dataset of ImageNet doesn’t contain labels which could overlap with these activities, yet the result of this “action recognition” task achieves best average precision on this dataset.

This article gives us hope, that we can similarly transfer layers of Deep CNN trained on the ImageNet visual recognition source task to a different target task of Google Street View imagery analysis for cost estimation over each edge segment.

Articles being submitted to the ILSVRC competition often include a section dedicated on using their designed models on different tasks then what they were trained upon. Effectively they should the models suitability for feature transfering.

\maybebreak 5pt

\secc Common structures
%------------------

When designing the architecture of custom CNN model, we are using certain layers and building block schemes established as common practice in the ILSVRC competition. For a new unresearched task it has been suggested (by lecturers and online sources such as ~\cite[cnn_course]) to stick to an established way of designing the overall architecture.

\midinsert
\clabel[4_structure]{Generic CNN architecture formula}
\picw=13cm \cinspic figures/2.2.4.building_blocks_with_blocks.pdf 
\caption/f Formula describing a generic CNN architecture used with image data.
\endinsert

Refer to Figure \ref[4_structure]. For illustration of this recommended architecture. Naturally for custom tasks this architecture is later adapted and tweaked to serve well in its specific situation. We will return to this suggested architecture scheme when building our own custom CNN models in \ref[ch4_building_blocks].

\label[ch3]
\chap The Task
%==============================================================

\sec Route planning for bicycles 
%===================

The task we are faced with consists of planning a route for bicycle on a map of nodes and edges. We are designing an evaluation method, which will give each edge segment appropriate cost. In such a way we are building one part of route planner, which will use our model for cost evaluation and fit into a larger scheme mentioned in \ref[ch2_planning].

As has been stated in \ref[ch2_data_collection] a cost function can be an explicitly defined formula depending on many measured variables. Similar formula has been used by the ATG research group, which produced a partially annotated section of map with scores of bike attractivity. We want to enrich this dataset with additional visual information from Google Street View and with vector data of Open Street Map.

We want to train a model on the small annotated map segment and later use it in areas where such detailed information is not available. We argue that Google Street View and Open Street Map data are more readily obtainable, than supply of highly qualitative measurements.

\label[ch3_available_data]
\sec Available imagery data 
%===================

\label[ch3_initial_dataset]
\secc Initial dataset
%------------------

We are given a dataset from the ATG research group of nodes and edges with score ranking ranging from 0 to 1. Score of 0 denotes, that in simulation this route segment was not used and value 1 means that it was a highly attractive road to take.

\midinsert
\clabel[5_geojson]{Sample of the initial dataset}
\picw=11cm \cinspic figures/3_example_geojson_data.pdf
\caption/f Sample of the structure of initial dataset.
\endinsert


Each node in supplied with longitude, latitude location, which gives us the option to enrich them with additional real world information. Figure \ref[5_geojson]. shows the structure of initial data source.
	

\secc Google Street View
%------------------

As each edge segment connecting two nodes is representing a real world street connecting two crossroads, we can get additional information from the location. We can download one or more images alongside the road and associate it with the edge and it’s score from the initial dataset.

We are using a Google Street View API which allows us to generate links of images at specific locations and facing specific ways.

\label[ch3_downloading_street_view_images]
\secc Downloading Street View images
%------------------

Google Street View API uses the parameters of location which is the latitude and longitude and heading, which is the deviation angle from the North Pole in degrees. See Figure \ref[6_google_api]. In calculation of heading we are making a simplification of Earth being spherical using formula for initial bearing in Figure \ref[6_bearing].

\midinsert
\clabel[6_bearing]{Initial bearing formula}
\picw=10cm \cinspic figures/3.2.2.1._formula_bearing.pdf 
\caption/f Formula for calculation of initial bearing when looking from one location (lon$_{1}$, lat$_{1}$) to another (lon$_{2}$, lat$_{2}$).
\endinsert

\midinsert
\clabel[6_google_api]{Google Street View API url generation}
\picw=10cm \cinspic figures/3.2.2.1._google_api-get_svg_v2.pdf 
\caption/f Illustration of Google Street View API url generation
\endinsert

In order to make good use of the location and collect enough data, we decided to break down longer edges into smaller segments maintaining the minimal edge size fixed. We also select both of the starting and ending locations of each segment. In each position we also rotate around the spot.

We collect total of 6 images from each segment, 3 in each of its corners while rotating 120\degrees degrees around the spot. This allows us to get enough distinct images from each location and we don’t overlap with neighboring edges. See the illustration in Figure 7. Note that all of these images will correspond to one edge and thus to one shared score value.

Also note that we are limited to downloading images of maximal size 640x640 pixels as per the limitation of free use of Google Street View API.

\midinsert
\clabel[7_edge_breakdown]{Edge splitting and image generation scheme}
\picw=13cm \cinspic figures/3.2.2.1._breakdown_of_edge.pdf 
\caption/f Splitting of initial possibly large edge segments into sections not smaller than the minimal length limit for edge segment. Each of these generates six images, which are usually not overlapping even with neighboring edges.  
\endinsert

In CNNs we often use the method of data augmentation to extend datasets by simple image transformations to overcome limitations of small datasets. We can generate crops of the original images, flip and rotate them or alter their colors.

We will return to the issue of dataset augmentation in \ref[ch4_data_augmentation].

\label[ch3_neighborhood_in_osm]
\sec Neighborhood data from Open Street Map
%===================

We are looking for another source of information about an edge segment in the neighborhood surrounding its location in Open Street Map data.

Open Street Map data is structured as vector objects placed with location parameters and large array of attributes and values. We can encounter point objects, line objects and polygon objects, which represent points of interest, streets and roads, building, parks and other landmarks.

\midinsert
\clabel[8_ex_params]{OSM data structure with parameters}
\picw=13cm \cinspic figures/3.2.3._osm_map_with_params.pdf 
\caption/f Example of structure of OSM data with parameters showing its properties.
\endinsert

From implementation standpoint, we have downloaded the OSM data covering map of our location and loaded it into a PostgreSQL database. In this way we can send queries for lists of objects in the vicinity of queried location. We will get into more detail about implementation in \ref[ch5_osm_marker].

\label[ch3_osm_vector]
\secc OSM neighborhood vector
%------------------

The structure of OSM data consists of geometrical objects with attributes describing their properties. In the PostgreSQL database each row represents object and attributes are kept as table columns.

Depending on the object type, different attributes will have sensible values while the rest will be empty. For better understanding consult table in Figure \ref[8_osm_interesting_attributes] with examples of attributes and their values and table in Figure \ref[8_osm_objects]. for examples of objects in OSM dataset.

\midinsert
\clabel[8_osm_interesting_attributes]{Sample of OSM attributes}
\picw=13cm \cinspic figures/3.2.3.1_table_possible_values.pdf 
\caption/f Sample of interesting attributes and their possible values in OSM dataset.
\endinsert

For example attribute “highway” will be empty for most objects, unless they are representing roads, in which case it reflects its size. We will be interested in counting attribute-value pairs, for example the number of residential roads in the area, which will have “highway=residential”.

\midinsert
\clabel[8_osm_objects]{Sample of OSM objects}
\picw=9cm \cinspic figures/3.2.3.1_(semi)_table_objects_with_values.pdf 
\caption/f Example of values given to a selection of objects from OSM dataset.
\endinsert

Out of these pairs, we can build a vector of their occurrences. The only remaining issue is to determine which attribute-value pairs we select into our vector. If we used every pair possible, the vector would be rather large and more importantly mostly filled with zeros.

In order to select which pairs are important, we chose to look at OSM data statistics available at webpage taginfo.openstreetmap.org. From an ordered list of most commonly used attribute-value pairs, we have selected relevant pairs and generated our own list of pairs which we consider important.

\midinsert
\clabel[9_ex_loc]{Unique locations with neighborhood vectors}
\picw=15cm \cinspic figures/3.2.3.1_locations_with_table.pdf 
\caption/f Four unique locations with their corresponding neighborhood vectors. Note that these can be very similar for close enough locations.
\endinsert

Then for each distinct location of edge segment, we look into its neighborhood and count the number of occurrences of each pair from the list. See Figure \ref[10_vec_count].

\midinsert
\clabel[10_vec_count]{Construction of neighborhood vector}
\picw=11cm \cinspic figures/3.2.3.1_construction_of_osm_vec.pdf 
\caption/f The construction of neighborhood vector from collection of nearby objects.
\endinsert

Each distinct location of each edge will end up with same sized OSM vector marking their neighborhood. Note that due to the method of downloading multiple images per one edge, for example by using the same location and rotating around the spot, some of these will have the same OSM vectors.

\midinsert
\clabel[10b_six_imgs]{Generation of data entries from edge segment}
\picw=10cm \cinspic figures/3.2.3.1_6images_and_their_shared_data.pdf 
\caption/f Example of further not divided edge segment, which generates data entries, where certain values overlap. For example the score is shared among all images produced from one segment.
\endinsert

One further undivided edge segment contains 6 images which share the same score, and some of which will share location and therefore also their OSM vectors. For illustration see Figure \ref[10b_six_imgs].

\secc Radius choice
%------------------

Depending on the radius we choose different area will be considered as neighborhood. If we were to choose too small radius, the occurrences would mostly result in zero OSM vector. On the other hand selecting too high radius would lead many OSM vectors to be indistinguishable from each other as they would share the exact same values.

Our final choice for radius was value of 100 meters. %, which was empirically tested in \ref[ch6] Results.

\secc Data transformation
%------------------

Similar to the spirit of data augmentation for images, we can try editing the OSM vectors in order that they will be more easily used by CNN models.

Instead of raw data of occurrences, we can convert this information into one-hot categorical representations or reduce them into Boolean values. Multiple readings of varying radius size can also be used for better insight of the neighborhood area.

See more about data augmentation in \ref[ch4_data_augmentation].

\label[ch4]
\chap The Method
%==============================================================

Our method will rely upon using Convolutional Neural Networks (CNNs) mentioned in \ref[ch2_history_of_cnn] on an annotated dataset described in \ref[ch3]. As we have enriched our original dataset with multiple types of data, particularly imagery Street View data and the neighborhood vectors, we have an option to build more or less sophisticated models, depending on which data will they be using. We can build a model which uses only relatively simple OSM data, or big dataset of images, or finally the combination of both.

Depending on which data we choose to use, different model architecture will be selected. Furthermore we can slightly modify each of these models to tweak its performance.

\label[ch4_building_blocks]
\sec Building blocks 
%===================

Regardless of the model type or purpose, there are certain construction blocks, which are repeated in the architecture used by most CNN models.

\label[ch4_model_abstraction]
\secc Model abstraction
%------------------

When building a CNN model, we can observe an abstracted view of such model in terms of its design. Whereas at the input side of the model we want to extract general features from the dataset, at the output side we stride for a clear classification of the image.

\midinsert
\clabel[11_extractor_class]{Feature extractor and classificator}
\picw=13cm \cinspic figures/4.1.1_model_abstraction.pdf 
\caption/f Abstraction of CNN model into feature extractor section and classificator section.
\endinsert

Each of these segments will require different sets of building blocks and will prioritize different behavior. Good model design will lead to generalization of concrete task-specific data into general features, which will then be again converted into concrete categories or scores. The deeper the generic abstraction is, the better the model behavior in terms of overfitting will be.

Classification segment transforms the internal feature representation back into the realm of concrete data related to our task. In our tasks we are interested in score in range from 0 to 1 as illustrated by Figure \ref[12_sigmoid_output].

\midinsert
\clabel[12_sigmoid_output]{Classificator section formula}
\picw=13cm \cinspic figures/4.1.1._output_formula.pdf 
\caption/f Classificator section formula describing how we generate one value at the end of CNN model ranging from 0 to 1 as a score estimate.
\endinsert

\label[ch4_fully_connected_layers]
\secc Fully-connected layers
%------------------

The fully-connected layer denoted as “Dense” in Keras stands for a structure of neurons connected with every input and output by weighed connections. In Neural Networks these are named as hidden layers.

\midinsert
\clabel[13_fc]{Fully connected layer}
\picw=13cm \cinspic figures/4.1.2._fully-connected.pdf 
\caption/f Shows the model of connections of neurons in fully connected layer.
\endinsert

The fully-connected layer suffers from a large amount of parameters it generates: weights in each connection between neurons and biases in individual neuron units. Fully-connected layers are usually present in the classification section of the model.

\secc Convolutional layers
%------------------

\midinsert
\clabel[14_conv]{Convolutional layer}
\picw=10cm \cinspic figures/4.1.3_convolutional_layer.pdf 
\caption/f Schematic illustration of connectivity of convolutional layer. Connections are limited to the scope of receptive field.
\endinsert

Convolutional layers are trying to circumvent the large amount of parameters of fully-connected layers by localized connectivity. Each neuron looks only at certain area of the previous layer.

For their property to use considerably less parameters while at the same time to focus on features present in particular sections of image, they are often used as the main workforce in the feature extractor section of CNN models.

\secc Pooling layers
%------------------

Pooling layers are put in between convolutional layers in order to decrease the size of data effectively by downsampling the volume. This forces the model to reduce its number of parameters and to generalize better over the data. Pooling layers can apply different functions while they are downsamplig the data – max, average, or some other type of normalization function.

\midinsert
\clabel[15_pool]{Pooling layer}
\picw=10cm \cinspic figures/4.1.4._pooling_layer.pdf 
\caption/f Pooling layer structure effectively downsampling the volume of input data.
\endinsert

\label[ch4_dropout_layers]
\secc Dropout layers
%------------------

Dropout layer is special layer suggested by ~\cite[srivastava2014dropout] which has since been widely used on the design of CNN architectures as a tool to prevent model overfitting.

\midinsert
\clabel[16_drop]{Dropout layer}
\picw=8cm \cinspic figures/4.1.5._dropout.pdf 
\caption/f Illustration of dropout layers effect during the training period, which renders certain connection invalid with set probability. This prevents the network to depend too much on overly complicated formations of neurons.
\endinsert

The dropout layer placed between two fully-connected layers functions randomly drops connections between neurons with certain probability during the training period. Instead of fully-connected network of connections we are left with a thinned network with only some of the connections remaining. This thinned networks is used during training and prevents neurons to rely too much on co-adaptation. They are instead forced to develop more ways to fit the data as there is the effect of connection dropping. During test evaluation, the full model is used with its weights changed by the probability of dropout probability.


\label[osm_model]
\sec Open Street Map neighborhood vector model 
%===================

In this version of model, we broke down edge segments formerly representing streets in real world into regularly sized sections each containing two locations of its beginning and ending location. These locations were enriched with OSM neighborhood vectors in \ref[ch3_neighborhood_in_osm].
	
\midinsert
\clabel[17_one_edge_to_many_loc]{Distinct locations from edge segment}
\picw=12cm \cinspic figures/4.2._edge_progress.pdf 
\caption/f Edge segment broken down into set of distinct locations, where each of these locations is assigned its own neighborhood vector.
\endinsert

Single unit of data is therefore a neighborhood vector linked to each distinct location of the original dataset. We have designed a model which takes these vectors as inputs and scores as outputs.

The OSM vector model is built from repeated building blocks of fully-connected layer followed by dropout layer. Fully connected layer of width 1 with sigmoid activation function is used as the final classification segment. Figure \ref[18_osm_model] shows the model alongside its dimensions.

\midinsert
\clabel[18_osm_model]{OSM neighborhood vector CNN model}
\picw=15cm \cinspic figures/4.2._model.pdf 
\caption/f CNN model making use of only the neighborhood OSM vector data.
\endinsert

\label[img_model]
\sec Street View images model 
%===================

Each edge segment is represented by multiple images captured via the Street View API. Images generated from the same edge segment will share the same score label, however the individual images will differ. We can understand one image-score pair as a single unit of data.

The image data can be augmented in order to achieve richer dataset, see \ref[ch4_data_augmentation].

As discussed in \ref[ch2_history_of_cnn] we are using a CNN model which has been trained on ImageNet dataset. We reuse parts of the original model keeping its weights and attach a new custom classification segment architecture at the top of the model.

\midinsert
\clabel[19_base_top]{Reused base model with custom top model}
\picw=12cm \cinspic figures/4.3._feature_transfer.pdf 
\caption/f Reusing part of already trained CNN model as a base model and adding our custom top model.
\endinsert

We can generally divide even the more complicated CNN models into two abstract segments as mentioned in \ref[ch4_model_abstraction]. The beginning of the model, which usually comprises of repeated structure of convolutional layers, followed by a classification section usually made of fully-connected layers.

The former works in extracting high dimensional features of incoming imagery data, whereas the classification section translates those features into a probability distribution over categories or score. In our case we are considering a regression problem model, which works with score instead of categories.

As was mentioned in \ref[ch2_feature_transfer_research] we reuse the model trained on large dataset of ImageNet, separate it from its classificatory and instead provide our own custom made top model. See Figure \ref[19_base_top].

We prevent the layers of base model from changing their weights and train only the newly attached top model for the new task. There are certain specifics connected with this approach which we will explore in \ref[ch4_feature_cooking] section.

\label[ch4_model_architecture]
\secc Model architecture
%------------------

The final model architecture is determined by two major choices: which CNN to choose as its base model and how to design the custom top model so it’s able to transfer the base models features to our task.

\secc{Base model}
%------------------

The framework we are working with, Keras, allows us to simply load many of the successful CNN models and by empirical experiments asses, which one is best suited for our task. More about Keras in the appropriate section \ref[ch4_keras].

The output of what remains of the base CNN model is data in feature space. The dimension will vary depending on the type of model we choose, the size of images we feed the base models and also the depth in which we chose to cut the base CNN model.

The remains of base CNN model are followed by a Flatten layer which converts the possibly multidimensional feature data into a one dimensional vector, which we can feed into the custom top model.

\midinsert
\clabel[20_diff_in_diff_out]{Dimensionality of feature vectors}
\picw=14cm \cinspic figures/4.3.1.1_different_sized_imgs.pdf 
\caption/f Example of differently sized images on the input which result in different feature vector size.
\endinsert

\label[ch4_custom_top]
\secc{Custom top model}
%------------------

We feed the feature vector into a custom model built of repeated blocks of fully-connected neuron layers interlaced with dropout layers. The number of neurons used in each of the layers influences the so called model “width” and the number of used layers influences the model “depth”. Both of these attributes influence the amount of parameters of our model. We can try various combinations of these parameters to explore the models optimal shape.

The final layer of the classification section consists of fully-connected layer of width 1 with sigmoid activation function which weighs in all neurons of the previous layer. See Figure \ref[21_custom_top]. Note that in the final model we chose to interlace individual fully-connected layers with dropout layers.

\midinsert
\clabel[21_custom_top]{Top model structure}
\picw=14cm \cinspic figures/4.3.1.2_feature-score.pdf 
\caption/f Top model structure which takes in the feature vector and follows with fully connected layers which are retrained on our own task.
\endinsert

\maybebreak 5pt

\secc{The final architecture}
%------------------

The final architecture composes of base CNN model with its weights trained on the ImageNet dataset and of custom classification top model trained to fit the base model for our task.

\midinsert
\clabel[22_img_model]{Image model structure}
\picw=15cm \cinspic figures/4.3.1.3._img_model.pdf 
\caption/f Final schematic representation of CNN model using Google Street View images. Note that parameters of width and depth can change its performance.
\endinsert

\label[mix_model]
\sec Mixed model
%===================

After discussing the architecture of two models making only a partial use of the data collected in our dataset, we would like to propose a model combining the two previous ones.

In this case one segment again generates multiple images, which share the same score and depending on how they are created they could also share the same neighborhood OSM vector representing the occurrences of interesting structures in its proximity. Different edges will generate not only different images, but also different scores and OSM vectors.

\midinsert
\clabel[23_single_data_unit_for_mix]{Structure of mixed model input data}
\picw=14.5cm \cinspic figures/4.4._mixed_data_repetition.pdf 
\caption/f Example of possible decomposition of original edge into rows of data accepted by the mixed model. We use both the image data and the OSM neighborhood vectors.
\endinsert

As a single unit of data we can consider the triplet of image, OSM vector and score. It’s useful to note that later in designing the evaluation method of models, we should take into account, that the neighborhood vector and score can be repeated across data. When splitting the dataset into training and validation sets, we should be careful and place images from one edge into only one of these sets. Otherwise data with distinct images, but possibly the same neighborhood vector and score could end up in both of these sets. Figure \ref[23_single_data_unit_for_mix] illustrates how single data units are generated from one edge.

We can join the architectures designed in previous steps, or we can design a new model. We chose to join the models in their classification segment. We propose a basic idea for a simple model architecture, which concatenates feature vectors obtained in previous models and follows with structure of repeated fully-connected layers with dropout layers in between. Concatenation joins the two differently sized one dimensional vectors into one. As is observable on Figure \ref[24_mix_model] we use several parameters to describe the models width and depth.

\midinsert
\clabel[24_mix_model]{Mixed model structure}
\picw=15cm \cinspic figures/4.4._mixed_model_v2.pdf 
\caption/f Final architecture of the mixed model, which uses both OSM vector and the image data.
\endinsert


\label[ch4_data_augmentation]
\sec Data Augmentation 
%===================

When using complicated models with high number of parameters on relatively small datasets, the danger of overfitting is always present. We would like to combat this by expanding our dataset with the help of data augmentation.

Overfitting occurs when the model is basically able to remember all the samples of the training dataset perfectly and incorporate them into its structure. It achieves very low error on the training data, but looses its ability to generalize and results in comparably worse results on the validation set.

The idea of data augmentation is to transform the data we have in order to get more samples and a model which in turn behaves better on more generalized cases.

This cannot be done just blindly, as some of these transformations could mislead our model (for example left to right vertical flip makes sense in our case, but a up side down horizontal flip wouldn't).

There are multiple ways we can approach the problem of generating as many images from our initial dataset as we can. Before getting to the data augmentation aspect, please note, that this is also the reason, why we are generating multiple images per segment. We stand in two corners of each edge segment and rotate 120\degrees degrees to get three images on each side. We have also employed a technique, which splits long edges into as many small segments as possible, while not hitting the self imposed minimal edge length.

It could be debated, that we could rotate for smaller angle or split edges to even smaller segments in order to take advantage of the initial dataset fully. However we came across an issue, that with too small minimal edge length or with different rotation scheme, we obtain very similar images, which actually do not improve the overall performance. This occurs when we don't generate differing enough images during downloading. For actual performance change see chapter \ref[ch6].

We face similar issue when choosing a radius for obtaining OSM neighborhood vector as specified in the section \ref[ch3_neighborhood_in_osm]. Instead of selecting one particular radius setting, we can make use of results of multiple queries. When building the OSM vector we would effectively multiply its length by concatenating it with other versions of OSM vectors. We could concatenate the vectors acquired with one fixed radius setting with another version with different radius. See Figure \ref[25_osm_multiple_radius_aug] for illustration.

\midinsert
\clabel[25_osm_multiple_radius_aug]{Multiple OSM vector use}
\picw=14cm \cinspic figures/4.5._data_augmentation_osm.pdf 
\caption/f Illustration of the construction of combined OSM vector and the expected result of more specific area targetting by the model.
\endinsert

	Finally we also come across the method of data augmentation by transformation of the original image dataset. Certain operation, such as vertically flipping the image make sense for our dataset. (Also the normalization of color palete can be used in our task.)
	We show an example of images undergoing such transformation on Figure \ref[26_image_augmentation_flips]. Note that in this particular example we chose vertical flipping alongside with clipping of 90\% of the image while making up for the lost 10\%. These operations are random and the resulting images are added to create a larger dataset. For the sake of repetition of experiments with the same data, we save these generated images into an expanded dataset.

\midinsert
\clabel[26_image_augmentation_flips]{Image data augmentation}
\picw=10cm \cinspic figures/4.5._data_augmentation_folder.jpg
\caption/f Data augmentation example.
\endinsert

\label[ch4_model_training]
\sec Model Training 
%===================

\secc Data Split 
%------------------

Traditionally we split our dataset into two sets – training set, which we use for training of model and so called validation set, which is used only for models performance evaluation. As we discuss in \ref[ch4_model_evaluation], we employ more complex strategy of k-fold cross validation test to obtain more precise results.

The difference between the error achieved on training data and on validation data can be used as a measure of our model overfitting.

\secc Training setting 
%------------------

We are using backpropagation algorithm to train our models as is supported by the selected Keras framework. We can choose from a selection of optimizers, which control the learning process. We made use of the more automatic optimizers supported by Keras, such as "rmsprop" ~\cite[rmsprop_tieleman2012lecture] and "adam" (see ~\cite[adam_optimizer] where Adam has been tested as effective on CNN learning related tasks). For greater parametric control we can also select the SGD optimizer.

Given the nature of our task, we are solving a regression problem, trying to minimize deviation from scored data. In most models mentioned in \ref[ch2_competition], the task revolves around selecting the correct category to classify objects.

Accordingly we have to select appropriate loss function. We have selected the mean squared error metric with the formula given by Figure \ref[27_mse_formula].

\midinsert
\clabel[27_mse_formula]{Mean squared error metric}
\picw=8cm \cinspic figures/4.6.2_mse_formula.pdf 
\caption/f Mean squared error metric used as loss function when training models.
\endinsert

\secc Training stages 
%------------------

As has been discussed in \ref[ch4_model_architecture], we are building models by reusing base of other already trained CNN models. In our case we make use of weights loaded from model trained on the ImageNet dataset.

We attach a custom classifier section to base model and try to train it on a new task. In order to preserve the information stored in connections of the base model, we lock its weights and prevent it from retraining. The only weight values which are changing are in the custom top model. This can be understood as a first stage of training the model as illustrated on Figure \ref[28_training_stages].

\midinsert
\clabel[28_training_stages]{Training stages schematics}
\picw=10cm \cinspic figures/4.6.3_training_stages.pdf 
\caption/f Training stages illustrated for feature transfer approach.
\endinsert

This is sometimes followed with a finetunning period, where we unlock certain levels of the base CNN model for training. However this is commonly done with customized optimizer setting, so that the changes to the whole model weights are not too drastic. We are also usually not retraining the whole model, because of the computational load this would take.

\label[ch4_feature_cooking]
\secc Feature cooking 
%------------------

This method is specific to situation when we are training a model with parts, which are frozen, as is in the case of \ref[img_model] image and \ref[mix_model] mixed model design. We can take advantage of the fact that certain section of the model will never change and precompute the image features for a fixed dataset.

In this way we can save computational costs associated with training the model. In the end the original model can be rebuilt by loading obtained weight values.

This allows for fast prototyping of the custom top model, even if the whole model composes of many parameters in the frozen base model. For illustration see figure \ref[29_reusing_features].

\midinsert
\clabel[29_reusing_features]{Reusing saved image features from a file}
\picw=14cm \cinspic figures/4.6.4_feature_cooking.pdf 
\caption/f Reusing saved image features from a file instead of costly computations.
\endinsert

\label[ch4_model_evaluation]
\sec Model evaluation 
%===================

As was mentioned in \ref[ch4_model_training] we are using the practice of splitting dataset into training and validation dataset, with the k-fold cross validation technique.

In order to prevent from being influenced by the selection bias, we split our entire dataset into k folds and then in sequence we use these to build training datasets and validation datasets. Every fold will take role of validation set for a model, which will train on data composed from all the remaining folds. Each of these will run a full training ended by an evaluation giving us score. Eventually we can calculate the average score with standard deviation.

This approach obviously increases the computational requirements, because it repeats the whole experiment for each fold. It is not used while prototyping models, but as a reliable method to later generate score. We have chosen the number of folds to be 10, as is a traditionally recommended approach.

\midinsert
\clabel[30_k_fold]{K-fold cross validation}
\picw=11cm \cinspic figures/4.7._k_fold_cross_validation.pdf 
\caption/f Splitting schema employed in the k-fold cross validation. We are given average score alongside with its standard deviation.
\endinsert

\sec Frameworks and projects 
%===================

The existence of this thesis depends on couple of frameworks which helped it greatly. Framework called “Keras” allowed for fast prototyping of Deep CNN models as well as efficient training environment. Thanks to the Metacentrum project we had access to machines with high enough computational power needed to teach these models.

\label[ch4_keras]
\secc Keras
%------------------

Keras is a framework which supports fast prototyping of models with easy API to underlaying backend using low level Theano and TensorFlow. We can build entire CNN or Machine Learning models by listing individual layers of the model from prebuilt list of commands. For most of models mentioned in \ref[ch2_history_of_cnn] which performed well in the ILSVRC competition there exists a model description in Keras. Models, which we will show how to load in Keras syntax in Code \ref[code13_applications], have weights available from learning on the large ImageNet dataset. This makes Keras perfect for our task, even for the feature transfer design we intend to use for our models.

Keras also has a training and testing API prepared to use models efficiently with good hardware support including computation on GPUs.

\secc Metacentrum project
%------------------

Metacentrum is a project supporting research groups including the academic research groups at our university. They provide a server cluster capable of running our experiments on wide array of machines with powerful hardware. We control the individual runs of experiments by special tasking and job queuing system, PBS Professional. We also make use of its storage units with image datasets containing many items and large feature files.

See \ref[ch5_metacentrum] and Code \ref[code18_metacentrum_bash_task] for examples of scripts we use to queue jobs and experiments.

\label[ch5]
\chap The Implementation
%==============================================================

\sec Project overview 
%===================

In planning of the composition of project code, we have somewhat separated the sections responsible for downloading data, from those managing the dataset and modeling and running experiments.

Downloader is tasked to download images from Google Street View API from the initial dataset of edges and nodes mentioned in \ref[ch3_initial_dataset]. Segment object works as a unit holding information about edge and its corresponding images and score. See \ref[ch5_downloader].

However for later working with data, we have created a DatasetHandler which contains all the necessary functions. See \ref[ch5_dataset_handler].

To run more instances of models and later evaluate them, we have chosen to build individual experiments from custom written setting files in \ref[ch5_settings]. Experiment Runner provides the common framework for all these more complicated computations in \ref[ch5_experiment_runner]. In Settings folder we hold setting files defining each experiment.

\midinsert
\clabel[31_overview]{Project structure overview}
\picw=11cm \cinspic figures/5.1._project_overview.pdf 
\caption/f Project structure in a schema.
\endinsert

\label[ch5_downloader]
\sec Downloader functionality 
%===================

The Downloader is the main method of acquiring imagery data and preparation of dataset. It first creates necessary folders according to the directory path and custom name we provide it with.

"Defaults.py" contains default settings for the downloader, such as default number of times the code should try downloading each image and internal codes with which to mark unsuccessfully downloaded segments. Besides these internal representation settings, it also controls the pixel size of downloaded images.

\midinsert
\clabel[code1_downloader]{RunDownload}
\picw=13cm \cinspic pseudocodes/5.3._downloader.pdf
\caption/t RunDownload code sample.
\endinsert

Downloading procedure is initiated in "RunDownload()" method (see Code \ref[code1_downloader]), which parses node and edge data in provided GeoJSON files and build an array of Segments.

For each segment in this array, we generate list of images to download. Long segments will be split by interpolating the start and end location to create smaller edge sections (see Code \ref[code3_longedges]). We generate a url link in format corresponding to Google Street View API discussed in Code \ref[code2_googleapi] alongside with unique  file name.

\midinsert
\clabel[code2_googleapi]{Google Street View API url}
\begtt
http://maps.googleapis.com/maps/api/streetview?size=600x400
   &location=<lat>,<long>&heading=<angle from north>&key=<api>
\endtt
\caption/t Exact Google Street View API url we need to generate
\endinsert

\midinsert
\clabel[code3_longedges]{Break down long edges}
\picw=13cm \cinspic pseudocodes/5.3._break_down_long_edge.pdf
\caption/t Code segment which breaks down long edges and generates lists of urls and filenames Segment.getGoogleViewUrls().
\endinsert

One by one we try to download each of these images, marking segments with error flag, if we were unable to access them. The code attempts to retry each request for default number of times, to prevent temporary instability of network to hinder the downloading process.

We also check for invalid images in areas where Google Street View doesn’t have any data available (see Figure \ref[32_noimagery]).

\midinsert
\clabel[32_noimagery]{No imagery available on Google Street View}
\picw=6cm \cinspic figures/sorry_no_imagery_here.jpg 
\caption/f Location outside of streets with available photographic imagery.
\endinsert


Eventually we save the pickled array into file for further processing. Folder structure is as follows in Figure \ref[code4_folder_struct]

\midinsert
\clabel[code4_folder_struct]{Dataset folder structure}
\begtt
dataset folder: 5556x_example_dataset_299px/
SegmentsData.dump
images/0000_0.jpg
images/0000_1.jpg
…
images/5546_5.jpg
\endtt
\caption/t Folder structure of dataset.
\endinsert

Function "RunCheck()" in \ref[code5_downloader_check] allows us to load previously downloaded Segments file and check for erroneous data - for example in case of sudden network failure we can download only the missing files and then save the fixed Segments file.

\midinsert
\clabel[code5_downloader_check]{Check downloaded dataset}
\picw=13cm \cinspic pseudocodes/5.3._downloader_check.pdf
\caption/t RunCheck code sample checks the downloaded dataset and redownloads missing images.
\endinsert

\label[ch5_osm_marker]
\sec OSM Marker 
%===================

Our code works with data downloaded from Open Street Maps to estimate what is the neighborhood of each segment. We have downloaded the .osm data file of corresponding location and exported it into a PostgreSQL database following commands represented in Code \ref[code6_convert_osm_to_db].

\midinsert
\clabel[code6_convert_osm_to_db]{Loading data into PostgreSQL database}
\picw=13cm \cinspic pseudocodes/5.4._osm_db_import.pdf
\caption/t Loading data into PostgreSQL database.
\endinsert

	Having a  PostgreSQL database is advantageous, because it allows us sending repeated queries to database of objects with geographical location while using PostgreSQL macros and functions for getting distance and intersections between areas. PostgreSQL database has hierarchical structure of data storage which is necessary for fast data access. Our python code can access this database with simple queries and then process its responses.

\midinsert
\clabel[code7_osm_marker]{Marking data with OSM vector}
\picw=13cm \cinspic pseudocodes/5.4._osm_marker.pdf
\caption/t Marking data with OSM vector.
\endinsert

The code \ref[code7_osm_marker] loads array of Segments and one by one accesses all the distinct locations stored in each Segment. This could be only two locations for segments too short to be broken down, or more if the original segment was split. Minimally these are the starting and the ending locations of the segment.

We generate a SQL command, which targets particular columns which we chose to observe and also filters the data with "WHERE" clause for objects in distance smaller than chosen radius from segments location.

OSM data we imported into PostgreSQL database takes structure of four tables representing "point", "line", "polyline" and "road" objects. We look into each of these tables and combine the results.

Result of query gives us list of rows, each representing one object in vicinity of our location and columns containing attributes. We can produce a “"attribute=value"” pair from the values in each row alongside with the count of their occurrences.

Final neighborhood vector counts all these occurrences in fixed positions as was discussed in \ref[ch3_osm_vector].

Each segment stores multiple neighborhood vectors, one for each distinct location. Marked array of Segments is saved into pickled ".dump" file. We can store multiple versions of these files, each with different radius setting, while reusing all the downloaded images.

\label[ch5_dataset_augmentation]
\sec Dataset Augmentation
%===================

\midinsert
\clabel[code7b_data_aug]{Data augmentation with ImageDataGenerator}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[bbb]{aaa}
      \picw=7cm \cinspic pseudocodes/5.x._augmentation_expand.pdf
      %\caption/f aaaa
   \par}\vtop{%
      %\clabel[ccc]{ddd}
      \picw=7cm \cinspic pseudocodes/5.x._augmentation_aggresive.pdf 
      %\caption/f bbbb
   \par}}
\centerline {Expansion scheme\hfil\hfil Aggresive Expansion scheme}\nobreak\medskip
\caption/f Data augmentation with the Keras ImageDataGenerator syntax.
\endinsert

We made use of Keras inbuilt ability of ImageDataGenerator which can be controlled with syntax visible \ref[code7b_data_aug]. We present two schemes of data augmentation first where we allow for flipping and image shifting in limited amount of 10\% of the whole image. 

A more aggressive augmentation scheme is suggested for the second case, where we allow for flipping, shifting up to 20\%, rescale of 10\% and even shear and rotation of the image by 10\degrees degrees.

For each image in our original dataset we create a fixed amount of images with random transformations allowed from this augmentation scheme. For practical reasons of limiting the size of our dataset we chose to generate 2 new images (effectively tripling the size of original dataset). For reproducibility of the results we stored the expanded dataset into their own folders reusing the same randomly generated data.

\label[ch5_dataset_handler]
\sec Datasets and DatasetHandler 
%===================

"DatasetObject" is a structure shielding us from low level manipulation with the segments stored in ".dump" file, which can then remain unchanged and be reused for many experiments.
	
Figure \ref[33_dataset_object] shows us the structure of "DatasetObject", with its most important functions and variables exposed.

\midinsert
\clabel[33_dataset_object]{Dataset object}
\picw=12cm \cinspic figures/5.5._dataset_object.pdf 
\caption/f Dataset object with important functions highlighted.
\endinsert

Dataset is initialized by reading a "SegmentsData.dump" file, however it stores data in its own four internal arrays. These are: list of urls to images, labels marking the scores, OSM vectors if we have loaded a marked Segments file and unique IDs of original segments. Note that we are not directly loading all the images yet, rather we are keeping only their filenames. We can access the images when necessary, saving us from wasteful memory allocation.

We also provide multiple getter functions which split data into training and validation datasets for the purpose of testing experiments. Each model type will require access to different data – image only model will for example omit all the OSM neighborhood vectors.

\midinsert
\clabel[code8_dataset_handler]{DatasetHandler}
\picw=13cm \cinspic pseudocodes/5.5._dataset_handler.pdf
\caption/t Initialization of Dataset in DatasetHandler from saved segments file.
\endinsert

\midinsert
\clabel[code9_dataset_visualization]{Data visualization}
\picw=13cm \cinspic pseudocodes/5.5._dataset_statistics.pdf
\caption/t Functions for data statistics visualization.
\endinsert

%\midinsert
%\clabel[33_dataset_visualization]{Dataset visualization}
%\picw=6cm \cinspic figures/nullfig.pdf 
%\caption/f Example of dataset visualization.
%\endinsert

%\secc Dataset statistics
%\secc DatasetHandler structure

\sec ModelHandler 
%===================

"ModelHandler" is composed of three functional sections. "ModelOI" provides the functions handling input and output – including interaction with "DatasetHandler" and reporting functions. "ModelGenerator" is responsible for building models in Keras syntax depending on settings we choose for current model. "ModelTester" provides functions controlling the training and testing capabilities of model.

These individual blocks are controlled from "ExperimentRunner", which ties them together and manages shared Settings information.


\secc Model description in Keras
%------------------

Generic code for building models in Keras works in two modes – Sequencial and Functional. We will use Functional in these particular examples as it is convenient both for writing and understanding.

We will mention some of the layers we can use to build Keras models.

\begtt
Out = Dense(256, activation=’relu’)(In)
Out = Dense(1, activation=’sigmoid’)(In)
\endtt
	
Dense layers stand for fully connected layers mentioned in \ref[ch4_fully_connected_layers]. Here we can see two possible uses for this type of layer. With width set to "1", this layer represents one neuron which can be used as the final output of our classifier.

\begtt
Out = Dropout(probability)(In)
\endtt

Dropout layer is used accompanying the Dense layers and follows the process mentioned in \ref[ch4_dropout_layers]. With selected probability the connection of neurons will be dropped during the training period.

\midinsert
\clabel[code10_build_generic_model]{Building generic model in Keras}
\picw=13cm \cinspic pseudocodes/5.6._build_generic_model.pdf
\caption/t Building generic model in Keras.
\endinsert

%We are using these mentioned layers in our models, for more detailed survey check \ref[APPENDIX KERAS SYNTAX] APPENDIX KERAS SYNTAX.

After building the model we need to compile it specifying settings which will be used for training such as the optimizer, loss function and additional measured metrics.

\begtt
Model.compile()
\endtt


For details about which settings we use for training, check \ref[ch4_model_training].

Finally our model is trained on dataset with fit command.
	
\begtt
model.fit(data, labels, ...)
\endtt

The full code to build and train a generic Keras model on a dataset can be seen in Figure \ref[code11_run_generic_experiment].

\midinsert
\clabel[code11_run_generic_experiment]{Fit generic model to data}
\picw=13cm \cinspic pseudocodes/5.6._run_generic_experiment.pdf
\caption/t Fit generic model to data in Keras.
\endinsert

\secc ModelHandler Structure
%------------------

Figure \ref[34_model_handler_structure] illustrates the various functions  which are part of the "ModelHandler". In \ref[code16_model_handler_functions] we can see snippets of code dealing with building and testing of models. Note that in one run we are handling multiple instances of models and datasets, as is explained in \ref[ch5_experiment_runner].

\midinsert
\clabel[34_model_handler_structure]{ModelHandler Structure}
\picw=13cm \cinspic figures/5.6.3._model_handler_struct.pdf 
\caption/f Illustration of "ModelHandler" structure using its three functional parts - "ModelOI", "ModelGenerator" and "ModelTester".
\endinsert	
	
\midinsert
\clabel[code16_model_handler_functions]{ModelHandler functions}
\picw=13cm \cinspic pseudocodes/5.6._model_handler_functions.pdf
\caption/t "ModelHandler" functions. One experiment run interacts with multiple models each with their individual settings and dataset. Here we can see the pseudocode of model generation and training - we process one by one.
\endinsert


\sec Models
%===================

Particular models described in section \ref[ch4] are shown here with functions which generate them. We have stated that their structure can be parametrized, and that we can alter these parameters to change their shape and performance. We can change the depth and width of these models.
	
\secc OSM only model
%------------------

See Code sample \ref[code12_osm_model], which builds the model discussed in \ref[osm_model]. As input we are using the OSM neighborhood data. Number of fully-connected Dense layers as well as their width is variable.

\midinsert
\clabel[code12_osm_model]{OSM only model code}
\picw=13cm \cinspic pseudocodes/5.6._osm_model.pdf
\caption/t Function producing a Keras model described in \ref[osm_model].
\endinsert

\secc Images only model
%------------------

When we are working with model described in \ref[img_model], we make use of the feature transfer method and as such the training process can be made easier as was discussed in \ref[ch4_feature_cooking].

Also the Keras syntax will be correspondingly altered. We create two models instead of a single whole model. One represents the feature extractor section of model and is mostly created by the reused base of pretrained CNN model. We can load these as base of our models with weights initialized from model trained on ImageNet dataset, such as is done in \ref[code13_applications].

\midinsert
\clabel[code13_applications]{Keras base models}
\picw=13cm \cinspic pseudocodes/5.6._applications.pdf
\caption/t Available CNN models in Keras for loading as base models.
\endinsert

Our second model is then made of just our custom top architecture discussed in \ref[ch4_custom_top]. We will feed it with outputs of features from the first model. Note that for repeated experiments with the same dataset and model setting, we can save these features into files and spare a lot of valuable computation time. Code snippet in \ref[code14_feature_cooking_img_model] shows us an example of the use of two models.

\midinsert
\clabel[code14_feature_cooking_img_model]{Feature cooking}
\picw=13cm \cinspic pseudocodes/5.6._feature_cooking_img_model.pdf
\caption/t Images only model using the feature cooking method.
\endinsert

\secc Mixed model
%------------------

Mixed model uses the same techniques of feature transfer and is fully described in \ref[mix_model].

Code in \ref[code15_mix_model] shows how we build this model. Note that we can even reuse the saved feature files from image only model, as long as we maintain the same order in our dataset. "DatasetHandler" can shuffle data in deterministic manner which allows us to do so.

\midinsert
\clabel[code15_mix_model]{Building mixed model}
\picw=13cm \cinspic pseudocodes/5.6._mix_model.pdf
\caption/t Building mixed model.
\endinsert



\label[ch5_settings]
\sec Settings structure 
%===================

Throughout the whole run of experiments, we keep a shared Setting structure in form of python dictionary. Each experiment we run first loads default settings values and then it alters them depending on the experiment description file.

Each experiment has Settings defined for the whole run and also a list in "Settings[“models”]" further specifying multiple models per experiment. Each of these can be using different dataset, or it can reuse already loaded one without wasting resources.

Finally the reasoning behind experiments is to join these individual model runs and have an easy method of linking their results together while maintaining order in what are we testing. We can graph the learning progress of defined models into same plot for easier visualization.

\midinsert
\clabel[code16_default_settings]{Default Settings initialization}
\picw=13cm \cinspic pseudocodes/5.7._default_settings.pdf
\caption/t Default Settings initialization.
\endinsert

Settings is initialized from default values, build as a python object dictionary as seen in Code \ref[code16_default_settings]. Then a custom Setup function is called which specifies differences from the default settings. This allows for descriptions of new experiments to be relatively small and to reuse already written code. This also allows for very small settings description files specifying either simple problems, while also being able to design complex experiments going into great detail. See Code \ref[code16_minimal_settings_file] for example of a minimal experiment specification.

\midinsert
\clabel[code16_minimal_settings_file]{Minimal Settings description file}
\picw=13cm \cinspic pseudocodes/5.7._minimal_settings_file.pdf
\caption/t Minimal settings experiment definition file specifying a simple experiment with custom dataset and number of epochs.
\endinsert

\midinsert
\clabel[34_settings_syntax]{Settings syntax}
\picw=12cm \cinspic figures/5.7._settings_syntax.pdf 
\caption/f Settings syntax.
\endinsert

Following are tables of important values of Settings syntax. Note that we write these following syntax shown in Figure \ref[34_settings_syntax]. Figure \ref[ch5_settings_table_whole] contains table of parameters which are applied over the whole experiment run. All other parameters are model specific and are listed in tables of Figures \ref[ch5_settings_table_model] and \ref[ch5_settings_table_model_cont1].



\midinsert{ 
\clabel[ch5_settings_table_whole]{Settings parameters for the whole experiment}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.27\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.2\hsize\noindent\strut #\strut \hfil} 
& \vrule\hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
  parameter name & default value & description \cr
  \noalign{\hrule}
  experiment\_name & basic & Name of the experiment as well as name of the folder used to store the results in. \cr
  \noalign{\hrule}
  graph\_histories & [‘all’, ‘together’] & Accepts values of ‘all’, which produces one graph for each model, ‘together’, which draws graphs of all models into one shared graph. Also accepts lists of numbers, which specify which models we would like to have plotted together. For example [0,2] would plot histories of the first and third model together. \cr
  \noalign{\hrule}
  interrupt & False & Internal flag used to interrupt the whole experiment in case of error. \cr
  \noalign{\hrule}
  filename\_features\_train, filename\_features\_test & ‘’ & Internal string flags used to share file paths amongst different parts of code in ModelHandler. \cr
}
\hrule
}
\vskip5pt
\startcenter
\caption/f List of parameters shared throughout the whole experiment.
\stopcenter
\endinsert


\midinsert{ 
\clabel[ch5_settings_table_model]{Settings parameters for each individual model}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.23\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.2\hsize\noindent\strut #\strut \hfil} 
& \vrule\hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
  parameter name & default value & description \cr
  \noalign{\hrule}
  Dataset specific \cr
  \noalign{\hrule}
dataset\_pointer & -1 & If left at value ‘-1’ we instantiate a new dataset for this model. Otherwise we use the dataset of the indicated different model. For example if the first model has -1, then the second model can have dataset\_pointer set to 0 to reuse the same dataset. \cr
  \noalign{\hrule}
dataset\_name & ‘1200x\_markable \_299x299’ & States the name of folder we want to use for loading the dataset from (this folder has to contain SegmentsData.dump and a folder ‘images’ with Google Street View images). Downloader can prepare these folders for us. \cr
  \noalign{\hrule}
dump\_file\_override & ‘’ & Can be used to indicate usage of nonstandard segments file. For example we can keep multiple versions of OSM marked data in different files and access them via this setting (for example loading file ‘SegmentsData\_marked\_R100.dump’). \cr
  \noalign{\hrule}
pixels & 299 & Pixel dimension indicator of the loaded images (image files have dimension of pixels*pixels*3). \cr
  \noalign{\hrule}
number\_of\_images & None & Indicates if we want to use only a subset of the dataset. When left at None, whole dataset will be used, otherwise a uniform sampling will be used to give us indicated number of images. \cr
  \noalign{\hrule}
seed & 13 & Seed for maintaining deterministic nature of certain random sampling operations (such as the initial reordering of loaded dataset). This value is important as it guarantees our ability to reuse precomputed feature files. \cr
  \noalign{\hrule}
validation\_split & 0.25 & Under which fraction we split data in simple (those not guided by k-fold cross-validation scheme) experiments. Ranges from 0 to 1. With 0.25 one quarter of data will designated as validation set and the remainder of three-quarters will be the training set. \cr
  \noalign{\hrule}
shuffle\_dataset & True & Indicator that we want to shuffle our dataset in deterministic manner (using the seed value). \cr
  \noalign{\hrule}
shuffle\_dataset \_method & 'default-same-segment' & Shuffling method which maintains the order of images from the same segment to be kept together, which is important not to bring in dualities into our data. \cr
  \noalign{\hrule}
}
\hrule
}
\vskip5pt
\startcenter
\caption/f List of parameters specific to individual models.
\stopcenter
\endinsert



\midinsert{ 
\clabel[ch5_settings_table_model_cont1]{Settings parameters for each individual model (continued)}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.22\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.19\hsize\noindent\strut #\strut \hfil} 
& \vrule\hskip2pt \vtop{\hsize=.54\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
  parameter name & default value & description \cr
  \noalign{\hrule}
  Dataset specific \cr
  \noalign{\hrule}
edit\_osm\_vec & ‘’ & Additional transformation of the OSM data, accepts 'booleans', which turns all quantitative entries into 0 or 1. Setting 'low-mid-high' turns all entries into three categorical system of low, mid and high depending on percentiles of data distribution in dataset. These three categories are encoded as one-hot vector producing variants: 001, 010 or 100. Note that this effectively triples the length OSM vector. \cr
  \noalign{\hrule}
  Model specific \cr
  \noalign{\hrule}
unique\_id & & Unique name for this model, will be used for plotting graphs and later for identification which plotted history belonged to which model.\cr
  \noalign{\hrule}
model\_type & 'simple\_cnn\_ with\_top' & Specifies which model type will we use. Accepted values are: 'simple\_cnn\_with\_top', 'img\_osm\_mix', 'osm\_only'. \cr
  \noalign{\hrule}
cnn\_model & 'resnet50' & Which basic CNN model will we use in case of image\_only and mixed model. Allowed values are: 'vgg16', 'vgg16', 'resnet50', 'inception\_v3', 'xception'. \cr
  \noalign{\hrule}
cooking\_method & 'generators' & Internally used flag to indicating which method will we use to generate feature files. 'generators' tends to be less memory demanding, but consumes longer time when compared with the other allowed value 'direct'. \cr
  \noalign{\hrule}
top\_repeat\_FC \_block & 3 & Specifies depth of classifier in image\_only and osm\_only models. \cr
  \noalign{\hrule}
osm\_manual\_width & 256 & Manually setting the width of osm\_only model. \cr
  \noalign{\hrule}
save\_visualization & True & Whether we want to plot graphs for this experiment. \cr
  \noalign{\hrule}
Training specific \cr
  \noalign{\hrule}
epochs & 150 & Indication of how many epochs we want to spend in training of this model. \cr
  \noalign{\hrule}
optimizer & 'rmsprop' & Choice for the Keras optimizer. Suggested values are 'rmsprop' or 'adam', however also accepts the object of Keras optimizer such as customizable  optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9) \cr
  \noalign{\hrule}
loss\_func & 'mean\_squared \_error' & Metric used as a loss function during training of this model. \cr
  \noalign{\hrule}
metrics & ['mean\_absolute \_error'] & List of other metrics which we want to also track into history. \cr
  \noalign{\hrule}
}
\hrule
}
\vskip5pt
\startcenter
\caption/f List of parameters specific to individual models (continued).
\stopcenter
\endinsert


\label[ch5_experiment_runner]
\sec Experiment running 
%===================

A

\label[ch5_training]
\sec Training 
%===================

Basic model functionality is supported by Keras with commands in Code \ref[code17_training]. We specify settings of the training procedure by choosing parameters of optimizer, loss function and additional metrics. We chose optimizers "adam" and "rmsprop", which work well with their default settings, as is discussed in \ref[ch4_model_training].

As a loss function we have selected “mean squared error” with formula in Figure \ref[27_mse_formula]. Keras allows us to specify additional metrics, which can be used to simply track progress, but don’t actually influence the training process.

Finally we are given a history stored in dictionary of arrays with tracked values (both of the loss function and measured metrics). We make use if these histories to plot the progress of training into graphs.
	
\midinsert
\clabel[code17_training]{Model training syntax}
\picw=13cm \cinspic pseudocodes/5.8._training.pdf
\caption/t Model training syntax.
\endinsert

\sec Testing 
%===================

For more relevant results we use the advanced method of k-fold cross-validation. In code we actually use the same syntax as the one of \ref[ch5_training], however with data generation scheme described in \ref[ch4_model_evaluation].

Note that we can use the results of graphed histories even without using the k-fold cross-validation, however this gives us more in depth view.

\sec Reporting and ModelOI 
%===================

"ModelOI" is a programmatic unit responsible for inputs and outputs of the larger experiment runs. We use it to communicate with "DatasetHandler" while loading datasets at the beginning of experiments. We also use methods of "ModelOI" for reporting after the training has ended.

Histories produced during training in "ModelTester" are processed and plotted into graphs via the "Matplotlib" library.

For convenience we also chose to send interesting results via email after the often lengthy computations have finished.

\label[ch5_metacentrum]
\sec Metacentrum scripting 
%===================

Scripts controlling the job queuing on Metacentrum servers are using the PBS Professional job scheduler with couple of commands described in Figure \ref[ch5_metacentrum_scripts].

Note that further customization of commands is available, however these commands were all we needed for our experiments.
	
% Metacentrum scripts table
\midinsert \clabel[ch5_metacentrum_scripts]{Metacentrum scripts}
\ctable{ll}{
"qsub task.sh" & basic specification to run bash program in "task.sh" \cr
"-l walltime=$<$time$>$"   & memory requirements (for example "mem=32gb") \cr
"-l ncpus=$<$n$>$"         & number of CPUs used (for example "ncpus=4") \crl \tskip4pt
% -------------------------------------
"qstat -u $<$user$>$"      & command to get list of tasks run by a specific user \cr
"qstat $<$task\_id$>$"      & get information about task specified by unique  "task\_id"  \crl \tskip4pt
% -------------------------------------
"qdel $<$task\_id$>$" & delete planned job \cr
"qsig -s SIGINT $<$task\_id$>$"   & interruption of running task by the "SIGINT" signal \cr
}
\caption/t Metacentrum planner scripts
\endinsert

Contents of task.sh written in bash are in Code \ref[code18_metacentrum_bash_task]. We first load Anaconda environment and step into the correct directory. Then we run python code which is supplied with path to Settings file and an unique job id. Settings files are described in \ref[ch5_settings] and allow us to customize runs of experiments relatively freely.

\midinsert
\clabel[code18_metacentrum_bash_task]{Metacentrum task code}
\picw=16cm \cinspic pseudocodes/5.11._metacentrum_bash_task.pdf
\caption/t Bash code to run python file with custom targeted Settings file. Note that "$PBS_JOBID" is a unique job id given by the tasking and scheduling software.
\endinsert
%$$

\label[ch6]
\chap Results
%==============================================================

This chapter is divided into several sections. First we will look at the efficiency of various strategies used when generating the dataset in \ref[ch6_dataset] and then when building models in \ref[ch6_model]. %In section \ref[ch6_overall] we will explore the overall performance of our best models on the task.

\sec How to read graphs in this section

We have implemented a k-fold cross-validation discussed in \ref[ch4_model_evaluation] and used it to evaluate performance of the tested model and dataset combination. We will present a higher level view of the experiment by plotting the evolution of error over training epochs. Note that we can follow both the training and validation error to see how much is our model overfitting.

We also provide a more detailed view into the situation after the training was finished. We look at the box plot graphs of errors in the last epoch of training.

Note that in some cases we may want to hold the model with the best achieved validation error, instead of the one from last iteration. This applies in the case when with further training iterations the model overfits. For this view we may plot box plot graphs of the epoch where each model fared the best.

\label[ch6_dataset]
\sec Strategies employed in dataset generation
%===================

In this section we will focus on the results of various attempts to alter the dataset generation scheme. For in-depth explanation of the reasoning why we chose these approaches consult the discussion section \ref[ch7_dataset].

As we described in \ref[ch3_available_data] we have multiple ways to construct our dataset. We have tried varying the size of downloaded images in \ref[ch6_dataset_pixelsize].

We also explore two types of increasing the images available for our models. In \ref[ch6_dataset_longedgesplit] we try splitting long edges to smaller segments, while maintaining a limit of how small can resulting segment be. The smaller the minimal segment length, the higher the number of segments ans consequentially the number of images. See \ref[ch3_downloading_street_view_images] and Code \ref[code3_longedges] for details.

In \ref[ch6_dataset_augmentation] we try alternative approach to dataset augmentation from data already obtained by applying additional transformations such as flipping the image, shifting or scaling. In this way we create new images from already existing set.

\label[ch6_dataset_pixelsize]
\secc Dimensions of downloaded images
%------------------

We chose to try two settings to illustrate the effect of our choice in the matter of image dimensions. Following the original article of ~\cite[inception], we chose our first image size to be 299x299 pixels. Our second dataset is made of images sized 640x640 pixels, which was the limitation of Google Street View as the most detailed image resolution available freely via their API.

Note that the 640x640 dataset is bigger in its size and it also takes more time for the models to be trained on it. Also the consequent feature files which we store in between experiments to allow their reuse are bigger.

\midinsert
\clabel[35_pixel_size_mixed]{Experiment with pixel size, mixed model}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[bbb]{aaa}
      \picw=4.2cm \cinspic graphs/6.1.1._pixel_size/figLeft_boxplotcomp_val_mix.pdf
      %\caption/f aaaa
   \par}\vtop{%
      %\clabel[ccc]{ddd}
      \picw=9cm \cinspic graphs/6.1.1._pixel_size/figRight_graphcomp_evol_mix.pdf 
      %\caption/f bbbb
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f Results of experiment with pixel size of input images. Here we see the graph of evolution of loss function over epochs. K-fold cross-validation scheme was used with k=10. We can see error on validation dataset of all the 10 runs with average results highlighted. Mixed model was used.
\endinsert

We have used datasets internally marked as "5556x_mark_res_299x299" for 299x299 dataset and "5556x_markable_640x640" for the 640x640 dataset. Their only difference is in the image pixel dimensions.

As for the choice of models we don’t include the OSM only model as it is not affected by image pixel size. Figure \ref[35_pixel_size_mixed] shows the results when choosing mixed model (see \ref[mix_model]). Figure \ref[35_pixel_size_image] shows the results when using image only model (see \ref[img_model]).

\midinsert
\clabel[35_pixel_size_image]{Experiment with pixel size, image model}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[bbb]{aaa}
      \picw=4.2cm \cinspic graphs/6.1.1._pixel_size/figLeft_boxplotcomp_val_img.pdf
      %\caption/f aaaa
   \par}\vtop{%
      %\clabel[ccc]{ddd}
      \picw=9cm \cinspic graphs/6.1.1._pixel_size/figRight_graphcomp_evol_img.pdf 
      %\caption/f bbbb
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f Results of experiment with pixel size of input images. Here we see the graph of evolution of loss function over epochs. K-fold cross-validation scheme was used with k=10. We can see error on validation dataset of all the 10 runs with average results highlighted. Image model was used.
\endinsert

We can see both the validation error as well as training error evolution over time of epochs as well as box plot with more detailed view of last epoch.

\label[ch6_dataset_longedgesplit]
\secc Splitting long edges
%------------------


\midinsert{ 
\clabel[ch6_longedge_table]{Dataset overview in experiment with splitting long edges}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.4\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
Dataset & \# of images \cr
  \noalign{\hrule}
Original "5556x_markable_640x640" & 8376 \cr
min length 10m "5556x_minlen10_640px" & 24078 \cr
min length 20m "5556x_minlen20_640px" & 18534 \cr
min length 30m "5556x_minlen30_640px" & 13458 \cr
}
\hrule
}
\vskip5pt
\startcenter
\caption/f Dataset overview for the experiment of splitting long edges into smaller segments. Note that the number of images corresponds with the size of inputs for models.
\stopcenter
\endinsert

See table of Figure \ref[ch6_longedge_table] with a list of tested datasets. When we choose to split long edges, we generate more smaller segments and from each of these segments even more images. Note that we count only valid segments, where there is imagery data available and where we had a label in the initial dataset.

We chose to run the experiment with dataset "5556x_markable_640x640" as comparison of how models fared without long edge splitting.

For this experiment it again makes sense to test on the image only and mixed models as those are the ones affected by imagery data input.

\midinsert
\clabel[36_splitedge_2x2]{Split long edges, overall comparison}
\picw=13cm \cinspic graphs/6.1.2._splitting_long_edges-minlen/fig3_2x2comparison_mix.pdf
\caption/f Overall illustration of the training process of three datasets which were enhanced by long edge splitting and an original dataset for reference. Mixed model.
\endinsert

\midinsert
\clabel[36_splitedge_valerr]{Split long edges, validation error over iterations}
\picw=13cm \cinspic graphs/6.1.2._splitting_long_edges-minlen/fig2_average_valerr_mix.pdf
\caption/f For better clarity we present only the averages of validation errors over the 10 runs of k-fold cross-validation with k=10. Mixed model.
\endinsert

\midinsert
\clabel[36_splitedge_boxplot]{Split long edges, comparison of last epoch between datasets}
\picw=13cm \cinspic graphs/6.1.2._splitting_long_edges-minlen/fig1_boxplot_last_mix.pdf
\caption/f Comparison of the results of splitting long edges into smaller segments. The smaller the minimal edge length was, the more images were generated. Dataset "5556x_markable_640x640" under the label original was used for comparison. Mixed model.
\endinsert

We chose to present the detailed information about the progress of training in Figure \ref[36_splitedge_2x2], which shows all four dataset variants alongside with its validation and training error.

In Figure \ref[36_splitedge_valerr] we see the average validation errors plotted together in order to better compare their performances.

Lastly in \ref[36_splitedge_boxplot] we can see comparison of all four datasets in their last epoch.

Note that we also tested the image only model with the same experiment and the results are placed in \ref[APPENDIXresultsoverflow_splittinglongedges_image model].

\label[ch6_dataset_augmentation]
\secc Dataset augmentation
%------------------

\midinsert{ 
\clabel[ch6_augmentation_table]{Datasets used for augmentation}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.4\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
Dataset & \# of images \cr
  \noalign{\hrule}
Original "5556x_markable_640x640" & 8376 \cr
Expanded & 25128 \cr
Aggresively expanded & 25128 \cr
  \noalign{\hrule}
Original "5556x_minlen30_640px" & 13458 \cr
Expanded & 40374 \cr
Aggresively expanded & 40374 \cr
}
\hrule
}
\vskip5pt
\startcenter
\caption/f List of datasets used for the experiment of data augmentation with the number of valid images they contain.
\stopcenter
\endinsert

As was explained in \ref[code7b_data_aug] we chose two schemes for data augmentation which gives us three datasets to work with as listed by Figure \ref[ch6_augmentation_table].

\midinsert
\clabel[37_augmentation]{TTTTT}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[37_augmentation_boxplots_last]{Dataset augmentation last epoch}
      \picw=4.2cm \cinspic graphs/6.1.3._augmentation-original,expanded,agg_expanded/figLeft_markable.pdf
      %\caption/f Last epoch validation error
   \par}\vtop{%
      %\clabel[37_augmentation_evol]{Dataset augmentation evolution over epochs}
      \picw=9cm \cinspic graphs/6.1.3._augmentation-original,expanded,agg_expanded/figRight_markable.pdf
      %\caption/f Evolution over epochs
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f Comparison of the two augmented datasets with the original referential one. Expanded dataset used the transformations of image flipping and shifting. Aggressively expanded dataset used additional transformations of shear, scale and rotation. Original dataset was "5556x_markable_640x640". 
\endinsert

Figure \ref[37_augmentation] shows the evolution of validation error over the epochs, as well as the validation and training error distribution from k-fold cross-validation in its last epoch.

\label[ch6_model]
\sec Strategies employed in model architecture
%===================

We already could see changing behavior of different model types interacting with differently generated datasets in \ref[ch6_dataset], but here we will focus directly on the changes of model architectures and their influence on performance.

Note that we will mostly use the datasets which showed promising results in \ref[ch6_dataset] ignoring those which lead to decrease of performance.

When using the mixed and image models, we are using an already existing base CNN model for feature transfer (see \ref[img_model] and Figure \ref[19_base_top]), so in the our first experiment in \ref[ch6_model_base] we will explore the possible types of used CNN models.

In \ref[ch6_model_versus] we will look into the comparison of performances of the three designed models – image only, OSM only and mixed model.

In \ref[ch6_model_osm_wd] we will look at a OSM model specific setting of depth and width of the CNN design.

\label[ch6_model_base]
\secc Different CNN base model for feature transfer
%------------------

As was mentioned in Code \ref[code13_applications], Keras allows to use multiple pretrained models alongside with weights initiated from the ImageNet dataset.

FIGURE: Illustration of five models in one graph. For better clarity we chose to show only the average validation error evolution over epochs. These were <mixed> models using different CNNs as their base.

Note that these models have different number of parameters in their structure and therefore the whole evaluation of experiment can take different amount of time and exhibit different memory requirements.

\midinsert{ 
\clabel[ch6_cnns_table]{Comparison of alternate base CNNs}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.4\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
Mixed model with ... & \# Number of parameters \cr
  \noalign{\hrule}
resnet50 & a \cr
vgg16 & a \cr
vgg19 & a \cr
inception v3 & a \cr
xception & a \cr}
\hrule
}
\vskip5pt
\startcenter
\caption/f List of models available in Keras framework with the number of parameters the mixed model using it as its base CNN model will have.
\stopcenter
\endinsert


\label[ch6_model_versus] 
\secc Model competition – image vs. OSM vs. mixed
%------------------

We have proposed three major CNN architecture schemes to build models – OSM only described in \ref[osm_model] and build with Keras by Code \ref[code12_osm_model], image only described in \ref[img_model] and in Code \ref[code14_feature_cooking_img_model] and finally the mixed model described in \ref[mix_model] and build by Code \ref[code15_mix_model].

We chose one dataset to compare these three models on. We use the default setting for building these models.

\midinsert
\clabel[38_modelversus_evol]{Model competition, evolution over epochs}
\picw=13cm \cinspic graphs/6.2.2._model_competition-img-vs-osm-vs-mix/fig1_evolution_markable.pdf
\caption/f Model competition, evolution over epochs. Dataset "5556x_markable_640x640".
\endinsert

\midinsert
\line{\hsize=.5\hsize \vtop{%
      \clabel[38_modelversus_last]{Model competition, last epoch}
      \picw=6cm \cinspic graphs/6.2.2._model_competition-img-vs-osm-vs-mix/fig2_last_epoch_markable.pdf
      \caption/f Model competition, last epoch. Dataset "5556x_markable_640x640".
   \par}\vtop{%
      \clabel[38_modelversus_best]{Model competition, best epoch}
      \picw=6cm \cinspic graphs/6.2.2._model_competition-img-vs-osm-vs-mix/fig3_best_epoch_markable.pdf
      \caption/f Model competition, best epoch. Dataset "5556x_markable_640x640".
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\endinsert

See Figure \ref[38_modelversus_evol] for high level view of evolution of each models performance over time of epochs. For detailed view of models state in the final epoch see \ref[38_modelversus_last]. To be fair for evaluating the models, we also look at Figure \ref[38_modelversus_best] comparing the best performance of each model when just looking at its validation error. Note that the exact epoch when the model performed the best can be different for these three models.

\label[ch6_model_osm_wd]
\secc OSM specific – width and depth
%------------------

This experiment focuses on the performance of OSM model (see \ref[osm_model]) in different settings of its width and depth. See Figure \ref[18_osm_model], where we can see what these two parameters influence. Note that by increasing width, we are setting the same width for all layers and that there are possible unexplored architectures of models with variable width in different depths (imagine for example a model with first layer wide 128 neurons and second and third layer 32 neurons wide).

We are trying values of (1, 2, 3, 4) for depth and (32, 64, 128, 256) for width. We have a grid of 4x4 possible combinations plotted in Figure \ref[39_osm_wd_4x4] with just their validation error in last epoch shown.

\midinsert
\clabel[39_osm_wd_4x4]{OSM model alterations in depth and width}
\picw=10cm \cinspic graphs/6.2.3._OSM_model_GRID_4x4/fig1_4x4madness.pdf
\caption/f OSM model alterations in depth and width.
\endinsert

We chose to display a cut of this table with fixed value of depth to 2 and variable width in Figure \ref[40_osm_wd_d2cut] which follows also their performance over epochs.

\midinsert
\clabel[40_osm_wd_d2cut]{OSM model depth 2 and altering width}
\picw=12cm \cinspic graphs/6.2.3._OSM_model_GRID_4x4/fig2_cutOfD2fixed.pdf
\caption/f OSM model with depth fixed at value 2 and variable width.
\endinsert

For more combinations see section \ref[APPENDIXresultsoverflow_modelosmvariable_wd] in Appendix, where are other cuts (with one attribute fixed while the other one variable) available.

%\label[ch6_overall]
%\sec Detailed results of one model and dataset combination
%===================

%\midinsert
%\clabel[TemporaryGraph]{TemporaryGraph}
%\picw=15cm \cinspic graphs/00temporary__handmade_design_test.pdf 
%\caption/f TemporaryGraph.
%\endinsert	

\label[ch7]
\chap Discussion
%==============================================================

B

\label[ch8]
\chap Conclusions
%==============================================================

V

\endinput
%%
%% End of file `ch00.tex'.
