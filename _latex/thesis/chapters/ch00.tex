% !TEX root = ../thesis.tex
\label[ch1]
\chap Introduction
%==============================================================

Introduction, which chapter contains what.

\label[ch2]
\chap Research
%==============================================================

\label[ch2_planning]
\sec Planning
%===================


There are many applications for the task to search for the shortest route on graph representation of map. We can abstract many real world scenario problems into this representation and then use many already existing algorithms commonly used for this class of tasks.

\midinsert
\clabel[1_abstracted_map]{Abstracted representation of map}
\picw=13cm \cinspic figures/2.1._abstracted_representation_of_map.pdf 
\caption/f Abstracted representation of map, description of real world location via set of nodes and edges. This representation allows us to run generic algorithms over it.
\endinsert

We have a set of nodes N, which can be understood as places on map and edges E, which are the possible paths from one node to another, In order to have measurement of quality of traveling between two nodes, we also need a cost function. This cost function will assign a positive value c $\in$ \realnumbers + to each edge. Typically we are facing the task of finding the shortest path, in which we are minimizing the aggregated cost. See illustration of Figure \ref[1_abstracted_map].

In real life scenario, road segments exhibit many different parameters which influence how fast we can traverse them. More or less objective criteria such as surface material, size of the road, time of the day, or criteria which depends solely on the preference of driver. What is the surrounding environment, what is the comfort level of the road. 

This tasks gets more interesting, when we look at more complicated examples, where the cost function is multi-criterial. In such case we need more data at our disposal and we can also  expect the model to be more computationally difficult. For practical use, we need to use effective algorithm with speed up heuristics (see ~\cite[hrnvcivr2016practical]). Great part of research is also in the area of hardware efficient algorithms, which would work on maps containing continent-sized datasets of nodes and edges, and yet coming up with solution in realistic time. We can expect such task on hand-held devices of car gps.

\label[ch2_data_collection]
\secc Data collection
%------------------

Cost function can be very simple, but in order that it works on real life scenarios, we usually need more complicated one with lots of data recorded. Estimation of how much “cost” we associate with one street (represented by edge e $\in$ E connecting two crossroads represented by nodes) should reflect how much time we spend in crossing it.

In case of planning for cars we generally just want to get across as fast as possible, or to cover minimizing distance. When the user is driving a bicycle, more factors become relevant. We need to know the quality of terrain, steepness of the road, amount of traffic in the area and overall pleasantness of the road. In many cases the bikers will not follow the strictly shortest path, choosing their own criteria, such as for example stopping for a rest in a park.

Some of these these criteria are measurable and objectively visible in the real world. For these we need to have highly detailed data available with parameters such as the quality of road and others. Other criteria are based on subjective, personal preference of some routes over other and for these we might need a long period of traces recording which routes have users selected in past. For example the work of [Navigation made personal] makes heavy use of user recorded traces.

See ~\cite[hrnvcivr2016practical] and Figure \ref[2_slowdown_features] for examples of types of measurements we would likely need to estimate cost of each edge considering the slowdown effect of these features.

\midinsert
\clabel[2_slowdown_features]{Measurable features of real world road segment}
\picw=13cm \cinspic figures/2.1.1._example_slowdown_features.pdf 
\caption/f Example of the categories of highly detailed data we would require for multi-criteria cost function formulation as presented by ~\cite[hrnvcivr2016practical]. List of features contributing to a slowdown effect on route segment.
\endinsert

In any case highly qualitative, detailed and annotated dataset is required to begin with and a carefully fitted cost function which would take all these parameters into weighted account is also needed. Large companies are usually protective of their proprietary formulas of evaluating costs for route planners. For example ~\cite[navigation-made-personal] makes use of the road network data of Bing Maps with many parameters related to categories such as speed, delay on segment and turning to another street, however the exact representation remains unpublished.

As we will touch upon this topic in later chapters, its useful to realize that this highly qualitative dataset is not always available. We would like to carry information we can infer from small annotated dataset into different areas, where we lack detailed measurements. We are using visual information of Google Street View images, which is more readily available in certain areas than the highly qualitative dataset.

\label[ch2_history_of_cnn]
\sec History of Convolutional Neural Networks 
%===================

Initial idea to use Convolutional Neural Networks (CNNs) as model was introduced in ~\cite[lecun1998gradient] by LeCun with his LeNet network design trained on the task of handwritten digits recognition.

In this section we try to trace the important steps in the field of Computer Vision which lead to the widespread use of CNNs in current state of the art research.

\secc ImageNet dataset
%------------------

Computer Vision research has experienced a great boost in the work of ~\cite[deng2009imagenet] in the form of image database ImageNet. ImageNet contains full resolution images built into the hierarchical structure of WordNet, database of synsets, “synonym sets” linked into a hierarchy.

WordNet is often used as a resource in the tasks of natural language processing, such as word sense disambiguation, spellcheck and other. ImageNet is a project which tries to populate the entries of WordNet with imagery representation of given synset with accurate and diverse enough images illustrating the object in various poses, viewing points and with changing occlusion.

As the work suggests, with internet and social media, the available data is plenty, but qualitative annotation is not, which is why such hierarchical dataset like ImageNet is needed. The argument for choosing WordNet is that the resulting structure of ImageNet is more diverse than any other related database. The goal is to populate the whole structure of WordNet with 500-1000 high quality images per synset, which would roughly total to 50 million images. Even till today, the ImageNet project is not yet finished, however many following articles already take advantage of this database with great benefits.

Effectively ImageNet became the huge qualitative dataset which was needed to properly teach large CNN models.

\label[ch2_competition]
\secc ImageNet Large Scale Visual Recognition Competition
%------------------

The ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) ~\cite[Russakovsky2014] fueled the path of progress in the field of Image Recognition. While the tasks for each year slightly differs to encourage new novel approaches, it is considered the most prestigious  competition in this field. The victorious strategies, methods and trends of each years of this competition offer a reliable looking glass into the state of art techniques. The success of these works and fast rate of progress has lead to popularization of CNNs into more practical implementations as is the case of Japanese farmer automatizing the sorting procedure of cucumbers ~\cite[japanese-cucumber].

\seccc{AlexNet, CNN using huge datasets}1
%------------------

The task of object recognition has been waiting for larger databases with high quality of annotation to move from easier tasks done in relatively controlled environment, such as the MNIST task. In the work of ~\cite[krizhevsky2012imagenet] the ImageNet database was used to teach a Deep Convolutional Neural Network (CNN) in a model later referred to as AlexNet. At the time this method has achieved more than 10\% improvement over its competitors in an ILSVRC 2012 competition.

Given hardware limitations and limited time available for learning, this work made use of only subsets of the ImageNet database used in ILSVRC-2010 and ILSVRC-2012 competition. Choice of CNN as a machine learning model has been made with the reasoning that it behaves almost as well as fully connected neural networks, but the amount of parameters of connections is much smaller and the learning is then more efficient.

For the competition an architecture of five convolutional and three fully-connected layers composed of Rectified Linear Units (ReLUs) as neuron models was used. Other alterations on the CNN architecture were also employed to combat overfitting, to fine-tune and increase score and reflect the limitations of hardware. Output of last fully-connected layer feeds to a softmax layer which produces a distribution over 1000 classes as a result of CNN.

The stochastic gradient descent was used for training the model for roughly 90 cycles through training set composed of 1.2 million of images, which took five to six days to train on two NVIDIA GTX 580 3GB GPUs.

\seccc{VGG16, VGG19, going deeper}2
%------------------

Following the successful use of CNNs in ILSVRC2012, the submissions of following years tried to tweak the parameters of CNN architecture. Approach chosen by ~\cite[vgg_simonyan2014very] stands out because of its success. It focused on increasing the depth of CNN while altering the structure of network.  In their convolutional layers they chose to use very small convolution filter (with 3x3 receptive field), which leads to large decrease of amount of parameters generated by each layer.

This allowed them to build much deeper architectures and acquire second place in the classification task and first place in localization task of ILSVRC 2014.

\seccc{ResNet, recurrent connections and residual learning}3
%------------------

The work of ~\cite[resnet_He_2016_CVPR] introduced a new framework of deep residual learning which allowed them to go even deeper with their CNN models. They encountered the problem of degradation, where accuracy was in fact decreasing with deeper networks. This issue is not caused by overfitting as the error was increased both in the training and validation dataset.

Alternative architecture of model, where a identity shortcut  connection was introduced between building blocks of the model, allowing it to combat this degradation issue and in fact gain better results with increasing CNN depth.

Their model ResNet 152 using 152 layers achieved a first place in the classification task of ILSVRC 2015.

\seccc{Ensemble models}4
%------------------

The state of the art models as of ILSVRC 2016 made use of the ensemble approach. Multiple models are used for the task and final ensemble model weights their contribution into an aggregated score.

The widespread of using the ensemble technique reflects the democratization and emergence of more platforms and public cloud computing solutions giving more processing power to the competing teams of ILSVRC.

\label[ch2_feature_transfer_research]
\secc Feature transfer
%------------------

The success of large CNNs with many parameters trained on large datasets like ImageNet has not only been positive, it also poses a question – will we always need huge datasets like ImageNet to properly teach CNN models? ImageNet has millions of annotated images and it has been gradually growing over time.

Article ~\cite[Oquab_2014_CVPR] talks about this issue and proposes a strategy called feature transfer or model finetuning, which composes of using CNN models trained at one task to be retrained to a different task.

They offer a solution, where similar architecture of CNNs can be taught upon one task and then several of its layers can be reused, effectively transferring the mid-level feature representations to different tasks.

This can be used, when the source task has a rich annotated dataset available (for example ImageNet), whereas the target one doesn’t. When talking about two tasks, the source and target classes might differ, when for example the labeling is different. Furthermore the whole domain of class labels can be also different – for example two datasets of images, where first mostly exhibits single objects and the second one rather contains more objects composed into scenes. This issue is referred to as a “dataset capture bias”.

The issue of different class labels is combated by adding two new adaptation layers which are retrained on the new set of classes. The problems of different positions and distributions of objects in image is addressed by employing a strategy of sliding window decomposition of the original image, continuing with sensible subsamples and finally having the result classifying all objects in the source image separately.

The article also works with a special target dataset Pascal VOC 2012 containing difficult class categories of activities described like “taking photos” or “playing instrument”. In this case the source dataset of ImageNet doesn’t contain labels which could overlap with these activities, yet the result of this “action recognition” task achieves best average precision on this dataset.

This article gives us hope, that we can similarly transfer layers of Deep CNN trained on the ImageNet visual recognition source task to a different target task of Google Street View imagery analysis for cost estimation over each edge segment.

Articles being submitted to the ILSVRC competition often include a section dedicated on using their designed models on different tasks then what they were trained upon. Effectively they should the models suitability for feature transfering.

\midinsert
\clabel[3_feature_transfer]{Feature transfer illustrated}
\picw=13cm \cinspic figures/2.2.3._feature_transform.pdf 
\caption/f Illustration of the basic idea of feature transfer between source and target task. Imagine the source task source task being classification task on ImageNet and the target task as a new problem without large dataset at its disposal.
\endinsert

\secc Common structures
%------------------

When designing the architecture of custom CNN model, we are using certain layers and building block schemes established as common practice in the ILSVRC competition. For a new unresearched task it has been suggested (by lecturers and online sources such as ~\cite[cnn_course]) to stick to an established way of designing the overall architecture.

\midinsert
\clabel[4_structure]{Generic CNN architecture formula}
\picw=13cm \cinspic figures/2.2.4.building_blocks_with_blocks.pdf 
\caption/f Formula describing a generic CNN architecture used with image data.
\endinsert

Refer to Figure \ref[4_structure]. For illustration of this recommended architecture. Naturally for custom tasks this architecture is later adapted and tweaked to serve well in its specific situation. We will return to this suggested architecture scheme when building our own custom CNN models in \ref[ch4_building_blocks].

\label[ch3]
\chap The Task
%==============================================================

\sec Route planning for bicycles 
%===================

The task we are faced with consists of planning a route for bicycle on a map of nodes and edges. We are designing an evaluation method, which will give each edge segment appropriate cost. In such a way we are building one part of route planner, which will use our model for cost evaluation and fit into a larger scheme mentioned in \ref[ch2_planning].

As has been stated in \ref[ch2_data_collection] a cost function can be an explicitly defined formula depending on many measured variables. Similar formula has been used by the ATG research group, which produced a partially annotated section of map with scores of bike attractivity. We want to enrich this dataset with additional visual information from Google Street View and with vector data of Open Street Map.

We want to train a model on the small annotated map segment and later use it in areas where such detailed information is not available. We argue that Google Street View and Open Street Map data are more readily obtainable, than supply of highly qualitative measurements.

\sec Available data and collection 
%===================

\label[ch3_initial_dataset]
\secc Initial dataset
%------------------

We are given a dataset from the ATG research group of nodes and edges with score ranking ranging from 0 to 1. Score of 0 denotes, that in simulation this route segment was not used and value 1 means that it was a highly attractive road to take.

Each node in supplied with longitude, latitude location, which gives us the option to enrich them with additional real world information. Figure \ref[5_geojson]. shows the structure of initial data source.
	
\midinsert
\clabel[5_geojson]{Sample of the initial dataset}
\picw=11cm \cinspic figures/3_example_geojson_data.pdf
\caption/f Sample of the structure of initial dataset.
\endinsert

\secc Google Street View
%------------------

As each edge segment connecting two nodes is representing a real world street connecting two crossroads, we can get additional information from the location. We can download one or more images alongside the road and associate it with the edge and it’s score from the initial dataset.

We are using a Google Street View API which allows us to generate links of images at specific locations and facing specific ways.

\seccc{Downloading Street View images}1
%------------------

Google Street View API uses the parameters of location which is the latitude and longitude and heading, which is the deviation angle from the North Pole in degrees. See Figure \ref[6_google_api]. In calculation of heading we are making a simplification of Earth being spherical using formula for initial bearing in Figure \ref[6_bearing].

\midinsert
\clabel[6_bearing]{Initial bearing formula}
\picw=10cm \cinspic figures/3.2.2.1._formula_bearing.pdf 
\caption/f Formula for calculation of initial bearing when looking from one location (lon$_{1}$, lat$_{1}$) to another (lon$_{2}$, lat$_{2}$).
\endinsert


\midinsert
\clabel[6_google_api]{Google Street View API url generation}
\picw=10cm \cinspic figures/3.2.2.1._google_api-get_svg_v2.pdf 
\caption/f Illustration of Google Street View API url generation
\endinsert

In order to make good use of the location and collect enough data, we decided to break down longer edges into smaller segments maintaining the minimal edge size fixed. We also select both of the starting and ending locations of each segment. In each position we also rotate around the spot.

We collect total of 6 images from each segment, 3 in each of its corners while rotating 120\degrees degrees around the spot. This allows us to get enough distinct images from each location and we don’t overlap with neighboring edges. See the illustration in Figure 7. Note that all of these images will correspond to one edge and thus to one shared score value.

Also note that we are limited to downloading images of maximal size 640x640 pixels as per the limitation of free use of Google Street View API.

\midinsert
\clabel[7_edge_breakdown]{Edge splitting and image generation scheme}
\picw=13cm \cinspic figures/3.2.2.1._breakdown_of_edge.pdf 
\caption/f Splitting of initial possibly large edge segments into sections not smaller than the minimal length limit for edge segment. Each of these generates six images, which are usually not overlapping even with neighboring edges.  
\endinsert

\seccc{Data augmentation introduction}2
%------------------

In CNNs we often use the method of data augmentation to extend datasets by simple image transformations to overcome limitations of small datasets. We can generate crops of the original images, flip and rotate them or alter their colors.

We will return to the issue of dataset augmentation in \ref[ch4_data_augmentation].

\label[ch3_neighborhood_in_osm]
\secc Neighborhood in Open Street Map data
%------------------

We are looking for another source of information about an edge segment in the neighborhood surrounding its location in Open Street Map data.

Open Street Map data is structured as vector objects placed with location parameters and large array of attributes and values. We can encounter point objects, line objects and polygon objects, which represent points of interest, streets and roads, building, parks and other landmarks.

\midinsert
\clabel[8_ex_params]{OSM data structure with parameters}
\picw=13cm \cinspic figures/3.2.3._osm_map_with_params.pdf 
\caption/f Example of structure of OSM data with parameters showing its properties.
\endinsert

From implementation standpoint, we have downloaded the OSM data covering map of our location and loaded it into a PostgreSQL database. In this way we can send queries for lists of objects in the vicinity of queried location. We will get into more detail about implementation in \ref[ch5_osm_marker].

\label[ch3_osm_vector]
\seccc{OSM neighborhood vector}1
%------------------

The structure of OSM data consists of geometrical objects with attributes describing their properties. In the PostgreSQL database each row represents object and attributes are kept as table columns.

Depending on the object type, different attributes will have sensible values while the rest will be empty. For better understanding consult table in Figure \ref[8_osm_interesting_attributes] with examples of attributes and their values and table in Figure \ref[8_osm_objects]. for examples of objects in OSM dataset.

\midinsert
\clabel[8_osm_interesting_attributes]{Sample of OSM attributes}
\picw=13cm \cinspic figures/3.2.3.1_table_possible_values.pdf 
\caption/f Sample of interesting attributes and their possible values in OSM dataset.
\endinsert

\midinsert
\clabel[8_osm_objects]{Sample of OSM objects}
\picw=7cm \cinspic figures/3.2.3.1_(semi)_table_objects_with_values.pdf 
\caption/f Example of values given to a selection of objects from OSM dataset. IMAGE IS STILL TEMPORARY, WILL BE FILLED WITH REAL DATA.
\endinsert

For example attribute “highway” will be empty for most objects, unless they are representing roads, in which case it reflects its size. We will be interested in counting attribute-value pairs, for example the number of residential roads in the area, which will have “highway=residential”.

Out of these pairs, we can build a vector of their occurrences. The only remaining issue is to determine which attribute-value pairs we select into our vector. If we used every pair possible, the vector would be rather large and more importantly mostly filled with zeros.

In order to select which pairs are important, we chose to look at OSM data statistics available at webpage taginfo.openstreetmap.org. From an ordered list of most commonly used attribute-value pairs, we have selected relevant pairs and generated our own list of pairs which we consider important.

Then for each distinct location of edge segment, we look into its neighborhood and count the number of occurrences of each pair from the list.

\midinsert
\clabel[9_ex_loc]{Unique locations with neighborhood vectors}
\picw=15cm \cinspic figures/3.2.3.1_locations_with_table.pdf 
\caption/f Four unique locations with their corresponding neighborhood vectors. Note that these can be very similar for close enough locations.
\endinsert

\midinsert
\clabel[10_vec_count]{Construction of neighborhood vector}
\picw=11cm \cinspic figures/3.2.3.1_construction_of_osm_vec.pdf 
\caption/f The construction of neighborhood vector from collection of nearby objects.
\endinsert

Each distinct location of each edge will end up with same sized OSM vector marking their neighborhood. Note that due to the method of downloading multiple images per one edge, for example by using the same location and rotating around the spot, some of these will have the same OSM vectors.

\midinsert
\clabel[10b_six_imgs]{Generation of data entries from edge segment}
\picw=10cm \cinspic figures/3.2.3.1_6images_and_their_shared_data.pdf 
\caption/f Example of further not divided edge segment, which generates data entries, where certain values overlap. For example the score is shared among all images produced from one segment.
\endinsert

One further undivided edge segment contains 6 images which share the same score, and some of which will share location and therefore also their OSM vectors. For illustration see Figure \ref[10b_six_imgs].

\seccc{Radius choice}2
%------------------

Depending on the radius we choose different area will be considered as neighborhood. If we were to choose too small radius, the occurrences would mostly result in zero OSM vector. On the other hand selecting too high radius would lead many OSM vectors to be indistinguishable from each other as they would share the exact same values.

Our final choice for radius was value of 100 meters, which was empirically tested in \ref[ch6] Results.

\seccc{Data transformation}3
%------------------

Similar to the spirit of data augmentation for images, we can try editing the OSM vectors in order that they will be more easily used by CNN models.

Instead of raw data of occurrences, we can convert this information into one-hot categorical representations or reduce them into Boolean values. Multiple readings of varying radius size can also be used for better insight of the neighborhood area.

See more about data augmentation in \ref[ch4_data_augmentation].

\label[ch4]
\chap The Method
%==============================================================

Our method will rely upon using Convolutional Neural Networks (CNNs) mentioned in \ref[ch2_history_of_cnn] on an annotated dataset described in \ref[ch3]. As we have enriched our original dataset with multiple types of data, particularly imagery Street View data and the neighborhood vectors, we have an option to build more or less sophisticated models, depending on which data will they be using. We can build a model which uses only relatively simple OSM data, or big dataset of images, or finally the combination of both.

Depending on which data we choose to use, different model architecture will be selected. Furthermore we can slightly modify each of these models to tweak its performance.

\label[ch4_building_blocks]
\sec Building blocks 
%===================

Regardless of the model type or purpose, there are certain construction blocks, which are repeated in the architecture used by most CNN models.

\label[ch4_model_abstraction]
\secc Model abstraction
%------------------

When building a CNN model, we can observe an abstracted view of such model in terms of its design. Whereas at the input side of the model we want to extract general features from the dataset, at the output side we stride for a clear classification of the image.

\midinsert
\clabel[11_extractor_class]{Feature extractor and classificator}
\picw=13cm \cinspic figures/4.1.1_model_abstraction.pdf 
\caption/f Abstraction of CNN model into feature extractor section and classificator section.
\endinsert

Each of these segments will require different sets of building blocks and will prioritize different behavior. Good model design will lead to generalization of concrete task-specific data into general features, which will then be again converted into concrete categories or scores. The deeper the generic abstraction is, the better the model behavior in terms of overfitting will be.

Classification segment transforms the internal feature representation back into the realm of concrete data related to our task. In our tasks we are interested in score in range from 0 to 1 as illustrated by Figure \ref[12_sigmoid_output].

\midinsert
\clabel[12_sigmoid_output]{Classificator section formula}
\picw=13cm \cinspic figures/4.1.1._output_formula.pdf 
\caption/f Classificator section formula describing how we generate one value at the end of CNN model ranging from 0 to 1 as a score estimate.
\endinsert

\label[ch4_fully_connected_layers]
\secc Fully-connected layers
%------------------

The fully-connected layer denoted as “Dense” in Keras stands for a structure of neurons connected with every input and output by weighed connections. In Neural Networks these are named as hidden layers.

\midinsert
\clabel[13_fc]{Fully connected layer}
\picw=13cm \cinspic figures/4.1.2._fully-connected.pdf 
\caption/f Shows the model of connections of neurons in fully connected layer.
\endinsert

The fully-connected layer suffers from a large amount of parameters it generates – weights in each connection between neurons and biases in individual neuron units. Fully-connected layers are usually present in the classification section of the model.

\secc Convolutional layers
%------------------

Convolutional layers are trying to circumvent the large amount of parameters of fully-connected layers by localized connectivity. Each neuron looks only at certain area of the previous layer.

\midinsert
\clabel[14_conv]{Convolutional layer}
\picw=10cm \cinspic figures/4.1.3_convolutional_layer.pdf 
\caption/f Schematic illustration of connectivity of convolutional layer. Connections are limited to the scope of receptive field.
\endinsert

For their property to use considerably less parameters while at the same time to focus on features present in particular sections of image, they are often used as the main workforce in the feature extractor section of CNN models.

\secc Pooling layers
%------------------

Pooling layers are put in between convolutional layers in order to decrease the size of data effectively by downsampling the volume. This forces the model to reduce its number of parameters and to generalize better over the data. Pooling layers can apply different functions while they are downsamplig the data – max, average, or some other type of normalization function.

\midinsert
\clabel[15_pool]{Pooling layer}
\picw=10cm \cinspic figures/4.1.4._pooling_layer.pdf 
\caption/f Pooling layer structure effectively downsampling the volume of input data.
\endinsert

\label[ch4_dropout_layers]
\secc Dropout layers
%------------------

Dropout layer is special layer suggested by ~\cite[srivastava2014dropout] which has since been widely used on the design of CNN architectures as a tool to prevent model overfitting.

The dropout layer placed between two fully-connected layers functions randomly drops connections between neurons with certain probability during the training period. Instead of fully-connected network of connections we are left with a thinned network with only some of the connections remaining. This thinned networks is used during training and prevents neurons to rely too much on co-adaptation. They are instead forced to develop more ways to fit the data as there is the effect of connection dropping. During test evaluation, the full model is used with its weights changed by the probability of dropout probability.

\midinsert
\clabel[16_drop]{Dropout layer}
\picw=8cm \cinspic figures/4.1.5._dropout.pdf 
\caption/f Illustration of dropout layers effect during the training period, which renders certain connection invalid with set probability. This prevents the network to depend too much on overly complicated formations of neurons.
\endinsert

\label[osm_model]
\sec Open Street Map neighborhood vector model 
%===================

In this version of model, we broke down edge segments formerly representing streets in real world into regularly sized sections each containing two locations of its beginning and ending location. These locations were enriched with OSM neighborhood vectors in \ref[ch3_neighborhood_in_osm].
	
\midinsert
\clabel[17_one_edge_to_many_loc]{Distinct locations from edge segment}
\picw=12cm \cinspic figures/4.2._edge_progress.pdf 
\caption/f Edge segment broken down into set of distinct locations, where each of these locations is assigned its own neighborhood vector.
\endinsert

Single unit of data is therefore a neighborhood vector linked to each distinct location of the original dataset. We have designed a model which takes these vectors as inputs and scores as outputs.

The OSM vector model is built from repeated building blocks of fully-connected layer followed by dropout layer. Fully connected layer of width 1 with sigmoid activation function is used as the final classification segment. Figure \ref[18_osm_model] shows the model alongside its dimensions.

\midinsert
\clabel[18_osm_model]{OSM neighborhood vector CNN model}
\picw=15cm \cinspic figures/4.2._model.pdf 
\caption/f CNN model making use of only the neighborhood OSM vector data.
\endinsert

\label[img_model]
\sec Street View images model 
%===================

Each edge segment is represented by multiple images captured via the Street View API. Images generated from the same edge segment will share the same score label, however the individual images will differ. We can understand one image-score pair as a single unit of data.

The image data can be augmented in order to achieve richer dataset, see \ref[ch4_data_augmentation].

As discussed in \ref[ch2_history_of_cnn] we are using a CNN model which has been trained on ImageNet dataset. We reuse parts of the original model keeping its weights and attach a new custom classification segment architecture at the top of the model.

We can generally divide even the more complicated CNN models into two abstract segments as mentioned in \ref[ch4_model_abstraction]. The beginning of the model, which usually comprises of repeated structure of convolutional layers, followed by a classification section usually made of fully-connected layers.

The former works in extracting high dimensional features of incoming imagery data, whereas the classification section translates those features into a probability distribution over categories or score. In our case we are considering a regression problem model, which works with score instead of categories.

As was mentioned in \ref[ch2_feature_transfer_research] we reuse the model trained on large dataset of ImageNet, separate it from its classificatory and instead provide our own custom made top model. See Figure \ref[19_base_top].

\midinsert
\clabel[19_base_top]{Reused base model with custom top model}
\picw=12cm \cinspic figures/4.3._feature_transfer.pdf 
\caption/f Reusing part of already trained CNN model as a base model and adding our custom top model.
\endinsert

We prevent the layers of base model from changing their weights and train only the newly attached top model for the new task. There are certain specifics connected with this approach which we will explore in \ref[ch4_feature_cooking] section.

\label[ch4_model_architecture]
\secc Model architecture
%------------------

The final model architecture is determined by two major choices: which CNN to choose as its base model and how to design the custom top model so it’s able to transfer the base models features to our task.

\secc{Base model}
%------------------

The framework we are working with, Keras, allows us to simply load many of the successful CNN models and by empirical experiments asses, which one is best suited for our task. More about Keras in the appropriate section \ref[ch4_keras].

The output of what remains of the base CNN model is data in feature space. The dimension will vary depending on the type of model we choose, the size of images we feed the base models and also the depth in which we chose to cut the base CNN model.

The remains of base CNN model are followed by a Flatten layer which converts the possibly multidimensional feature data into a one dimensional vector, which we can feed into the custom top model.

\midinsert
\clabel[20_diff_in_diff_out]{Dimensionality of feature vectors}
\picw=13cm \cinspic figures/4.3.1.1_different_sized_imgs.pdf 
\caption/f Example of differently sized images on the input which result in different feature vector size. TEMPORARY VALUES
\endinsert

\label[ch4_custom_top]
\secc{Custom top model}
%------------------

We feed the feature vector into a custom model built of repeated blocks of fully-connected neuron layers interlaced with dropout layers. The number of neurons used in each of the layers influences the so called model “width” and the number of used layers influences the model “depth”. Both of these attributes influence the amount of parameters of our model. We can try various combinations of these parameters to explore the models optimal shape.

The final layer of the classification section consists of fully-connected layer of width 1 with sigmoid activation function which weighs in all neurons of the previous layer. See Figure \ref[21_custom_top]. Note that in the final model we chose to interlace individual fully-connected layers with dropout layers.

\midinsert
\clabel[21_custom_top]{Top model structure}
\picw=14cm \cinspic figures/4.3.1.2_feature-score.pdf 
\caption/f Top model structure which takes in the feature vector and follows with fully connected layers which are retrained on our own task.
\endinsert

\secc{The final architecture}
%------------------

The final architecture composes of base CNN model with its weights trained on the ImageNet dataset and of custom classification top model trained to fit the base model for our task.

\midinsert
\clabel[22_img_model]{Image model structure}
\picw=15cm \cinspic figures/4.3.1.3._img_model.pdf 
\caption/f Final schematic representation of CNN model using Google Street View images. Note that parameters of width and depth can change its performance.
\endinsert

\label[mix_model]
\sec Mixed model 
%===================

After discussing the architecture of two models making only a partial use of the data collected in our dataset, we would like to propose a model combining the two previous ones.

In this case one segment again generates multiple images, which share the same score and depending on how they are created they could also share the same neighborhood OSM vector representing the occurrences of interesting structures in its proximity. Different edges will generate not only different images, but also different scores and OSM vectors.

As a single unit of data we can consider the triplet of image, OSM vector and score. It’s useful to note that later in designing the evaluation method of models, we should take into account, that the neighborhood vector and score can be repeated across data. When splitting the dataset into training and validation sets, we should be careful and place images from one edge into only one of these sets. Otherwise data with distinct images, but possibly the same neighborhood vector and score could end up in both of these sets. Figure \ref[23_single_data_unit_for_mix] illustrates how single data units are generated from one edge.

\midinsert
\clabel[23_single_data_unit_for_mix]{Structure of mixed model input data}
\picw=14.5cm \cinspic figures/4.4._mixed_data_repetition.pdf 
\caption/f Example of possible decomposition of original edge into rows of data accepted by the mixed model. We use both the image data and the OSM neighborhood vectors.
\endinsert

We can join the architectures designed in previous steps, or we can design a new model. We chose to join the models in their classification segment. We propose a basic idea for a simple model architecture, which concatenates feature vectors obtained in previous models and follows with structure of repeated fully-connected layers with dropout layers in between. Concatenation joins the two differently sized one dimensional vectors into one. As is observable on Figure \ref[24_mix_model] we use several parameters to describe the models width and depth.

\midinsert
\clabel[24_mix_model]{Mixed model structure}
\picw=15cm \cinspic figures/4.4._mixed_model.pdf 
\caption/f Final architecture of the mixed model, which uses both OSM vector and the image data.
\endinsert


\label[ch4_data_augmentation]
\sec Data Augmentation 
%===================

When using complicated models with high number of parameters on relatively small datasets, the danger of overfitting is always present. We would like to combat this by expanding our dataset with the help of data augmentation.

Overfitting occurs when the model is basically able to remember all the samples of the training dataset perfectly and incorporate them into its structure. It achieves very low error on the training data, but looses its ability to generalize and results in comparably worse results on the validation set.

The idea of data augmentation is to transform the data we have in order to get more samples and a model which in turn behaves better on more generalized cases.

This cannot be done just blindly, as some of these transformations could mislead our model (for example left to right vertical flip makes sense in our case, but a up side down horizontal flip wouldn't).

There are multiple ways we can approach the problem of generating as many images from our initial dataset as we can. Before getting to the data augmentation aspect, please note, that this is also the reason, why we are generating multiple images per segment. We stand in two corners of each edge segment and rotate 120\degrees degrees to get three images on each side. We have also employed a technique, which splits long edges into as many small segments as possible, while not hitting the self imposed minimal edge length.

It could be debated, that we could rotate for smaller angle or split edges to even smaller segments in order to take advantage of the initial dataset fully. However we came across an issue, that with too small minimal edge length or with different rotation scheme, we obtain very similar images, which actually do not improve the overall performance. This occurs when we don't generate differing enough images during downloading. For actual performance change see chapter \ref[ch6].

We face similar issue when choosing a radius for obtaining OSM neighborhood vector as specified in the section \ref[ch3_neighborhood_in_osm]. Instead of selecting one particular radius setting, we can make use of results of multiple queries. When building the OSM vector we would effectively multiply its length by concatenating it with other versions of OSM vectors. We could concatenate the vectors acquired with one fixed radius setting with another version with different radius. See Figure \ref[25_osm_multiple_radius_aug] for illustration.

\midinsert
\clabel[25_osm_multiple_radius_aug]{Multiple OSM vector use}
\picw=13cm \cinspic figures/4.5._data_augmentation_osm.pdf 
\caption/f Illustration of the construction of combined OSM vector and the expected result of more specific area targetting by the model.
\endinsert

	Finally we also come across the method of data augmentation by transformation of the original image dataset. Certain operation, such as vertically flipping the image make sense for our dataset. (Also the normalization of color palete can be used in our task.)
	We show an example of images undergoing such transformation on Figure \ref[26_image_augmentation_flips]. Note that in this particular example we chose vertical flipping alongside with clipping of 90\% of the image while making up for the lost 10\%. These operations are random and the resulting images are added to create a larger dataset. For the sake of repetition of experiments with the same data, we save these generated images into an expanded dataset.

\midinsert
\clabel[26_image_augmentation_flips]{Image data augmentation}
\picw=12cm \cinspic figures/nullfig.pdf 
\caption/f Data augmentation example.
\endinsert

\label[ch4_model_training]
\sec Model Training 
%===================

\secc Data Split 
%------------------

Traditionally we split our dataset into two sets – training set, which we use for training of model and so called validation set, which is used only for models performance evaluation. As we discuss in \ref[ch4_model_evaluation], we employ more complex strategy of k-fold cross validation test to obtain more precise results.

The difference between the error achieved on training data and on validation data can be used as a measure of our model overfitting.

\secc Training setting 
%------------------

We are using backpropagation algorithm to train our models as is supported by the selected Keras framework. We can choose from a selection of optimizers, which control the learning process. We made use of the more automatic optimizers supported by Keras, such as ‘rmsprop’ and ‘adam’ (see ~\cite[adam_optimizer]). For greater parametric control we can also select the SGD optimizer.

Given the nature of our task, we are solving a regression problem, trying to minimize deviation from scored data. In most models mentioned in \ref[ch2_competition], the task revolves around selecting the correct category to classify objects.

Accordingly we have to select appropriate loss function. We have selected the mean squared error metric with the formula given by Figure \ref[27_mse_formula].

\midinsert
\clabel[27_mse_formula]{Mean squared error metric}
\picw=8cm \cinspic figures/4.6.2_mse_formula.pdf 
\caption/f Mean squared error metric used as loss function when training models.
\endinsert

\secc Training stages 
%------------------

As has been discussed in \ref[ch4_model_architecture], we are building models by reusing base of other already trained CNN models. In our case we make use of weights loaded from model trained on the ImageNet dataset.

We attach a custom classifier section to base model and try to train it on a new task. In order to preserve the information stored in connections of the base model, we lock its weights and prevent it from retraining. The only weight values which are changing are in the custom top model. This can be understood as a first stage of training the model as illustrated on Figure \ref[28_training_stages].

\midinsert
\clabel[28_training_stages]{Training stages schematics}
\picw=10cm \cinspic figures/4.6.3_training_stages.pdf 
\caption/f Training stages illustrated for feature transfer approach.
\endinsert

This is sometimes followed with a finetunning period, where we unlock certain levels of the base CNN model for training. However this is commonly done with customized optimizer setting, so that the changes to the whole model weights are not too drastic. We are also usually not retraining the whole model, because of the computational load this would take.

\label[ch4_feature_cooking]
\secc Feature cooking 
%------------------

This method is specific to situation when we are training a model with parts, which are frozen, as is in the case of \ref[img_model] image and \ref[mix_model] mixed model design. We can take advantage of the fact that certain section of the model will never change and precompute the image features for a fixed dataset.

In this way we can save computational costs associated with training the model. In the end the original model can be rebuilt by loading obtained weight values.

This allows for fast prototyping of the custom top model, even if the whole model composes of many parameters in the frozen base model. For illustration see figure \ref[29_reusing_features].

\midinsert
\clabel[29_reusing_features]{Reusing saved image features from a file}
\picw=14cm \cinspic figures/4.6.4_feature_cooking.pdf 
\caption/f Reusing saved image features from a file instead of costly computations.
\endinsert

\label[ch4_model_evaluation]
\sec Model evaluation 
%===================

As was mentioned in \ref[ch4_model_training] we are using the practice of splitting dataset into training and validation dataset, with the k-fold cross validation technique.

In order to prevent from being influenced by the selection bias, we split our entire dataset into k folds and then in sequence we use these to build training datasets and validation datasets. Every fold will take role of validation set for a model, which will train on data composed from all the remaining folds. Each of these will run a full training ended by an evaluation giving us score. Eventually we can calculate the average score with standard deviation.

This approach obviously increases the computational requirements, because it repeats the whole experiment for each fold. It is not used while prototyping models, but as a reliable method to later generate score. We have chosen the number of folds to be 10, as is a traditionally recommended approach.

\midinsert
\clabel[30_k_fold]{K-fold cross validation}
\picw=11cm \cinspic figures/4.7._k_fold_cross_validation.pdf 
\caption/f Splitting schema employed in the k-fold cross validation. We are given average score alongside with its standard deviation.
\endinsert

\sec Frameworks and projects 
%===================

The existence of this thesis depends on couple of frameworks which helped it greatly. Framework called “Keras” allowed for fast prototyping of Deep CNN models as well as efficient training environment. Thanks to the Metacentrum project we had access to machines with high enough computational power needed to teach these models.

\label[ch4_keras]
%\secc Keras
%------------------
%A

%\secc Metacentrum project
%------------------
%A

\label[ch5]
\chap The Implementation
%==============================================================

\sec Project overview 
%===================

In planning of the composition of project code, we have somewhat separated the sections responsible for downloading data, from those managing the dataset and modeling and running experiments.

Downloader is tasked to download images from Google Street View API from the initial dataset of edges and nodes mentioned in \ref[ch3_initial_dataset]. Segment object works as a unit holding information about edge and its corresponding images and score. See \ref[ch5_downloader].

However for later working with data, we have created a DatasetHandler which contains all the necessary functions. See \ref[ch5_dataset_handler].

To run more instances of models and later evaluate them, we have chosen to build individual experiments from custom written setting files in \ref[ch5_settings]. Experiment Runner provides the common framework for all these more complicated computations in \ref[ch5_experiment_runner]. In Settings folder we hold setting files defining each experiment.

\midinsert
\clabel[31_overview]{Project structure overview}
\picw=11cm \cinspic figures/5.1._project_overview.pdf 
\caption/f Project structure in a schema.
\endinsert

\label[ch5_downloader]
\sec Downloader functionality 
%===================

The Downloader is the main method of acquiring imagery data and preparation of dataset. It first creates necessary folders according to the directory path and custom name we provide it with.

Defaults.py contains default settings for the downloader, such as default number of times the code should try downloading each image and internal codes with which to mark unsuccessfully downloaded segments. Besides these internal representation settings, it also controls the pixel size of downloaded images.

\midinsert
\clabel[code1_downloader]{RunDownload}
\picw=13cm \cinspic pseudocodes/5.3._downloader.pdf
\caption/t RunDownload code sample.
\endinsert

Downloading procedure is initiated in RunDownload() method (see Code \ref[code1_downloader]), which parses node and edge data in provided GeoJSON files and build an array of Segments.

For each segment in this array, we generate list of images to download. Long segments will be split by interpolating the start and end location to create smaller edge sections (see Code \ref[code3_longedges]). We generate a url link an shape corresponding to Google Street View API discussed in Code \ref[code2_googleapi] alongside with unique  file name.

\midinsert
\clabel[code2_googleapi]{Google Street View API url}
\begtt
http://maps.googleapis.com/maps/api/streetview?size=600x400
   &location=<lat>,<long>&heading=<angle from north>&key=<api>
\endtt
\caption/t Exact Google Street View API url we need to generate
\endinsert

\midinsert
\clabel[code3_longedges]{Break down long edges}
\picw=13cm \cinspic pseudocodes/5.3._break_down_long_edge.pdf
\caption/t Code segment which breaks down long edges and generates lists of urls and filenames Segment.getGoogleViewUrls().
\endinsert

One by one we try to download each of these images, marking segments with error flag, if we were unable to access them. The code attempts to retry each request for default number of times, to prevent temporary instability of network to hinder the downloading process.

We also check for invalid images in areas where Google Street View doesn’t have any data available (see Figure \ref[32_noimagery]).

\midinsert
\clabel[32_noimagery]{No imagery available on Google Street View}
\picw=6cm \cinspic figures/sorry_no_imagery_here.jpg 
\caption/f Location outside of streets with available photographic imagery.
\endinsert


Eventually we save the pickled array into file for further processing. Folder structure is as follows in Figure \ref[code4_folder_struct]

\midinsert
\clabel[code4_folder_struct]{Dataset folder structure}
\begtt
dataset folder: 5556x_example_dataset_299px/
SegmentsData.dump
images/0000_0.jpg
images/0000_1.jpg
…
images/5546_5.jpg
\endtt
\caption/t Folder structure of dataset.
\endinsert

\midinsert
\clabel[code5_downloader_check]{Check downloaded dataset}
\picw=13cm \cinspic pseudocodes/5.3._downloader_check.pdf
\caption/t RunCheck code sample checks the downloaded dataset and redownloads missing images.
\endinsert

Function RunCheck() in \ref[code5_downloader_check] allows us to load previously downloaded Segments file and check for erroneous data – for example in case of sudden network failure we can download only the missing files and then save the fixed Segments file.

\label[ch5_osm_marker]
\sec OSM Marker 
%===================

Our code works with data downloaded from Open Street Maps to estimate what is the neighborhood of each segment. We have downloaded the .osm data file of corresponding location and exported it into a PostgreSQL database following commands represented in Code \ref[code6_convert_osm_to_db].

\midinsert
\clabel[code6_convert_osm_to_db]{Loading data into PostgreSQL database}
\picw=13cm \cinspic pseudocodes/5.4._osm_db_import.pdf
\caption/t Loading data into PostgreSQL database.
\endinsert

	Having a  PostgreSQL database is advantageous, because it allows us sending repeated queries to database of objects with geographical location while using PostgreSQL macros and functions for getting distance and intersections between areas. PostgreSQL database has hierarchical structure of data storage which is necessary for fast data access. Our python code can access this database with simple queries and then process its responses.

\midinsert
\clabel[code7_osm_marker]{Marking data with OSM vector}
\picw=13cm \cinspic pseudocodes/5.4._osm_marker.pdf
\caption/t Marking data with OSM vector.
\endinsert

The code \ref[code7_osm_marker] loads array of Segments and one by one accesses all the distinct locations stored in each Segment. This could be only two locations for segments too short to be broken down, or more if the original segment was split. Minimally these are the starting and the ending locations of the segment.

We generate a SQL command, which targets particular columns which we chose to observe and also filters the data with “WHERE” clause for objects in distance smaller than chosen radius from segments location.

OSM data we imported into PostgreSQL database takes structure of four tables representing “point”, “line”, “polyline” and “road” objects. We look into each of these tables and combine the results.

Result of query gives us list of rows, each representing one object in vicinity of our location and columns containing attributes. We can produce a “attribute=value” pair from the values in each row alongside with the count of their occurrences.

Final neighborhood vector counts all these occurrences in fixed positions as was discussed in \ref[ch3_osm_vector].

Each segment stores multiple neighborhood vectors, one for each distinct location. Marked array of Segments is saved into pickled .dump file. We can store multiple versions of these files, each with different radius setting, while reusing all the downloaded images.

\label[ch5_dataset_handler]
\sec Datasets and DatasetHandler 
%===================

DatasetObject is a structure shielding us from low level manipulation with the segments stored in .dump file, which can then remain unchanged and be reused for many experiments.
	
Figure \ref[33_dataset_object] shows us the structure of DatasetObject, with its most important functions and variables exposed.

\midinsert
\clabel[33_dataset_object]{Dataset object}
\picw=12cm \cinspic figures/5.5._dataset_object.pdf 
\caption/f Dataset object.
\endinsert

Dataset is initialized by reading a SegmentsData.dump file, however it stores data in its own four internal arrays. These are: list of urls to images, labels marking the scores, OSM vectors if we have loaded a marked Segments file and unique IDs of original segments. Note that we are not directly loading all the images yet, rather we are keeping only their filenames. We can access the images when necessary, saving us from wasteful memory allocation.

We also provide multiple getter functions which split data into training and validation datasets for the purpose of testing experiments. Each model type will require access to different data – image only model will for example omit all the OSM neighborhood vectors.

\midinsert
\clabel[code8_dataset_handler]{DatasetHandler}
\picw=13cm \cinspic pseudocodes/5.5._dataset_handler.pdf
\caption/t DatasetHandler.
\endinsert

\midinsert
\clabel[code9_dataset_visualization]{Data visualization}
\picw=13cm \cinspic pseudocodes/5.5._dataset_statistics.pdf
\caption/t Functions for data statistics visualization.
\endinsert

\midinsert
\clabel[33_dataset_visualization]{Dataset visualization}
\picw=6cm \cinspic figures/nullfig.pdf 
\caption/f Example of dataset visualization.
\endinsert

%\secc Dataset statistics
%\secc DatasetHandler structure

\sec Models and ModelHandler 
%===================

ModelHandler is composed of three functional sections. ModelOI provides the functions handling input and output – including interaction with DatasetHandler and reporting functions. ModelGenerator is responsible for building models in Keras syntax depending on settings we choose for current model. ModelTester provides functions controlling the training and testing capabilities of model.

These individual blocks are controlled from ExperimentRunner, which ties them together and manages shared Settings information.


\secc Model description in Keras
%------------------

Generic code for building models in Keras works in two modes – Sequencial and Functional. We will use Functional in these particular examples as it is convenient both for writing and understanding.

We will mention some of the layers we can use to build Keras models.

\begtt
Out = Dense(256, activation=’relu’)(In)
Out = Dense(1, activation=’sigmoid’)(In)
\endtt
	
Dense layers stand for fully connected layers mentioned in \ref[ch4_fully_connected_layers]. Here we can see two possible uses for this type of layer. With width set to 1, this layer represents one neuron which can be used as the final output of our classifier.

\begtt
Out = Dropout(probability)(In)
\endtt

Dropout layer is used accompanying the Dense layers and follows the process mentioned in \ref[ch4_dropout_layers]. With selected probability the connection of neurons will be dropped during the training period.

\midinsert
\clabel[code10_build_generic_model]{Building generic model in Keras}
\picw=13cm \cinspic pseudocodes/5.6._build_generic_model.pdf
\caption/t Building generic model in Keras.
\endinsert

We are using these mentioned layers in our models, for more detailed survey check \ref[APPENDIX KERAS SYNTAX] APPENDIX KERAS SYNTAX.

After building the model we need to compile it specifying settings which will be used for training such as the optimizer, loss function and additional measured metrics.

\begtt
Model.compile()
\endtt


For details about which settings we use for training, check \ref[ch4_model_training].

Finally our model is trained on dataset with fit command.
	
\begtt
model.fit(data, labels, ...)
\endtt

The full code to build and train a generic Keras model on a dataset can be seen in Figure \ref[code11_run_generic_experiment].

\midinsert
\clabel[code11_run_generic_experiment]{Fit generic model to data}
\picw=13cm \cinspic pseudocodes/5.6._run_generic_experiment.pdf
\caption/t Fit generic model to data in Keras.
\endinsert



\secc Model building
%------------------

Particular models described in section \ref[ch4] are shown here with functions which generate them. We have stated that their structure can be parametrized, and that we can alter these parameters to change their shape and performance. We can change the depth and width of these models.
	
\seccc {OSM only model}1
%------------------

See Code sample \ref[code12_osm_model], which builds the model discussed in \ref[osm_model].

\midinsert
\clabel[code12_osm_model]{OSM only model code}
\picw=13cm \cinspic pseudocodes/5.6._osm_model.pdf
\caption/t Function producing a Keras model described in \ref[osm_model].
\endinsert

\seccc {Images only model}2
%------------------

When we are working with model described in \ref[img_model], we make use of the feature transfer method and as such the training process can be made easier as was discussed in \ref[ch4_feature_cooking].

Also the Keras syntax will be correspondingly altered. We create two models instead of a single whole model. One represents the feature extractor section of model and is mostly created by the reused base of pretrained CNN model. We can load these as base of our models with weights initialized from model trained on ImageNet dataset, such as is done in \ref[code13_applications].

\midinsert
\clabel[code13_applications]{Applications}
\picw=13cm \cinspic pseudocodes/5.6._applications.pdf
\caption/t Applications
\endinsert

Our second model is then made of just our custom top architecture discussed in \ref[ch4_custom_top]. We will feed it with outputs of features from the first model. Note that for repeated experiments with the same dataset and model setting, we can save these features into files and spare a lot of valuable computation time. Code snippet in \ref[code14_feature_cooking_img_model] shows us an example of the use of two models.

\midinsert
\clabel[code14_feature_cooking_img_model]{Feature cooking}
\picw=13cm \cinspic pseudocodes/5.6._feature_cooking_img_model.pdf
\caption/t Feature cooking
\endinsert

\seccc {Mixed model}3
%------------------

Mixed model uses the same techniques of feature transfer and is fully described in \ref[mix_model].

Code in \ref[code15_mix_model] shows how we build this model. Note that we can even reuse the saved feature files from image only model, as long as we maintain the same order in our dataset. DatasetHandler can shuffle data in deterministic manner which allows us to do so.

\midinsert
\clabel[code15_mix_model]{Building mixed model}
\picw=13cm \cinspic pseudocodes/5.6._mix_model.pdf
\caption/t Building mixed model.
\endinsert

\secc ModelHandler Structure
%------------------

Figure \ref[34_model_handler_structure] illustrates the various functions  which are part of the ModelHandler. In \ref[code16_model_handler_functions] we can see snippets of code dealing with building and testing of models. Note that in one run we are handling multiple instances of models and datasets, as is explained in \ref[ch5_experiment_runner].

\midinsert
\clabel[34_model_handler_structure]{ModelHandler Structure}
\picw=13cm \cinspic figures/5.6.3._model_handler_struct.pdf 
\caption/f ModelHandler Structure.
\endinsert	
	
\midinsert
\clabel[code16_model_handler_functions]{ModelHandler functions}
\picw=13cm \cinspic pseudocodes/5.6._model_handler_functions.pdf
\caption/t ModelHandler functions.
\endinsert


\label[ch5_settings]
\sec Settings structure 
%===================

A

\label[ch5_experiment_runner]
\sec Experiment running 
%===================

A

\sec Training 
%===================

A

\sec Testing 
%===================

A

\sec Reporting and folder structure 
%===================

A

\sec Metacentrum scripting 
%===================

A

\label[ch6]
\chap Results
%==============================================================

C

\label[ch7]
\chap Discussion
%==============================================================

B

\label[ch8]
\chap Conclusions
%==============================================================

V

\endinput
%%
%% End of file `ch00.tex'.
