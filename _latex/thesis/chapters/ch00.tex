% !TEX root = ../thesis.tex
\label[ch1]
\chap Introduction
%==============================================================

Section \ref[ch2] Research contains information about similar tasks as well as usage history of Convolutional Neural Networks.

Section \ref[ch3] Task describes what we are trying to achieve as well as the dataset available for this task. We also comment on the possible ways of enhancing the initial dataset either with imagery data from Google Street View, or with vector representation of neighborhood collected from Open Street Map database.

Section \ref[ch4] Method goes deeper into the methodology of Convolutional Neural Networks and discusses the architecture decisions we have made in designing our models. We also cover the topics of model training and evaluation schemes.

Section \ref[ch5] Implementation explores the actual code solutions and tries to illustrate chosen approaches with pseudocode. We also explore the syntax necessary for model building in framework Keras.

Section \ref[ch6] Results presents measurements and evaluation of each individual approach we employed to enhance the performance of our models. It also shows the final assessment of our model capabilities.

Section \ref[ch7] Discussion comments upon the results of taken experiments.

Section \ref[ch8] Conclusion closes the topic with final words.

\label[ch2]
\chap Research
%==============================================================

\label[ch2_planning]
\sec Planning
%===================

There are many applications for the task to search for the shortest route on graph representation of map. We can abstract many real world scenario problems into this representation and then use many already existing algorithms commonly used for this class of tasks.

\midinsert
\clabel[1_abstracted_map]{Abstracted representation of map}
\picw=13cm \cinspic figures/2.1._abstracted_representation_of_map.pdf 
\caption/f Abstracted representation of map, description of real world location via set of nodes and edges. This representation allows us to run generic algorithms over it.
\endinsert

We have a set of nodes N, which can be understood as places on map and edges E, which are the possible paths from one node to another. In order to have a measurement of quality of traveling between two nodes, we also need a cost function. This cost function will assign a positive value c $\in$ \realnumbers + to each edge. Typically we are facing the task of finding the shortest path, in which we are minimizing the aggregated cost. See illustration of Figure \ref[1_abstracted_map].

In real life scenario, road segments exhibit many different parameters which influence how fast we can traverse them. There are more or less objective criteria such as surface material, size of the road, time of the day, and criteria which depends solely on the preference of driver. What is the surrounding environment, what is the comfort level of the road, etc.

This tasks gets more interesting, when we look at more complicated examples, where the cost function is multi-criterial. In such case we need more data at our disposal and we can also  expect the model to be more computationally difficult. For practical use, we need to use effective algorithm with speed up heuristics (see ~\cite[hrnvcivr2016practical]). Great part of research is also in the area of hardware efficient algorithms, which would work on maps containing continent-sized datasets of nodes and edges, and yet coming up with solution in realistic time. We can imagine such necessity on hand-held devices of car gps.

\label[ch2_data_collection]
\secc Data collection
%------------------

Cost function can be very simple, but in order that it works on real life scenarios, we usually need more complicated one with lots of data recorded. Estimation of how much “cost” we associate with one street (represented by edge e $\in$ E connecting two crossroads represented by nodes) should reflect how much time we spend in crossing it.

In case of planning for cars we generally just want to get across as fast as possible, or to cover minimal distance. When the user is driving a bicycle, more factors become relevant. We need to know the quality of terrain, steepness of the road, amount of traffic in the area and overall pleasantness of the road. In many cases the bikers will not follow the strictly shortest path, choosing their own criteria, such as for example stopping for a rest in a park.

Some of these these criteria are measurable and objectively visible in the real world. For these we need to have highly detailed data available with parameters such as the quality of road and others. Other criteria are based on subjective, personal preference of some routes over other and for these we might need a long period of traces recording which routes have users selected in past. For example the work of ~\cite[navigation_made_personal] makes heavy use of user recorded traces.

See ~\cite[hrnvcivr2016practical] and Figure \ref[2_slowdown_features] for examples of types of measurements we would likely need to estimate cost of each edge considering the slowdown effect of these features.

\midinsert
\clabel[2_slowdown_features]{Measurable features of real world road segment}
\picw=13cm \cinspic figures/2.1.1._example_slowdown_features.pdf 
\caption/f Example of the categories of highly detailed data we would require for multi-criteria cost function formulation as presented by ~\cite[hrnvcivr2016practical]. List of features contributing to a slowdown effect on route segment.
\endinsert

In any case highly qualitative, detailed and annotated dataset is required alongside with a carefully fitted cost function which would take all these parameters into account. Large companies are usually protective of their proprietary formulas of evaluating costs for route planners. For example ~\cite[navigation_made_personal] makes use of the road network data of Bing Maps with many parameters related to categories such as speed, delay on segment and turning to another street, however the exact representation remains unpublished.

As we will touch upon this topic in later chapters, its useful to realize that this highly qualitative dataset is not always available. We would like to carry information we can infer from small annotated dataset into different areas, where we lack detailed measurements. We are instead using visual information of Google Street View images, which is more readily available in certain areas than the highly qualitative dataset.

\label[ch2_history_of_cnn]
\sec History of Convolutional Neural Networks 
%===================

Initial idea to use Convolutional Neural Networks (CNNs) as a model was introduced in ~\cite[lecun1998gradient] by LeCun with his LeNet network design trained on the task of handwritten digits recognition. 

In this section we trace the important steps in the field of Computer Vision which lead to the widespread use of CNNs in current state of the art research. For more detailed overview we recommend ~\cite[schmidhuber2015overview].

\secc ImageNet dataset
%------------------

Computer Vision research has experienced a great boost in the work of ~\cite[deng2009imagenet] in the form of image database ImageNet. ImageNet contains full resolution images built into the hierarchical structure of WordNet, database of synsets, “synonym sets” linked into a hierarchy.

WordNet is often used as a resource in the tasks of natural language processing, such as word sense disambiguation, spellcheck and other. ImageNet is a project which tries to populate the entries of WordNet with imagery representation of given synset with accurate and diverse enough images illustrating the object in various poses, viewing points and with changing occlusion.

As the work suggests, with internet and social media, the available data is plenty, but qualitative annotation is not, which is why such hierarchical dataset like ImageNet is needed. The argument for choosing WordNet is that the resulting structure of ImageNet is more diverse than any other related database. The goal is to populate the whole structure of WordNet with 500-1000 high quality images per synset, which would roughly total to 50 million images. Even till today, the ImageNet project is not yet finished, however many following articles already take advantage of this database with great benefits.

Effectively ImageNet became the huge qualitative dataset which was needed to properly teach large CNN models.

\label[ch2_competition]
\secc ImageNet Large Scale Visual Recognition Competition
%------------------
The ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) ~\cite[Russakovsky2014] fueled the path of progress in the field of Image Recognition. While the tasks for each year slightly differs to encourage new novel approaches, it is considered the most prestigious  competition in this field. The victorious strategies, methods and trends of each years of this competition offer a reliable looking glass into the state of art techniques. The success of these works and fast rate of progress has lead to popularization of CNNs into more practical implementations as is the case of Japanese farmer automatizing the sorting procedure on his cucumber farm ~\cite[japanese-cucumber].

\secc AlexNet, CNN using huge datasets
%------------------

The task of object recognition has been waiting for larger databases with high quality of annotation to move from easier tasks done in relatively controlled environment, such as the MNIST ~\cite[lecun1998mnist] handwritten digit recognition task. In the work of ~\cite[krizhevsky2012imagenet] the ImageNet database was used to teach a Deep Convolutional Neural Network (CNN) in a model later referred to as AlexNet. At the time this method has achieved more than 10\% improvement over its competitors in an ILSVRC 2012 competition.

Given hardware limitations and limited time available for learning, this work made use of only subsets of the ImageNet database used in ILSVRC-2010 and ILSVRC-2012 competition. Choice of CNN as a machine learning model has been made with the reasoning that it behaves almost as well as fully connected neural networks, but the amount of parameters of connections is much smaller and the learning is therefore more efficient.

For the competition an architecture of five convolutional and three fully-connected layers composed of Rectified Linear Units (ReLUs) as neuron models was used. Other alterations on the CNN architecture were also employed to combat overfitting, to fine-tune and increase score and reflect the limitations of hardware. Output of last fully-connected layer feeds to a softmax layer which produces a probabilistic distribution over 1000 classes as a result of CNN.

The stochastic gradient descent was used for training the model for roughly 90 cycles through training set composed of 1.2 million of images, which took five to six days to train on two NVIDIA GTX 580 3GB GPUs.

\secc VGG16, VGG19, going deeper
%------------------
Following the successful use of CNNs in ILSVRC2012, the submissions of following years tried to tweak the parameters of CNN architecture. Approach chosen by ~\cite[vgg_simonyan2014very] stands out because of its success. It focused on increasing the depth of CNN while altering the structure of network.  In their convolutional layers they chose to use very small convolution filter (with 3x3 receptive field), which leads to large decrease of amount of parameters generated by each layer.

This allowed them to build much deeper architectures and acquire second place in the classification task and first place in localization task of ILSVRC 2014. Similar approach of going deeper with their CNN design was chosen by the works of ~\cite[inception_pre] and ~\cite[inception].

\secc ResNet, recurrent connections and residual learning
%------------------
The work of ~\cite[resnet_He_2016_CVPR] introduced a new framework of deep residual learning which allowed them to go even deeper with their CNN models. They encountered the problem of degradation, where accuracy was in fact decreasing with deeper networks. This issue is not caused by overfitting as the error was increased both in the training and validation dataset.

Alternative architecture of model, where a identity shortcut  connection was introduced between building blocks of the model, allowing it to combat this degradation issue and in fact gain better results with increasing CNN depth.

Their model ResNet 152 using 152 layers achieved a first place in the classification task of ILSVRC 2015.

\secc Ensemble models
%------------------

The state of the art models as of ILSVRC 2016 made use of the ensemble approach. Multiple models are used for the task and final ensemble model weights their contribution into an aggregated score.

The widespread of using the ensemble technique reflects the democratization and emergence of more platforms and public cloud computing solutions giving more processing power to the competing teams of ILSVRC.

\label[ch2_feature_transfer_research]
\secc Feature transfer
%------------------

The success of large CNNs with many parameters trained on large datasets like ImageNet has not only been positive, it also poses a question - will we always need huge datasets like ImageNet to properly teach CNN models? ImageNet has millions of annotated images and it has been gradually growing over time.

Article ~\cite[Oquab_2014_CVPR] talks about this issue and proposes a strategy called feature transfer or model finetuning, which consists of using CNN models trained at one task to be retrained to a different task.

They offer a solution, where similar architecture of CNNs can be taught upon one task and then several of its layers can be reused, effectively transferring the mid-level feature representations to a different task.

\midinsert
\clabel[3_feature_transfer]{Feature transfer illustrated}
\picw=13cm \cinspic figures/2.2.3._feature_transform.pdf 
\caption/f Illustration of the basic idea of feature transfer between source and target task. Imagine the source task being a classification task on ImageNet and the target task as a new problem without large dataset at its disposal.
\endinsert

This can be used, when the source task has a rich annotated dataset available (for example ImageNet), whereas the target one doesn’t. When talking about two tasks, the source and target classes might differ, when for example the labeling is different. Furthermore the whole domain of class labels can be also different - for example two datasets of images, where first mostly exhibits single objects and the second one rather contains more objects composed into scenes. This issue is referred to as a “dataset capture bias”.

The issue of different class labels is combated by adding new adaptation layers which are retrained on the new set of classes. The problems of different positions and distributions of objects in image is addressed by employing a strategy of sliding window decomposition of the original image, continuing with sensible subsamples and finally having the result classifying all objects in the source image separately.

The article also works with a special target dataset Pascal VOC 2012 containing difficult class categories of activities described like “taking photos” or “playing instrument”. In this case the source dataset of ImageNet doesn’t contain labels which could overlap with these activities, yet the result of this “action recognition” task achieves best average precision on this dataset.

This article gives us hope, that we can similarly transfer layers of Deep CNN trained on the ImageNet visual recognition source task to a different target task of Google Street View imagery analysis for cost estimation over each edge segment.

Study of ~\cite[decaf] also explores the feature transfer technique, essentially using an already taught CNN as a black box feature extractor for target task.

Articles being submitted to the ILSVRC competition often include a section dedicated on using their designed models on different tasks than what they were trained upon. Effectively they test the models suitability for feature transfering.

Object localization is an example of typical target task, where a large qualitative dataset is not available, yet where the feature transfer technique brings in good results in works of ~\cite[OverFeat_localization] and  ~\cite[Regions_with_CNN].

%\maybebreak 5pt

\secc Common structures
%------------------

When designing the architecture of custom CNN model, we are using certain layers and building block schemes established as common practice in model building and in the ILSVRC competition. For a new unresearched task it has been suggested (by lecturers and online sources such as ~\cite[cnn_course]) to stick to an established way of designing the overall architecture.

\midinsert
\clabel[4_structure]{Generic CNN architecture formula}
\picw=13cm \cinspic figures/2.2.4.building_blocks_with_blocks.pdf 
\caption/f Formula describing a generic CNN architecture used with image data.
\endinsert

Refer to Figure \ref[4_structure] for illustration of this recommended architecture. Naturally for custom tasks this architecture is later adapted and tweaked to serve well in its specific situation. We will return to this suggested architecture scheme when building our own custom CNN models in \ref[ch4_building_blocks].

\label[ch2_related_works]
\sec Related works
%===================

\secc Practical application of CNNs
%------------------

We can find practically applied CNNs on the task of object recognition in several highly specialized datasets, such as plant identification in ~\cite[deep_plant] and bird species categorization in ~\cite[bird_species]. Both of these works make use of the feature transfer and later CNN fine-tuning. They note the problems of over-fitting on dataset dramatically smaller than the source dataset, which need to be addressed.

Work of ~\cite[wegner2016cataloging] uses R-CNN to automate the process of object categorization at specified geological location for the purpose of cataloging. They experiment on small scale area with plans to go US-wide and then cover the whole planet. Objects like trees, lamp posts, mailboxes, traffic lights etc. can be targeted. Main focus is on the automation of processes which otherwise require a lot of manual labor.

\secc Geolocation
%------------------

Large research field is the so called geolocation, assigning a location to images. Instead of answering the “what” of object identification task, we are asking “where” and in the work of ~\cite[matzen2014scene_chronology] eventually also “when”. Article ~\cite[leung2010proximate] names this task “proximate sensing” using an extensive geolocation referrenced dataset collected from social networks, blogs and other not-reviewed data collections. For comparison they also use Geograph British Isles photograph dataset which was taken with the intention to objectively represent the area of Great Britain and Ireland. They note the influence of photographers intent on the usability of images for datasets for machine learning models and mention the necessity of filtration. We may also note the work of ~\cite[netvlad_cnn] which also deals with the geolocation tasks by the means of CNN model with custom top “NetVLAD” layer.

On the topic of processing publicly available photographs we have the study ~\cite[mapping_the_world] which focuses on analysis of where the images are taken by producing heatmaps and lists of landmarks ordered by the quantity of photographs and their viability as image datasets. Similarly the study of ~\cite[Quack_world_mining_community] focuses on mining the community created geotagged images. They cluster sets of publicly available images of the same subject.

Paper ~\cite[cross_view_lin2013cross] makes use of aerial imagery with the street level photography in cross-view manner. They explore the intermediate relational attributes such as the quality of neighborhoods to circumvent the lack of ground level images in certain scenarios. Their algorithm produces a probability distribution of localization over limited map segment. Work of ~\cite[ground_to_aerial_Lin_2015_CVPR] also discusses the problems associated with matching the two datasets of aerial and ground level images.

Note that these datasets could be used as alternative data sources even for our task - instead of just using downloaded images from Street View Image, we could download a set of representative images for nearby area, perhaps getting the feel for the neighborhood. Neighborhoods of those edges with high scores could be a guide of what we are looking for in our bicycle route planning.

As for the problem of temporal localization on top of the task of geolocation, the work of ~\cite[matzen2014scene_chronology] explores the scene chronology and task of time-stamping photos. It focuses on presence of temporary objects, such as poster, signs, street art, etc. The presence of temporal information is usually ignored, which produces what is described as \textit{time chimeras} of objects from different times placed and used together. This is an issue in the case of 3D scene reconstruction from images rather than in our case, however it should be noted that Google Street View images do carry a time-stamp. For most precise information we may prefer the most recent imagery data, however certain locations carry older photographs. A source of possible problems would also be a sudden appearance of images from different time of the year, such as images from Summer while using a dataset from images taken during Winter.

\secc Google Street View
%------------------

We have already mentioned using the Google Street View service ~\cite[anguelov2010google_street_view] to obtain images applicable for our task. There has been extensive research over the application of Neural Networks on tasks regarding the Google Street View itself, such as in ~\cite[privacy_protection] where multiple classifiers are used to identify and blur out human faces and license plates on the recorded images for the sake of privacy protection.

Data collected from Google Street view service has also been used in ~\cite[goodfellow2013multidigit] for the task of digit recognition. Using Street View House Numbers (SVHN) dataset introduced by ~\cite[nips2011_housenumbers] as well as an internal dataset generated from Google Street View imagery, they achieve almost human operator grade performance on digit recognition as well as on reCAPTCHA reverse turing test challenge.

The tool of Google Street View is also being used as a cheap method of in-field visits for social observations in ~\cite[odgers2012systematic] or in ~\cite[rundle2011using]. This study measures the applicability of using Street View service for cheap location auditing. While this study needed human intervention in both its generated datasets, an in-person field audit and Street View human operated audit, it is of interest to us. While some labels cannot be conclusively obtained just from the imagery data, other environmental characteristics are easily recognized. Measures of pedestrian safety, such as speed bumps, infrastructure object of public transportation systems and parking spaces could be discovered on the images. This study gives us an idea of what kind of information can be concluded from Street View images by human evaluators.

Another work of ~\cite[scene-completion2007] makes use of large datasets of Google Street View like images for the task of missing masked out section of photograph recreation, by estimating inter-image similarity and patch of image transplantation suitability.

\label[ch3]
\chap Task
%==============================================================

\sec Route planning for bicycles 
%===================

The task we are faced with consists of planning a route for bicycle on a map of nodes and edges. We are designing an evaluation method, which will give each edge segment appropriate cost. In such a way we are building one part of route planner, which will use our model for cost evaluation and fit into a larger scheme mentioned in \ref[ch2_planning]. We are building a machine learning model, such as the one jokingly described in xkcd comic strip ~\cite[xkcd_machine_learning].

As has been stated in \ref[ch2_data_collection] a cost function can be an explicitly defined formula depending on many measured variables. Similar formula has been used by the ATG research group, which produced a partially annotated section of map with scores of bike attractivity. We want to enrich this dataset with additional visual information from Google Street View and with vector data from Open Street Map.

We want to train a model on the small annotated map segment and later use it in areas where such detailed information is not available. We argue that Google Street View and Open Street Map data are more readily obtainable, than supply of highly qualitative measurements.

\label[ch3_available_data]
\sec Available imagery data 
%===================

\label[ch3_initial_dataset]
\secc Initial dataset
%------------------

We are given a dataset from the ATG research group of nodes and edges with score ranking ranging from 0 to 100. Score of 0 denotes, that in simulation this route segment was not used and value 100 means that it was a highly attractive road to take. We rescale these to the range of 0 to 1.

\midinsert
\clabel[5_geojson]{Sample of the initial dataset}
\picw=11cm \cinspic figures/3_example_geojson_data.pdf
\caption/f Sample of the structure of initial dataset.
\endinsert


Each node in supplied with longitude, latitude location, which gives us the option to enrich them with additional real world information. Figure \ref[5_geojson]. shows the structure of initial data source.
	

\secc Google Street View
%------------------

As each edge segment connecting two nodes is representing a real world street connecting two crossroads, we can get additional information from the location. We can download one or more images alongside the road and associate it with the edge and it’s score from the initial dataset.

We are using a Google Street View API which allows us to generate links of images at specific locations and facing specific ways.

\label[ch3_downloading_street_view_images]
\secc Downloading Street View images
%------------------

Google Street View API uses the parameters of location which is the latitude and longitude and heading, which is the deviation angle from the North Pole in degrees. See Figure \ref[6_google_api]. In calculation of heading we are making a simplification of Earth being spherical using formula for initial bearing in Figure \ref[6_bearing].

\midinsert
\clabel[6_bearing]{Initial bearing formula}
\picw=10cm \cinspic figures/3.2.2.1._formula_bearing.pdf 
\caption/f Formula for calculation of initial bearing when looking from one location (lon$_{1}$, lat$_{1}$) to another (lon$_{2}$, lat$_{2}$).
\endinsert

\midinsert
\clabel[6_google_api]{Google Street View API url generation}
\picw=10cm \cinspic figures/3.2.2.1._google_api-get_svg_v2.pdf 
\caption/f Illustration of Google Street View API url generation
\endinsert

In order to make good use of the location and collect enough data, we decided to break down longer edges into smaller segments maintaining the minimal edge size fixed. We also select both of the starting and ending locations of each segment. In each position we also rotate around the spot.

We collect total of 6 images from each segment, 3 in each of its corners while rotating 120\degrees degrees around the spot. This allows us to get enough distinct images from each location which don’t overlap with neighboring edges. See the illustration in Figure \ref[7_edge_breakdown]. Note that all of these images will correspond to one edge and thus to one shared score value.

Also note that we are limited to downloading images of maximal size 640x640 pixels as per the limitation of free use of Google Street View API.

\midinsert
\clabel[7_edge_breakdown]{Edge splitting and image generation scheme}
\picw=13cm \cinspic figures/3.2.2.1._breakdown_of_edge.pdf 
\caption/f Splitting of initial possibly large edge segments into sections not smaller than the minimal length limit for edge segment. Each of these generates six images, which are usually not overlapping even with neighboring edges.  
\endinsert

In CNNs we often use the method of data augmentation to extend datasets by simple image transformations to overcome limitations of small datasets. We can generate crops of the original images, flip and rotate them or alter their colors.

We will return to the issue of dataset augmentation in \ref[ch4_data_augmentation].

\label[ch3_neighborhood_in_osm]
\sec Neighborhood data from Open Street Map
%===================

We are looking for another source of information about an edge segment in the neighborhood surrounding its location in Open Street Map data.

Open Street Map data is structured as vector objects placed with location parameters and large array of attributes and values. We can encounter point objects, line objects and polygon objects, which represent points of interest, streets and roads, buildings, parks and other landmarks.

\midinsert
\clabel[8_ex_params]{OSM data structure with parameters}
\picw=13cm \cinspic figures/3.2.3._osm_map_with_params.pdf 
\caption/f Example of structure of OSM data with parameters showing its properties.
\endinsert

From implementation standpoint, we have downloaded the OSM data covering map of our location and loaded it into a PostgreSQL database. In this way we can send queries for lists of objects in the vicinity of queried location. We will get into more detail about implementation in \ref[ch5_osm_marker].

\label[ch3_osm_vector]
\secc OSM neighborhood vector
%------------------

The structure of OSM data consists of geometrical objects with attributes describing their properties. In the PostgreSQL database each row represents object and attributes are kept as table columns.

Depending on the object type, different attributes will have sensible values while the rest will be empty. For better understanding consult Table \ref[8_osm_interesting_attributes] with examples of attributes and their values and Table \ref[8_osm_objects]. for examples of objects in OSM dataset.

\midinsert
\clabel[8_osm_interesting_attributes]{Sample of OSM attributes}
\picw=13cm \cinspic tables/3.6._attributes_and_their_vals.pdf 
\captiontablehack/f Sample of interesting attributes and their possible values in OSM dataset.
\endinsert

For example attribute “highway” will be empty for most objects, unless they are representing roads, in which case it reflects its importance and size. We will be interested in counting attribute-value pairs, for example the number of residential roads in the area, which will have “highway=residential”.

\midinsert
\clabel[8_osm_objects]{Sample of OSM objects}
\picw=9cm \cinspic tables/3.7._example_of_objects_in_osm.pdf 
\captiontablehack/f Example of values given to a selection of objects from OSM dataset.
\endinsert

Out of these pairs, we can build a vector of their occurrences. The only remaining issue is to determine which attribute-value pairs will we select into our vector. If we used every pair possible, the vector would be rather large and more importantly mostly filled with zeros.

In order to select which pairs are important, we chose to look at OSM data statistics available at webpage \textit{taginfo.openstreetmap.org} ~\cite[taginfo]. From an ordered list of most commonly used attribute-value pairs, we have selected relevant pairs and generated our own list of pairs which we consider important.

\midinsert
\clabel[9_ex_loc]{Unique locations with neighborhood vectors}
\picw=15cm \cinspic figures/3.2.3.1_locations_with_table.pdf 
\caption/f Four unique locations with their corresponding neighborhood vectors. Note that these can be very similar for close enough locations.
\endinsert

Then for each distinct location of edge segment, we look into its neighborhood and count the number of occurrences of each pair from the list. See Figures \ref[9_ex_loc] and \ref[10_vec_count].

\midinsert
\clabel[10_vec_count]{Construction of neighborhood vector}
\picw=11cm \cinspic figures/3.2.3.1_construction_of_osm_vec.pdf 
\caption/f The construction of neighborhood vector from collection of nearby objects.
\endinsert

Each distinct location of each edge will end up with same sized OSM vector marking their neighborhood. Note that due to the method of downloading multiple images per one edge, for example by using the same location and rotating around the spot, some of these will have the same OSM vectors.

\midinsert
\clabel[10b_six_imgs]{Generation of data entries from edge segment}
\picw=10cm \cinspic figures/3.2.3.1_6images_and_their_shared_data.pdf 
\caption/f Example of further not divided edge segment, which generates data entries, where certain values overlap. For example the score is shared among all images produced from one segment.
\endinsert

One further undivided edge segment contains 6 images which share the same score, and some of which will share location and therefore also their OSM vectors. For illustration see Figure \ref[10b_six_imgs].

\secc Radius choice
%------------------

Depending on the radius choice, different area will be considered as neighborhood. If we were to choose too small radius, the occurrences would mostly result in zero OSM vector. On the other hand selecting too high radius would lead many OSM vectors to be indistinguishable from each other as they would share the exact same values.

Experimentally we have found radius of 100 meters to be effective.
%Our final choice for radius was value of 100 meters. %, which was empirically tested in \ref[ch6] Results.

\secc Data transformation
%------------------

Similar to the spirit of data augmentation for images, we can try editing the OSM vectors in order that they will be more easily used by CNN models.

Instead of raw data of occurrences, we can convert this information into one-hot categorical representations or reduce them into Boolean values. Multiple readings of varying radius size can also be used for better insight of the neighborhood area.

See more about data augmentation in \ref[ch4_data_augmentation].

\label[ch4]
\chap Method
%==============================================================

Our method will rely upon using Convolutional Neural Networks (CNNs) mentioned in \ref[ch2_history_of_cnn] on an annotated dataset described in \ref[ch3]. As we have enriched our original dataset with multiple types of data, particularly imagery Street View data and the neighborhood vectors, we have an option to build more or less sophisticated models, depending on which data will they be using. We can build a model which uses only relatively simple OSM data, or big dataset of images, or finally the combination of both.

Depending on which data we choose to use, different model architecture will be selected. Furthermore we can slightly modify each of these models to tweak its performance.

\label[ch4_building_blocks]
\sec Building blocks 
%===================

Regardless of the model type or purpose, there are certain construction blocks, which are repeated in the architecture used by most CNN models.

\label[ch4_model_abstraction]
\secc Model abstraction
%------------------

When building a CNN model, we can observe an abstracted view of such model in terms of its design. Whereas at the input side of the model we want to extract general features from the dataset, at the output side we strive for a clear classification of the image. See Figure \ref[11_extractor_class].

\midinsert
\clabel[11_extractor_class]{Feature extractor and classificator}
\picw=13cm \cinspic figures/4.1.1_model_abstraction.pdf 
\caption/f An abstraction of CNN model into feature extractor section and classificator section.
\endinsert

Each of these segments will require different sets of building blocks and will prioritize different behavior. Good model design will lead to generalization of concrete task-specific data into general features, which will then be again converted into concrete categories or scores. The deeper the generic abstraction is, the better the model behavior in terms of overfitting will be.

Classification segment transforms the internal feature representation back into the realm of concrete data related to our task. In our tasks we are interested in score in range from 0 to 1 as illustrated by Figure \ref[12_sigmoid_output].

\midinsert
\clabel[12_sigmoid_output]{Classificator section formula}
\picw=13cm \cinspic figures/4.1.1._output_formula.pdf 
\caption/f Classificator section formula describing how we generate one value at the end of CNN model ranging from 0 to 1 as a score estimate.
\endinsert

\label[ch4_fully_connected_layers]
\secc Fully-connected layers
%------------------

The fully-connected layer denoted as “Dense” in Keras stands for a structure of neurons connected with every input and output by weighed connections. In Neural Networks these are named as hidden layers.

\midinsert
\clabel[13_fc]{Fully connected layer}
\picw=13cm \cinspic figures/4.1.2._fully-connected.pdf 
\caption/f Shows the model of connections of neurons in fully connected layer.
\endinsert

The fully-connected layer suffers from a large amount of parameters it generates: weights in each connection between neurons and biases in individual neuron units. Fully-connected layers are usually present in the classification section of the model.

\secc Convolutional layers
%------------------

\midinsert
\clabel[14_conv]{Convolutional layer}
\picw=10cm \cinspic figures/4.1.3_convolutional_layer.pdf 
\caption/f Schematic illustration of connectivity of convolutional layer. Connections are limited to the scope of receptive field.
\endinsert

Convolutional layers are trying to circumvent the large amount of parameters of fully-connected layers by localized connectivity. Each neuron looks only at certain area of the previous layer.

For their property to use considerably less parameters while at the same time to focus on features present in particular sections of image, they are often used as the main workforce in the feature extractor section of CNN models.

\secc Pooling layers
%------------------

Pooling layers are put in between convolutional layers in order to decrease the size of data effectively by downsampling the volume. This forces the model to reduce its number of parameters and to generalize better over the data. Pooling layers can apply different functions while they are downsamplig the data – max, average, or some other type of normalization function.

\midinsert
\clabel[15_pool]{Pooling layer}
\picw=10cm \cinspic figures/4.1.4._pooling_layer.pdf 
\caption/f Pooling layer structure effectively downsampling the volume of input data.
\endinsert

\label[ch4_dropout_layers]
\secc Dropout layers
%------------------

Dropout layer is special layer suggested by ~\cite[srivastava2014dropout]. It has become widely used feature in the design of CNN architectures as a tool to prevent model overfitting.

\midinsert
\clabel[16_drop]{Dropout layer}
\picw=8cm \cinspic figures/4.1.5._dropout.pdf 
\caption/f Illustration of dropout layers effect during the training period, which renders certain connection invalid with set probability. This prevents the network to depend too much on overly complicated formations of neurons.
\endinsert

The dropout layer placed between two fully-connected layers functions randomly drops connections between neurons with certain probability during the training period. Instead of fully-connected network of connections we are left with a thinned network with only some of the connections remaining. This thinned networks is used during training and prevents neurons to rely too much on co-adaptation. They are instead forced to develop more ways to fit the data as there is the effect of connection dropping. During test evaluation, the full model is used with its weights multiplied by the dropout probability.


\label[osm_model]
\sec Open Street Map neighborhood vector model 
%===================

In this version of model, we broke down edge segments formerly representing streets in real world into regularly sized sections each containing two locations of its beginning and ending location. These locations were enriched with OSM neighborhood vectors in \ref[ch3_neighborhood_in_osm].
	
\midinsert
\clabel[17_one_edge_to_many_loc]{Distinct locations from edge segment}
\picw=12cm \cinspic figures/4.2._edge_progress.pdf 
\caption/f Edge segment broken down into set of distinct locations, where each of these locations is assigned its own neighborhood vector.
\endinsert

Single unit of data is therefore a neighborhood vector linked to each distinct location of the original dataset. We have designed a model which takes these vectors as inputs and scores as outputs.

The \textit{OSM model} is built from repeated building blocks of fully-connected layer followed by dropout layer. Fully connected layer of width 1 with sigmoid activation function is used as the final classification segment. Figure \ref[18_osm_model] shows the model alongside its dimensions.

\midinsert
\clabel[18_osm_model]{OSM neighborhood vector CNN model}
\picw=15cm \cinspic figures/4.2._model.pdf 
\caption/f CNN model making use of only the neighborhood OSM vector data.
\endinsert

\label[img_model]
\sec Street View images model 
%===================

Each edge segment is represented by multiple images captured via the Street View API. Images generated from the same edge segment will share the same score label, however the individual images will differ. We can understand one image-score pair as a single unit of data.

The image data can be augmented in order to achieve richer dataset, see \ref[ch4_data_augmentation].

As discussed in \ref[ch2_history_of_cnn] we are using a CNN model which has been trained on ImageNet dataset. We reuse parts of the original model keeping its weights and attach a new custom classification segment architecture at the top of the model.

\midinsert
\clabel[19_base_top]{Reused base model with custom top model}
\picw=12cm \cinspic figures/4.3._feature_transfer.pdf 
\caption/f Reusing part of already trained CNN model as a base model and adding our custom top model.
\endinsert

We can generally divide even the more complicated CNN models into two abstract segments as mentioned in \ref[ch4_model_abstraction]. The beginning of the model, which usually composes of repeated structure of convolutional layers is the base model, followed by a classification section usually made of fully-connected layers.

The former works in extracting high dimensional features of incoming imagery data, whereas the classification section translates those features into a probability distribution over categories or score. In our case we are considering a regression problem model, which works with score instead of categories.

As was mentioned in \ref[ch2_feature_transfer_research] we reuse the model trained on large dataset of ImageNet, separate it from its classifier and instead provide our own custom made top model. See Figure \ref[19_base_top].

We prevent the layers of base model from changing their weights and train only the newly attached top model for the new task. There are certain specifics connected with this approach which we will explore in \ref[ch4_feature_cooking] section.

\label[ch4_model_architecture]
\secc Model architecture
%------------------

The final model architecture is determined by two major choices: which CNN to choose as its base model and how to design the custom top model so it’s able to transfer the base model's features to our task.

\secc{Base model}
%------------------

The framework we are working with, Keras, allows us to simply load many of the successful CNN models and by empirical experiments asses, which one is best suited for our task. More about Keras in the appropriate section \ref[ch5_keras].

The output of the base CNN model is data in feature space. The dimension will vary depending on the type of the model we choose, the size of images we feed the base model and also the depth in which we chose to cut the base CNN model.

The remains of base CNN model are followed by a Flatten layer which converts the possibly multidimensional feature data into a one dimensional vector, which we can feed into the custom top model.

\midinsert
\clabel[20_diff_in_diff_out]{Dimensionality of feature vectors}
\picw=14cm \cinspic figures/4.3.1.1_different_sized_imgs.pdf 
\caption/f Example of differently sized images on the input which result in different feature vector size.
\endinsert

Base CNN models in Keras are loaded while specifying their "input_shape". The description files for these models are adjustable, they are defined by a sequence of interconnected layers. Without going deeper into the syntax of Keras (like we later do in \ref[ch5_keras]), it's worth noting that the size of input of each layer will influence the size of its output. For example pooling layers function effectively in downsampling their input, so their output size is therefore influenced by input dimension. Some other layers can have fixed output sizes. Where this matters is in the moment of joining base CNN model with later custom top model. We need to be prepared for the dimension of features captured at the output of base model to be influenced by the size of its inputs. As we can see on example of Figure \ref[20_diff_in_diff_out], this can lead to vastly different sizes of feature vectors.

\label[ch4_custom_top]
\secc{Custom top model}
%------------------

We feed the feature vector into a custom model built from repeated blocks of fully-connected neuron layers interlaced with dropout layers. The number of neurons used in each of the layers influences the so called model “width” and the number of used layers influences the model “depth”. Both of these attributes influence the amount of parameters of our model. We can try various combinations of these parameters to explore the models optimal shape.

The final layer of the classification section consists of fully-connected layer of width 1 with sigmoid activation function which weighs in all neurons of the previous layer. See Figure \ref[21_custom_top]. Note that in the final model we chose to interlace individual fully-connected layers with dropout layers.

\midinsert
\clabel[21_custom_top]{Top model structure}
\picw=14cm \cinspic figures/4.3.1.2_feature-score.pdf 
\caption/f Top model structure which takes in the feature vector and follows with fully connected layers which are retrained on our own task. Note that in our case we also place dropout layers in between the fully connected layers.
\endinsert

\maybebreak 5pt

\secc{The final architecture}
%------------------

The final architecture in Figure \ref[22_img_model] consists of base CNN model with its weights trained on the ImageNet dataset and of custom classification top model trained to fit the base model for our task.

\midinsert
\clabel[22_img_model]{Image model structure}
\picw=15cm \cinspic figures/4.3.1.3._img_model.pdf 
\caption/f Final schematic representation of CNN model using Google Street View images. Note that parameters of width and depth can change its performance.
\endinsert

\label[mix_model]
\sec Mixed model
%===================

After discussing the architecture of two models making only a partial use of the data collected in our dataset, we would like to propose a model combining the two previous ones.

In this case one segment again generates multiple images, which share the same score and depending on how they are created they could also share the same neighborhood OSM vector representing the occurrences of interesting structures in its proximity. Different edges will generate not only different images, but also different scores and OSM vectors.

\midinsert
\clabel[23_single_data_unit_for_mix]{Structure of mixed model input data}
\picw=14.5cm \cinspic figures/4.4._mixed_data_repetition.pdf 
\caption/f Example of possible decomposition of original edge into rows of data accepted by the Mixed model. We use both the image data and the OSM neighborhood vectors.
\endinsert

As a single unit of data we can consider the triplet of image, OSM vector and score. It’s useful to note that later in designing the evaluation method of models, we should take into account, that the neighborhood vector and score can be repeated across data. When splitting the dataset into training and validation sets, we should be careful and place images from one edge into only one of these sets. Otherwise data with distinct images, but possibly the same neighborhood vector and score could end up in both of these sets. Figure \ref[23_single_data_unit_for_mix] illustrates how single data units are generated from one edge.

We can join the architectures designed in previous steps, or we can design a new model. We chose to join the models in their classification segment. We propose a basic idea for a simple model architecture, which concatenates feature vectors obtained in previous models and follows with structure of repeated fully-connected layers with dropout layers in between. Concatenation joins the two differently sized one dimensional vectors into one. As is observable on Figure \ref[24_mix_model] we use several parameters to describe the models width and depth.

\midinsert
\clabel[24_mix_model]{Mixed model structure}
\picw=15cm \cinspic figures/4.4._mixed_model_v2.pdf 
\caption/f Final architecture of the Mixed model, which uses both OSM vector and the image data.
\endinsert


\label[ch4_data_augmentation]
\sec Data Augmentation 
%===================

When using complicated models with high number of parameters on relatively small datasets, the danger of overfitting is always present. We would like to combat this by expanding our dataset with the help of data augmentation.

Overfitting occurs when the model is basically able to remember all the samples of the training dataset perfectly and incorporate them into its structure. It achieves very low error on the training data, but looses its ability to generalize and results in comparably worse results on the validation set.

The idea of data augmentation is to transform the data we have in order to get more samples and a model which in turn behaves better on more generalized cases.

This cannot be done just blindly, as some of these transformations could mislead our model (for example left to right vertical flip makes sense in our case, but a up side down horizontal flip wouldn't).

There are multiple ways we can approach the problem of generating as many images from our initial dataset as we can. Before getting to the data augmentation aspect, please note, that this is also the reason, why we are generating multiple images per segment. We stand in two corners of each edge segment and rotate 120\degrees degrees to get three images on each side. We have also employed a technique, which splits long edges into as many small segments as possible, while not hitting the self imposed minimal edge length.

It could be debated, that we could rotate for smaller angle or split edges to even smaller segments in order to take advantage of the initial dataset fully. However we came across an issue, that with too small minimal edge length or with different rotation scheme, we obtain very similar images, which actually do not improve the overall performance. This occurs when we don't generate differing enough images during downloading. For actual performance change see chapter \ref[ch6_dataset_longedgesplit].

We face similar issue when choosing a radius for obtaining OSM neighborhood vector as specified in the section \ref[ch3_neighborhood_in_osm]. Instead of selecting one particular radius setting, we can make use of results of multiple queries. When building the OSM vector we would effectively multiply its length by concatenating it with other versions of OSM vectors. We could concatenate the vectors acquired with one fixed radius setting with another version with different radius. See Figure \ref[25_osm_multiple_radius_aug] for illustration.

\midinsert
\clabel[25_osm_multiple_radius_aug]{Multiple OSM vector use}
\picw=14cm \cinspic figures/4.5._data_augmentation_osm.pdf 
\caption/f Illustration of the construction of combined OSM vector and the expected result of more specific area targetting by the model.
\endinsert

	Finally we also come across the method of data augmentation by transformation of the original image dataset. Certain operation, such as vertically flipping the image make sense for our dataset.

	We show an example of images undergoing such transformation on Figure \ref[26_image_augmentation_flips]. Note that in this particular example we chose vertical flipping alongside with shifting the 90\% of the image while making up for the lost 10\%. These operations are random and the resulting images are added to create a larger dataset. For the sake of repetition of experiments with the same data, we save these generated images into an expanded dataset.

\midinsert
\clabel[26_image_augmentation_flips]{Image data augmentation}
\picw=10cm \cinspic figures/4.5._data_augmentation_folder.jpg
\caption/f Data augmentation example.
\endinsert

\label[ch4_model_training]
\sec Model Training 
%===================

\secc Data Split 
%------------------

Traditionally we split our dataset into two sets – training set, which we use for training of model and so called validation set, which is used only for models performance evaluation. As we discuss in \ref[ch4_model_evaluation], we employ more complex strategy of k-fold cross-validation test to obtain more precise results.

The difference between the error achieved on training data and on validation data can be used as a measure of our model overfitting.

\secc Settings 
%------------------

We are using backpropagation algorithm to train our models as is supported by the selected Keras framework. We can choose from a selection of optimizers, which control the learning process. We made use of the more automatic optimizers supported by Keras, such as "rmsprop" ~\cite[rmsprop_tieleman2012lecture] and "adam" (see ~\cite[adam_optimizer] where Adam has been tested as an effective CNN learning optimizer). For greater parametric control we can also select the SGD optimizer.

Given the nature of our task, we are solving a regression problem, trying to minimize deviation from scored data. In most models mentioned in \ref[ch2_competition], the task instead revolves around selecting the correct category to classify objects.

Accordingly we have to choose appropriate loss function. We have selected the mean squared error metric ~\cite[wang2009mean] with the formula given by Figure \ref[27_mse_formula].

\midinsert
\clabel[27_mse_formula]{Mean squared error metric}
\picw=8cm \cinspic figures/4.6.2_mse_formula.pdf 
\caption/f Mean squared error metric used as loss function when training models.
\endinsert

\secc Training stages 
%------------------

As has been discussed in \ref[ch4_model_architecture], with image data we are building models that reuse base of other already trained CNN models. In our case we make use of weights loaded from model trained on the ImageNet dataset.

We attach a custom classifier section to a base model and try to train it on a new task. In order to preserve the information stored in connections of the base model, we lock its weights and prevent it from retraining. The only weight values which are changing are in the custom top model. This can be understood as a first stage of training the model as illustrated on Figure \ref[28_training_stages].

\midinsert
\clabel[28_training_stages]{Training stages schematics}
\picw=10cm \cinspic figures/4.6.3_training_stages.pdf 
\caption/f Training stages illustrated for feature transfer approach.
\endinsert

This is sometimes followed with a finetunning period, where we unlock certain levels of the base CNN model for training. However this is commonly done with customized optimizer setting, so that the changes to the whole model weights are not too drastic. We are also usually not retraining the whole model, because of the computational load this would take.

\label[ch4_feature_cooking]
\secc Feature cooking 
%------------------

This method is specific to situation when we are training a model with parts, which are frozen, as is in the case of \ref[img_model] image and \ref[mix_model] mixed model design. We can take an advantage of the fact that certain section of the model will never change and precompute the image features for a fixed dataset.

In this way we can save computational costs associated with training the model. In the end the original model can be rebuilt by loading obtained weight values.

This allows for fast prototyping of the custom top model, even if the whole model composes of many parameters in the frozen base model. For illustration see figure \ref[29_reusing_features].

\midinsert
\clabel[29_reusing_features]{Reusing saved image features from a file}
\picw=14cm \cinspic figures/4.6.4_feature_cooking.pdf 
\caption/f Reusing saved image features from a file instead of costly computations.
\endinsert

\label[ch4_model_evaluation]
\sec Model evaluation 
%===================

As was mentioned in \ref[ch4_model_training] we are using the practice of splitting dataset into training and validation dataset, with the k-fold cross-validation technique.

In order to prevent from being influenced by the selection bias, we split our entire dataset into k folds and then in sequence we use these to build training datasets and validation datasets. Every fold will take role of validation set for a model, which will train on data composed from all the remaining folds. Each of these will run a full training ended by an evaluation giving us score. Eventually we can calculate the average score with standard deviation.

This approach obviously increases the computational requirements, because it repeats the whole experiment for each fold. It is not used while prototyping models, but as a reliable method to later generate score. We have chosen the number of folds to be 10, as is a traditionally recommended approach.

\midinsert
\clabel[30_k_fold]{K-fold cross-validation}
\picw=11cm \cinspic figures/4.7._k_fold_cross_validation.pdf 
\caption/f Splitting schema employed in the k-fold cross-validation. We are given average score alongside with its standard deviation.
\endinsert

\label[ch5]
\chap Implementation
%==============================================================

\sec Project overview 
%===================

In planning of the composition of project code, we have somewhat separated the sections responsible for downloading the data, from those managing the dataset and modeling and running experiments.

Downloader is tasked to download images from Google Street View API to enhance the initial dataset of edges and nodes mentioned in \ref[ch3_initial_dataset]. See \ref[ch5_downloader] for the Downloader functionality description. Segment object works as a unit holding information about edge and its corresponding images and score.

However for later processing of the data, we have created a DatasetHandler which contains all the necessary functions. See \ref[ch5_dataset_handler].

To run more instances of models and later evaluate them, we have chosen to build individual experiments from custom written setting files in \ref[ch5_settings]. Experiment Runner provides the common framework for all these more complicated computations in \ref[ch5_experiment_runner]. In Settings folder we hold setting files defining each experiment.

\midinsert
\clabel[31_overview]{Project structure overview}
\picw=11cm \cinspic figures/5.1._project_overview.pdf 
\caption/f Project structure in a schema.
\endinsert

\label[ch5_downloader]
\sec Downloader functionality 
%===================

The Downloader is the main method of acquiring imagery data and preparation of the dataset. It first creates necessary folders according to the directory path and custom name we provide it with.

"Defaults.py" contains default settings for the downloader, such as default number of times the code should try downloading each image and internal codes with which to mark unsuccessfully downloaded segments. Besides these internal representation settings, it also controls the pixel size of downloaded images.

\midinsert
\clabel[code1_downloader]{RunDownload}
\picw=13cm \cinspic pseudocodes/5.3._downloader.pdf
\caption/t RunDownload code sample.
\endinsert

Downloading procedure is initiated in "RunDownload()" method (see Code \ref[code1_downloader]), which parses node and edge data in provided GeoJSON files and builds an array of Segments.

For each segment in this array, we generate list of images to download. Long segments will be split by interpolating the start and end location to create smaller edge sections (see Code \ref[code3_longedges]). We generate a url link in format corresponding to Google Street View API presented in Code \ref[code2_googleapi] alongside with an unique file name.

\midinsert
\clabel[code2_googleapi]{Google Street View API url}
\begtt
http://maps.googleapis.com/maps/api/streetview?size=<width>x<height>
        &location=<lat>,<long>&heading=<angle from north>&key=<api>
\endtt
\caption/t Exact Google Street View API url we need to generate
\endinsert

\midinsert
\clabel[code3_longedges]{Break down long edges}
\picw=13cm \cinspic pseudocodes/5.3._break_down_long_edge.pdf
\caption/t Code segment which breaks down long edges and generates lists of urls and filenames Segment.getGoogleViewUrls().
\endinsert

One by one we try to download each of these images, marking segments with error flag, if we were unable to access them. The code attempts to retry each request for default number of times, to prevent temporary instability of network to hinder the downloading process.

We also check for invalid images in areas where Google Street View doesn’t have any data available, see Figure \ref[32_noimagery].

\midinsert
\clabel[32_noimagery]{No imagery available on Google Street View}
\picw=6cm \cinspic figures/sorry_no_imagery_here.jpg 
\caption/f Location outside of streets with available photographic imagery.
\endinsert


Eventually we save the pickled array into file for further processing. Folder structure is as follows in Figure \ref[code4_folder_struct].

\midinsert
\clabel[code4_folder_struct]{Dataset folder structure}
\begtt
dataset folder: 5556x_example_dataset_299px/
SegmentsData.dump
images/0000_0.jpg
images/0000_1.jpg
…
images/5546_5.jpg
\endtt
\caption/t Folder structure of dataset.
\endinsert

Function "RunCheck()" in \ref[code5_downloader_check] allows us to load previously downloaded Segments file and check for erroneous data - for example in case of sudden network failure we can download only the missing files and then save the fixed Segments file.

\midinsert
\clabel[code5_downloader_check]{Check downloaded dataset}
\picw=13cm \cinspic pseudocodes/5.3._downloader_check.pdf
\caption/t RunCheck code sample checks the downloaded dataset and redownloads missing images.
\endinsert

\label[ch5_osm_marker]
\sec OSM Marker 
%===================

Our code works with data downloaded from Open Street Maps to estimate what is the neighborhood of each segment. We have downloaded the .osm data file of corresponding location and exported it into a PostgreSQL database following commands shown in Code \ref[code6_convert_osm_to_db].

\midinsert
\clabel[code6_convert_osm_to_db]{Loading data into PostgreSQL database}
\picw=13cm \cinspic pseudocodes/5.4._osm_db_import.pdf
\caption/t Loading data into PostgreSQL database.
\endinsert

	Having a  PostgreSQL database is advantageous, because it allows us sending repeated queries to database of objects with geographical location while using PostgreSQL macros and functions for getting distance and intersections between areas. PostgreSQL database has hierarchical structure of data storage which is necessary for fast data access. Our python code can access this database with simple queries and then process its responses.

\midinsert
\clabel[code7_osm_marker]{Marking data with OSM vector}
\picw=13cm \cinspic pseudocodes/5.4._osm_marker.pdf
\caption/t Marking data with OSM vector.
\endinsert

The code \ref[code7_osm_marker] loads array of Segments and one by one accesses all the distinct locations stored in each Segment. This could be only two locations for segments too short to be broken down, or more if the original segment was split. Minimally these are the starting and the ending locations of the segment.

We generate a SQL command, which targets particular columns which we chose to observe and also filters the data with "WHERE" clause for objects in distance smaller than chosen radius from segments location.

OSM data we imported into PostgreSQL database takes structure of four tables representing "point", "line", "polyline" and "road" objects. We look into each of these tables and combine the results.

Result of query gives us list of rows, each representing one object in vicinity of our location and columns containing attributes. We can produce a “"attribute=value"” pair from the values in each row alongside with the count of their occurrences.

Final neighborhood vector counts all these occurrences in fixed positions as was discussed in \ref[ch3_osm_vector].

Each segment stores multiple neighborhood vectors, one for each distinct location. Marked array of Segments is saved into the pickled ".dump" file. We can store multiple versions of these files, each with different radius setting, while reusing all the downloaded images.

\label[ch5_dataset_augmentation]
\sec Dataset Augmentation
%===================

\midinsert
\clabel[code7b_data_aug]{Data augmentation with ImageDataGenerator}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[bbb]{aaa}
      \picw=7cm \cinspic pseudocodes/5.x._augmentation_expand.pdf
      %\caption/f aaaa
   \par}\vtop{%
      %\clabel[ccc]{ddd}
      \picw=7cm \cinspic pseudocodes/5.x._augmentation_aggresive.pdf 
      %\caption/f bbbb
   \par}}
\centerline {Expansion scheme\hfil\hfil Aggresive Expansion scheme}\nobreak\medskip
\caption/t Data augmentation with the Keras ImageDataGenerator syntax.
\endinsert

We made use of the Keras inbuilt object ImageDataGenerator which can be controlled with syntax presented in Code \ref[code7b_data_aug]. We present two schemes of data augmentation. First scheme in which we allow for flipping and image shifting in limited amount of 10\% of the whole image. 

A more aggressive augmentation scheme is suggested for the second case, where we allow for flipping, shifting up to 20\%, rescale of 20\% and even shear and rotation of the image by 10\degrees degrees.

For each image in our original dataset we create a fixed amount of images with random transformations allowed from this augmentation scheme. For practical reasons of limiting the size of our dataset we chose to generate 2 new images (effectively tripling the size of the original dataset). For reproducibility of the results we stored the expanded dataset into their own folders reusing the same randomly generated data.

\label[ch5_dataset_handler]
\sec Datasets and DatasetHandler 
%===================

"DatasetObject" is a structure shielding us from low level manipulation with the segments stored in ".dump" file, which can then remain unchanged and be reused for many experiments.
	
Figure \ref[33_dataset_object] shows us the structure of "DatasetObject", with its most important functions and variables exposed.

\midinsert
\clabel[33_dataset_object]{Dataset object}
\picw=12cm \cinspic figures/5.5._dataset_object.pdf 
\caption/f Dataset object with important functions highlighted.
\endinsert

Dataset is initialized in Code \ref[code8_dataset_handler] by reading a "SegmentsData.dump" file, however it stores data in its own four internal arrays. These are: list of urls to images, labels marking the scores, OSM vectors if we have loaded a marked Segments file and unique IDs of original segments. Note that we are not directly loading all of the images yet, rather we are keeping only their filenames. We can access the images when necessary, saving us from wasteful memory allocation.

We also provide multiple getter functions which split data into training and validation datasets for the purpose of testing experiments. Each model type will require access to different data – image only model will for example omit all the OSM neighborhood vectors.

\midinsert
\clabel[code8_dataset_handler]{DatasetHandler}
\picw=13cm \cinspic pseudocodes/5.5._dataset_handler.pdf
\caption/t Initialization of Dataset in DatasetHandler from saved segments file.
\endinsert

\midinsert
\clabel[code9_dataset_visualization]{Data visualization}
\picw=13cm \cinspic pseudocodes/5.5._dataset_statistics.pdf
\caption/t Functions for data statistics visualization.
\endinsert

%\midinsert
%\clabel[33_dataset_visualization]{Dataset visualization}
%\picw=6cm \cinspic figures/nullfig.pdf 
%\caption/f Example of dataset visualization.
%\endinsert

%\secc Dataset statistics
%\secc DatasetHandler structure

\label[ch5_keras]
\sec Keras syntax 
%===================

Keras ~\cite[chollet2015keras] is a framework which supports fast prototyping of models with easy API to underlaying backend using low level Theano and TensorFlow. We can build entire CNN or Machine Learning models by listing individual layers of the model from prebuilt list of commands. For most of models mentioned in \ref[ch2_history_of_cnn] which performed well in the ILSVRC competition there exists a model description in Keras. Models, which we show how to load in Keras syntax in Code \ref[code13_applications], have weights available from learning on the large ImageNet dataset. This makes Keras perfect for our task, even for the feature transfer design we intend to use for our models.

Keras also has a training and testing API prepared to use models efficiently with good hardware support including computation on GPUs.

Generic code for building models in Keras works in two modes – Sequencial and Functional. We will use Functional in these particular examples as it is convenient both for writing and understanding.

We will mention some of the layers we can use to build Keras models.

\begtt
Out = Dense(256, activation=’relu’)(In)
Out = Dense(1, activation=’sigmoid’)(In)
\endtt
	
Dense layers stand for fully connected layers mentioned in \ref[ch4_fully_connected_layers]. Here we can see two possible uses for this type of layer. With width set to "1", this layer represents one neuron which can be used as the final output of our classifier.

\begtt
Out = Dropout(probability)(In)
\endtt

Dropout layer is used accompanying the Dense layers and follows the process mentioned in \ref[ch4_dropout_layers]. With selected probability the connection of neurons will be dropped during the training period.

\midinsert
\clabel[code10_build_generic_model]{Building generic model in Keras}
\picw=13cm \cinspic pseudocodes/5.6._build_generic_model.pdf
\caption/t Building generic model in Keras.
\endinsert

%We are using these mentioned layers in our models, for more detailed survey check \ref[APPENDIX KERAS SYNTAX] APPENDIX KERAS SYNTAX.

After building the model we need to compile it specifying settings which will be used for training such as the optimizer, loss function and additional measured metrics.

\begtt
Model.compile(optimizer, loss, metrics)
\endtt


For details about which settings we use for training, check \ref[ch4_model_training].

Finally our model is trained on dataset with fit command.
	
\begtt
model.fit(data, labels, validation_data = (data_val, labels_val))
\endtt

The full code to build and train a generic Keras model on a dataset can be seen in Code \ref[code10_build_generic_model] and \ref[code11_run_generic_experiment].

\midinsert
\clabel[code11_run_generic_experiment]{Fit generic model to data}
\picw=13cm \cinspic pseudocodes/5.6._run_generic_experiment.pdf
\caption/t Fit generic model to data in Keras.
\endinsert

\sec ModelHandler 
%===================

"ModelHandler" is composed of three functional sections. "ModelOI" provides the functions handling input and output – including interaction with "DatasetHandler" and reporting functions. "ModelGenerator" is responsible for building models in Keras syntax depending on settings we choose for current model. "ModelTester" provides functions controlling the training and testing capabilities of the model.

These individual blocks are controlled from "ExperimentRunner", which ties them together and manages shared Settings information.

Figure \ref[34_model_handler_structure] illustrates the various functions  which are part of the "ModelHandler". In Code  \ref[code16_model_handler_functions] we can see snippets of code dealing with building and testing models. Note that in one run we are handling multiple instances of models and datasets, as is explained in \ref[ch5_experiment_runner].

\midinsert
\clabel[34_model_handler_structure]{ModelHandler Structure}
\picw=13cm \cinspic figures/5.6.3._model_handler_struct.pdf 
\caption/f Illustration of "ModelHandler" structure using its three functional parts - "ModelOI", "ModelGenerator" and "ModelTester".
\endinsert	
	
\midinsert
\clabel[code16_model_handler_functions]{ModelHandler functions}
\picw=13cm \cinspic pseudocodes/5.6._model_handler_functions.pdf
\caption/t "ModelHandler" functions. One experiment run interacts with multiple models each with their individual settings and dataset. Here we can see the pseudocode of model generation and training - we process one by one.
\endinsert


\sec Models
%===================

Particular models described in section \ref[ch4] are shown here with functions which generate them. We have stated that their structure can be parametrized, and that we can alter these parameters to change their shape and performance. We can change the depth and width of these models.
	
\secc OSM model
%------------------

See Code \ref[code12_osm_model], which builds the model discussed in \ref[osm_model]. As input we are using the OSM neighborhood data. Number of fully-connected Dense layers as well as their width is variable.

\midinsert
\clabel[code12_osm_model]{OSM only model code}
\picw=13cm \cinspic pseudocodes/5.6._osm_model.pdf
\caption/t Function producing a Keras OSM model described in \ref[osm_model].
\endinsert

\secc Images model
%------------------

When we are working with model described in \ref[img_model], we make use of the feature transfer method and as such the training process can be made easier as was discussed in \ref[ch4_feature_cooking].

Also the Keras syntax will be correspondingly altered. We create two models instead of a single whole model. One represents the feature extractor section of the model and is mostly created by the reused base of pretrained CNN model. We can load these as base of our models with weights initialized from model trained on ImageNet dataset, with syntax presented in Code \ref[code13_applications].

\midinsert
\clabel[code13_applications]{Keras base models}
\picw=13cm \cinspic pseudocodes/5.6._applications.pdf
\caption/t Available CNN models in Keras for loading as base models.
\endinsert

Our second model is then made of just our custom top architecture discussed in \ref[ch4_custom_top]. We will feed it with outputs of features from the first base model. Note that for repeated experiments with the same dataset and model setting, we can save these features into files and spare a lot of valuable computation time. Code snippet \ref[code14_feature_cooking_img_model] shows us an example of the use of these two models.

\midinsert
\clabel[code14_feature_cooking_img_model]{Image model feature cooking}
\picw=13cm \cinspic pseudocodes/5.6._feature_cooking_img_model.pdf
\caption/t Images only model using the feature cooking method.
\endinsert

\secc Mixed model
%------------------

Mixed model uses the same techniques of feature transfer and is fully described in \ref[mix_model].

Code \ref[code15_mix_model] shows how we build this model. Note that we can even reuse the saved feature files from image only model, as long as we maintain the same order of data in our dataset. "DatasetHandler" can shuffle data in deterministic manner which allows us to do so.

\midinsert
\clabel[code15_mix_model]{Building mixed model}
\picw=13cm \cinspic pseudocodes/5.6._mix_model.pdf
\caption/t Building mixed model. Note that instead of calculating "img_features" on the fly from the selected base model, we can load the feature files shared with Image models.
\endinsert



\label[ch5_settings]
\sec Settings structure 
%===================

Throughout the whole run of experiments, we keep a shared Setting structure in form of python dictionary. Each experiment we run first loads default settings values and then it alters them depending on the experiment description file.

Each experiment has Settings defined for the whole run and also a list in "Settings[“models”]" further specifying multiple models per experiment. Each of these can be using different dataset, or it can reuse already loaded one without wasting resources.

Finally the reasoning behind experiments is to join these individual model runs and have an easy method of linking their results together while maintaining order in what are we testing. We can graph the learning progress of defined models into same plot for easier visualization.

\midinsert
\clabel[code16_default_settings]{Default Settings initialization}
\picw=13cm \cinspic pseudocodes/5.7._default_settings.pdf
\caption/t Default Settings initialization.
\endinsert

Settings is initialized from default values, built as a python object dictionary as seen in Code \ref[code16_default_settings]. Then a custom Setup function is called which specifies differences from the default settings. This allows for descriptions of new experiments to be relatively small and to reuse already written code. This also allows for very small settings description files specifying either simple problems, while also being able to design complex experiments going into great detail. See Code \ref[code16_minimal_settings_file] for example of a minimal experiment specification.

\midinsert
\clabel[code16_minimal_settings_file]{Minimal Settings description file}
\picw=13cm \cinspic pseudocodes/5.7._minimal_settings_file.pdf
\caption/t Minimal settings experiment definition file specifying a simple experiment with custom dataset and number of epochs.
\endinsert

\midinsert
\clabel[34_settings_syntax]{Settings syntax}
\picw=12cm \cinspic figures/5.7._settings_syntax.pdf 
\caption/f Settings syntax.
\endinsert

Following are tables of important values of Settings syntax. Note that we write these following syntax shown in Figure \ref[34_settings_syntax]. Table \ref[ch5_settings_table_whole] contains list of parameters which are applied over the whole experiment run. All other parameters are model specific and are listed in Tables \ref[ch5_settings_table_model] and \ref[ch5_settings_table_model_cont1].

\midinsert{ 
\clabel[ch5_settings_table_whole]{Settings parameters for the whole experiment}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.27\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.2\hsize\noindent\strut #\strut \hfil} 
& \vrule\hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
  parameter name & default value & description \cr
  \noalign{\hrule}
  experiment\_name & ‘basic’ & Name of the experiment as well as name of the folder used to store the results in. \cr
  \noalign{\hrule}
  graph\_histories & [‘all’, ‘together’] & Accepts values of ‘all’, which produces one graph for each model, ‘together’, which draws graphs of all models into one shared graph. Also accepts lists of numbers, which specify which models we would like to have plotted together. For example [0,2] would plot histories of the first and third model together. \cr
  \noalign{\hrule}
  interrupt & False & Internal flag used to interrupt the whole experiment in case of error. \cr
  \noalign{\hrule}
  filename\_features\_train, filename\_features\_test & ‘’ & Internal string flags used to share file paths amongst different parts of code in ModelHandler. \cr
}
\hrule
}
\vskip5pt
\startcenter
\captiontablehack/f List of parameters shared throughout the whole experiment.
\stopcenter
\endinsert


\midinsert{ 
\clabel[ch5_settings_table_model]{Settings parameters for each individual model}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.23\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.2\hsize\noindent\strut #\strut \hfil} 
& \vrule\hskip2pt \vtop{\hsize=.5\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
  parameter name & default value & description \cr
  \noalign{\hrule}
  \bold{Dataset specific} \cr
  \noalign{\hrule}
dataset\_pointer & -1 & If left at value ‘-1’ we instantiate a new dataset for this model. Otherwise we use the dataset of the indicated different model. For example if the first model has -1, then the second model can have dataset\_pointer set to 0 to reuse the same dataset. \cr
  \noalign{\hrule}
dataset\_name & ‘1200x\_markable \_299x299’ & States the name of folder we want to use for loading the dataset from (this folder has to contain SegmentsData.dump and a folder ‘images’ with Google Street View images). Downloader can prepare these folders for us. \cr
  \noalign{\hrule}
dump\_file\_override & ‘’ & Can be used to indicate usage of nonstandard segments file. For example we can keep multiple versions of OSM marked data in different files and access them via this setting (for example loading file ‘SegmentsData\_marked\_R100.dump’). \cr
  \noalign{\hrule}
pixels & 299 & Pixel dimension indicator of the loaded images (image files have dimension of pixels*pixels*3). \cr
  \noalign{\hrule}
number\_of\_images & None & Indicates if we want to use only a subset of the dataset. When left at None, whole dataset will be used, otherwise a uniform sampling will be used to give us indicated number of images. \cr
  \noalign{\hrule}
seed & 13 & Seed for maintaining deterministic nature of certain random sampling operations (such as the initial reordering of loaded dataset). This value is important as it guarantees our ability to reuse precomputed feature files. \cr
  \noalign{\hrule}
validation\_split & 0.25 & Under which fraction we split data in simple (those not guided by k-fold cross-validation scheme) experiments. Ranges from 0 to 1. With 0.25 one quarter of data will designated as validation set and the remainder of three-quarters will be the training set. \cr
  \noalign{\hrule}
shuffle\_dataset & True & Indicator that we want to shuffle our dataset in deterministic manner (using the seed value). \cr
  \noalign{\hrule}
shuffle\_dataset \_method & 'default-same-segment' & Shuffling method which maintains the order of images from the same segment to be kept together, which is important not to bring in dualities into our data. \cr
  \noalign{\hrule}
}
\hrule
}
\vskip5pt
\startcenter
\captiontablehack/f List of parameters specific to individual models.
\stopcenter
\endinsert



\midinsert{ 
\clabel[ch5_settings_table_model_cont1]{Settings parameters for each individual model (continued)}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.22\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.19\hsize\noindent\strut #\strut \hfil} 
& \vrule\hskip2pt \vtop{\hsize=.54\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
  parameter name & default value & description \cr
  \noalign{\hrule}
  \bold{Dataset specific} \cr
  \noalign{\hrule}
edit\_osm\_vec & ‘’ & Additional transformation of the OSM data, accepts 'booleans', which turns all quantitative entries into 0 or 1. Setting 'low-mid-high' turns all entries into three categorical system of low, mid and high depending on percentiles of data distribution in dataset. These three categories are encoded as one-hot vector producing variants: 001, 010 or 100. Note that this effectively triples the length OSM vector. \cr
  \noalign{\hrule}
  \bold{Model specific} \cr
  \noalign{\hrule}
unique\_id & & Unique name for this model, will be used for plotting graphs and later for identification which plotted history belonged to which model.\cr
  \noalign{\hrule}
model\_type & 'simple\_cnn\_ with\_top' & Specifies which model type will we use. Accepted values are: 'simple\_cnn\_with\_top', 'img\_osm\_mix', 'osm\_only'. \cr
  \noalign{\hrule}
cnn\_model & 'resnet50' & Which basic CNN model will we use in case of image\_only and mixed model. Allowed values are: 'vgg16', 'vgg16', 'resnet50', 'inception\_v3', 'xception'. \cr
  \noalign{\hrule}
cooking\_method & 'generators' & Internally used flag to indicating which method will we use to generate feature files. 'generators' tends to be less memory demanding, but consumes longer time when compared with the other allowed value 'direct'. \cr
  \noalign{\hrule}
top\_repeat\_FC \_block & 3 & Specifies depth of classifier in image\_only and osm\_only models. \cr
  \noalign{\hrule}
osm\_manual\_width & 256 & Manually setting the width of osm\_only model. \cr
  \noalign{\hrule}
save\_visualization & True & Whether we want to plot graphs for this experiment. \cr
  \noalign{\hrule}
\bold{Training specific} \cr
  \noalign{\hrule}
epochs & 150 & Indication of how many epochs we want to spend on training of this model. \cr
  \noalign{\hrule}
optimizer & 'rmsprop' & Choice for the Keras optimizer. Suggested values are 'rmsprop' or 'adam', however also accepts the object of Keras optimizer such as customizable  optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9) \cr
  \noalign{\hrule}
loss\_func & 'mean\_squared \_error' & Metric used as a loss function during training of this model. \cr
  \noalign{\hrule}
metrics & ['mean\_absolute \_error'] & List of other metrics which we want to also track into history. \cr
  \noalign{\hrule}
}
\hrule
}
\vskip5pt
\startcenter
\captiontablehack/f List of parameters specific to individual models (continued).
\stopcenter
\endinsert

\newpagehax

\label[ch5_experiment_runner]
\sec Experiment running 
%===================

"ExperimentRunner" is special scheme which uses other modules mentioned in this chapter. It starts with loading a custom Settings file and then follows the same structure for various experiments eventually evaluating the tested model and dataset combination.

\midinsert
\clabel[code16_experiment_runner_pseudocode]{ExperimentRunner pseudocode}
\picw=13cm \cinspic pseudocodes/5.x._experiment_runner.pdf
\caption/t ExperimentRunner pseudocode outlining the run of whole experiment, we are making heavy use of the units explained in this chapter.
\endinsert

The structure of ExperimentRunner is outlined in Code \ref[code16_experiment_runner_pseudocode]

\label[ch5_training]
\sec Training 
%===================

Basic model functionality is supported by Keras with commands in Code \ref[code17_training]. We specify settings of the training procedure by choosing parameters of optimizer, loss function and additional metrics. We chose optimizers "adam" and "rmsprop", which work well with their default settings, as is discussed in \ref[ch4_model_training].

As a loss function we have selected “mean squared error” with formula in Figure \ref[27_mse_formula]. Keras allows us to specify additional metrics, which can be used to simply track progress, but don’t actually influence the training process.

Finally we are given a history stored in dictionary of arrays with tracked values (both of the loss function and measured metrics). We make use of these histories to plot the progress of training into graphs.
	
\midinsert
\clabel[code17_training]{Model training syntax}
\picw=13cm \cinspic pseudocodes/5.8._training.pdf
\caption/t Model training syntax.
\endinsert

\sec Testing 
%===================

For more relevant results we use the advanced method of k-fold cross-validation. In code we actually use the same syntax as the one of \ref[ch5_training], however with data generation scheme described in \ref[ch4_model_evaluation].

Note that we can use the results of graphed histories even without using the k-fold cross-validation, however this gives us more in depth view.

\sec Reporting and ModelOI 
%===================

"ModelOI" is a programmatic unit responsible for inputs and outputs of the larger experiment runs. We use it to communicate with "DatasetHandler" while loading datasets at the beginning of experiments. We also use methods of "ModelOI" for reporting after the training has ended.

Histories produced during training in "ModelTester" are processed and plotted into graphs via the "Matplotlib" library.

For convenience we also chose to send interesting results via email after the often lengthy computations have finished.

\label[ch5_metacentrum]
\sec Metacentrum project and scripting 
%===================

Thanks to the Metacentrum project ~\cite[vsustrmetacentrum] we had access to machines with high enough computational power needed to teach even complicated models.

Metacentrum is a project supporting research groups including the academic research groups at our university. They provide a server cluster capable of running our experiments on wide array of machines with powerful hardware. We control the individual runs of experiments by special tasking and job queuing system, PBS Professional. We also make use of its storage units with image datasets containing many items and large feature files.

Scripts controlling the job queuing on Metacentrum servers are using the PBS Professional job scheduler with couple of commands described in Code \ref[ch5_metacentrum_scripts].

Note that further customization of commands is available, however these commands were all we needed for our experiments.
	
% Metacentrum scripts table
\midinsert \clabel[ch5_metacentrum_scripts]{Metacentrum scripts}
\ctable{ll}{
"qsub task.sh" & basic specification to run bash program in "task.sh" \cr
"-l walltime=$<$time$>$"   & reserved time (for example "walltime=23:30:00") \cr
"-l mem=$<$memory$>$"   & memory requirements (for example "mem=32gb") \cr
"-l ncpus=$<$n$>$"         & number of CPUs used (for example "ncpus=4") \crl \tskip4pt
% -------------------------------------
"qstat -u $<$user$>$"      & command to get list of tasks run by a specific user \cr
"qstat $<$task\_id$>$"      & get information about task specified by unique  "task\_id"  \crl \tskip4pt
% -------------------------------------
"qdel $<$task\_id$>$" & delete planned job \cr
"qsig -s SIGINT $<$task\_id$>$"   & interruption of running task by the "SIGINT" signal \cr
}
\caption/t Metacentrum planner scripts
\endinsert

Contents of task.sh written in bash are in Code \ref[code18_metacentrum_bash_task]. We first load Anaconda environment and step into the correct directory. Then we run python code which is supplied with path to Settings file and an unique job id. Settings files are described in \ref[ch5_settings] and allow us to customize runs of experiments relatively freely.

\midinsert
\clabel[code18_metacentrum_bash_task]{Metacentrum task code}
\picw=16cm \cinspic pseudocodes/5.11._metacentrum_bash_task.pdf
\caption/t Bash code to run python file with custom targeted Settings file. Note that "$PBS_JOBID" is an unique job id given by the tasking and scheduling software.%$$
\endinsert


\label[ch6]
\chap Results
%==============================================================

This chapter is divided into several sections. First we will look at the efficiency of various strategies used when generating the dataset in \ref[ch6_dataset] and then when building models in \ref[ch6_model]. %In section \ref[ch6_overall] we will explore the overall performance of our best models on the task.

\sec How to read graphs in this section

We have implemented a k-fold cross-validation discussed in \ref[ch4_model_evaluation] and used it to evaluate performance of the tested model and dataset combination. We will present a higher level view of the experiment by plotting the evolution of error over training epochs. Note that we can follow both the training and validation error to see how much is our model overfitting.

We also provide a more detailed view into the situation after the training was finished. We look at the box plot graphs of errors in the last epoch of training.

Note that in some cases we may want to hold the model with the best achieved validation error, instead of the one from last iteration. This applies in the case when with further training iterations the model overfits. For this view we may plot box plot graphs of the epoch where each model fared the best.

\label[ch6_dataset]
\sec Strategies employed in dataset generation
%===================

In this section we will focus on the results of various attempts to alter the dataset generation scheme. For in-depth explanation of the reasoning why we chose these approaches consult the discussion section \ref[ch7_dataset].

As we described in \ref[ch3_available_data] we have multiple ways to construct our dataset. We have tried varying the size of downloaded images in \ref[ch6_dataset_pixelsize].

We also explore two types of increasing the images available for our models. In \ref[ch6_dataset_longedgesplit] we try splitting long edges to smaller segments, while maintaining a limit of how small can resulting segment be. The smaller the minimal segment length, the higher the number of segments and consequentially the number of images. See \ref[ch3_downloading_street_view_images] and Code \ref[code3_longedges] for details.

In \ref[ch6_dataset_augmentation] we try alternative approach to dataset augmentation from data already obtained by applying additional transformations such as flipping the image, shifting or scaling. In this way we create new images from an already existing set.

\label[ch6_dataset_pixelsize]
\secc Dimensions of downloaded images
%------------------

We chose to try two settings to illustrate the effect of our choice in the matter of image dimensions. Following the original article of ~\cite[inception], we chose our first image size to be 299x299 pixels. Our second dataset is made of images sized 640x640 pixels, which was the limitation of Google Street View as the most detailed image resolution available freely via their API.

Note that the 640x640 dataset is bigger in its size and it also takes more time for the models to be trained on it. Also the consequent feature files which we store in between experiments to allow their reuse are bigger.

\midinsert
\clabel[35_pixel_size_mixed]{Experiment with pixel size, Mixed model}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[bbb]{aaa}
      \picw=4.2cm \cinspic graphs/6.1.1._pixel_size/figLeft_boxplotcomp_val_mix.pdf
      %\caption/f aaaa
   \par}\vtop{%
      %\clabel[ccc]{ddd}
      \picw=9cm \cinspic graphs/6.1.1._pixel_size/figRight_graphcomp_evol_mix.pdf 
      %\caption/f bbbb
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f Model: \textit{Mixed}, Datasets: \textit{299x299}, \textit{640x640}. Results of experiment with pixel size of input images. Here we see the graph of evolution of loss function over epochs. K-fold cross-validation scheme was used with k=10. We can see error on validation dataset of all the 10 runs with average results highlighted.
\endinsert

We have used datasets internally marked as "5556x_mark_res_299x299" for 299x299 dataset and "5556x_markable_640x640" for the 640x640 dataset. Their only difference is in the image pixel dimensions.

As for the choice of models we don’t include the OSM only model as it is not affected by image pixel size. Figure \ref[35_pixel_size_mixed] shows the results when choosing mixed model (see \ref[mix_model]). Figure \ref[35_pixel_size_image] shows the results when using image only model (see \ref[img_model]).

\midinsert
\clabel[35_pixel_size_image]{Experiment with pixel size, Image model}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[bbb]{aaa}
      \picw=4.2cm \cinspic graphs/6.1.1._pixel_size/figLeft_boxplotcomp_val_img.pdf
      %\caption/f aaaa
   \par}\vtop{%
      %\clabel[ccc]{ddd}
      \picw=9cm \cinspic graphs/6.1.1._pixel_size/figRight_graphcomp_evol_img.pdf 
      %\caption/f bbbb
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f  Model: \textit{Image}, Datasets: \textit{299x299}, \textit{640x640}. Results of experiment with pixel size of input images. Here we see the graph of evolution of loss function over epochs. K-fold cross-validation scheme was used with k=10. We can see error on validation dataset of all the 10 runs with average results highlighted.
\endinsert

We can see both the validation error as well as training error evolution over time of epochs as well as box plots with more detailed view of last epoch.

In order to compare the performance of both the two used datasets and models, we use the box plots in Figure \ref[35b_pixel_size_four], which look at the best validation error score of the four models.

\midinsert
\clabel[35b_pixel_size_four]{Pixel size and best validation error}
\picw=10.5cm \cinspic graphs/6.1.1._pixel_size/fig_Four.pdf
\caption/f Models: \textit{Image}, \textit{Mixed}, Datasets: \textit{299x299}, \textit{640x640}. Comparison of four model and dataset combinations used in the pixel size experiment. We look at the best achieved validation error throughout their evolution over epochs.
\endinsert

\label[ch6_dataset_longedgesplit]
\secc Splitting long edges
%------------------

\midinsert
\clabel[ch6_longedge_table]{Dataset overview in experiment with splitting long edges}
\picw=10cm \cinspic tables/6.4._datasets_for_edgesplitting.pdf
\captiontablehack/f Dataset overview for the experiment of splitting long edges into smaller segments. Note that the number of images corresponds with the size of inputs for models.
\endinsert

See Table \ref[ch6_longedge_table] with a list of tested datasets. When we choose to split long edges, we generate more smaller segments and from each of these segments even more images. Note that we count only valid segments, where there is imagery data available and where we had a label in the initial dataset.

We chose to run the experiment with the original dataset "5556x_markable_640x640" as comparison of how models fared without long edge splitting.

For this experiment it again makes sense to test on the Image and Mixed models as those are the ones affected by imagery data input.

\midinsert
\clabel[36_splitedge_2x2]{Split long edges, Mixed model, overall comparison}
\picw=13cm \cinspic graphs/6.1.2._splitting_long_edges-minlen/fig3_2x2comparison_mix.pdf
\caption/f Model: \textit{Mixed}. Overall illustration of the training process of three datasets which were enhanced by long edge splitting and an original dataset for reference.
\endinsert

\midinsert
\clabel[36_splitedge_valerr]{Split long edges, Mixed model, validation error over iterations}
\picw=13cm \cinspic graphs/6.1.2._splitting_long_edges-minlen/fig2_average_valerr_mix.pdf
\caption/f Model: \textit{Mixed}. Long edge splitting experiment. For better clarity we present only the averages of validation errors over the 10 runs of k-fold cross-validation with k=10.
\endinsert

\midinsert
\clabel[36_splitedge_boxplot]{Split long edges, Mixed model, comparison of last epoch between datasets}
\picw=13cm \cinspic graphs/6.1.2._splitting_long_edges-minlen/fig1_boxplot_last_mix.pdf
\caption/f Model: \textit{Mixed}. Comparison of the results of splitting long edges into smaller segments. The smaller the minimal edge length was, the more images were generated. Dataset "5556x_markable_640x640" under the label original was used for comparison. 
\endinsert

Starting with Mixed model, we chose to present the detailed information about the progress of training in Figure \ref[36_splitedge_2x2], which shows all four dataset variants alongside with its validation and training error. In Figure \ref[36_splitedge_valerr] we see the average validation errors plotted together in order to better compare their performances. Lastly in \ref[36_splitedge_boxplot] we can see the comparison of all four datasets in their last epoch.

Note that we also tested the Image model in Appendix with Figures \ref[36_splitedge_valerr_img] and \ref[36_splitedge_boxplot_img].

\label[ch6_dataset_augmentation]
\secc Dataset augmentation
%------------------

\midinsert
\clabel[ch6_augmentation_table]{Datasets used for augmentation}
\picw=10cm \cinspic tables/6.8._datasets_for_dataaugmentation.pdf
\captiontablehack/f List of datasets used for the experiment of data augmentation with the number of valid images they contain.
\endinsert

As was explained in \ref[code7b_data_aug] we chose two schemes for data augmentation which gives us three datasets to work with as listed by Figure \ref[ch6_augmentation_table].

\midinsert
\clabel[37_augmentation]{Result of augmented datasets}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[37_augmentation_boxplots_last]{Dataset augmentation last epoch}
      \picw=4.2cm \cinspic graphs/6.1.3._augmentation-original,expanded,agg_expanded/figLeft_markable.pdf
      %\caption/f Last epoch validation error
   \par}\vtop{%
      %\clabel[37_augmentation_evol]{Dataset augmentation evolution over epochs}
      \picw=9cm \cinspic graphs/6.1.3._augmentation-original,expanded,agg_expanded/figRight_markable.pdf
      %\caption/f Evolution over epochs
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f Model: \textit{Mixed}, Dataset: \textit{5556x\_markable\_640x640}. Comparison of the two augmented datasets with the original referential one. Expanded dataset used the transformations of image flipping and shifting. Aggressively expanded dataset used additional transformations of shear, scale and rotation. 
\endinsert

Figure \ref[37_augmentation] shows the evolution of validation error over the epochs, as well as the validation and training error distribution from k-fold cross-validation in its last epoch. On Figure \ref[37b_augmentation] we have the same experiment using "5556x_minlen30_640px" as the original dataset.

\midinsert
\clabel[37b_augmentation]{Result of augmented datasets with edge splitting}
\line{\hsize=.5\hsize \vtop{%
      %\clabel[37_augmentation_boxplots_last]{Dataset augmentation last epoch}
      \picw=4.2cm \cinspic graphs/6.1.3._augmentation-original,expanded,agg_expanded/figLeft_minlen30.pdf
      %\caption/f Last epoch validation error
   \par}\vtop{%
      %\clabel[37_augmentation_evol]{Dataset augmentation evolution over epochs}
      \picw=9cm \cinspic graphs/6.1.3._augmentation-original,expanded,agg_expanded/figRight_minlen30.pdf
      %\caption/f Evolution over epochs
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\caption/f Model: \textit{Mixed}, Dataset: \textit{5556x\_minlen30\_640px}. Comparison of the two augmented datasets with the original referential one.
\endinsert

\label[ch6_model]
\sec Strategies employed in model architecture
%===================

We already could see changing behavior of different model types interacting with differently generated datasets in \ref[ch6_dataset], but here we will focus directly on the changes of model architectures and their influence on performance.

Note that we will mostly use the datasets which showed promising results in \ref[ch6_dataset] ignoring those which lead to decrease of performance.

When using the Mixed and Image models, we are using an already existing base CNN model for feature transfer (see \ref[img_model] and Figure \ref[19_base_top]), so in the our first experiment in \ref[ch6_model_base] we will explore the possible types of used CNN base models.

In \ref[ch6_model_versus] we look into the comparison of performances of the three designed models – Image only, OSM only and Mixed model.

In \ref[ch6_model_osm_wd] we will look at a OSM model specific setting of depth and width of the CNN design.

\label[ch6_model_base]
\secc Different CNN base model for feature transfer
%------------------

As was mentioned in Code \ref[code13_applications], Keras allows to use multiple pretrained models alongside with weights initiated from the ImageNet dataset.

\midinsert
\clabel[37b_base_models]{Different base CNN}
\picw=13cm \cinspic graphs/6.2.1._different_CNN/fig_average_valerr_.pdf
\caption/f Model: \textit{Mixed with variable base}, Dataset: \textit{5556x\_markable\_640x640}. Illustration of five models in one graph. For better clarity we chose to show only the averages of validation and training errors. These Mixed models used different CNNs as their base.
\endinsert

Note that these models plotted in \ref[37b_base_models] have different number of parameters in their structure and therefore the whole evaluation of experiment can take different amount of time and exhibit different memory requirements.

\midinsert{ 
\clabel[ch6_cnns_table]{Comparison of alternate base CNNs}
\hrule
\halign{
% design of columns
& \hskip2pt \vtop{\hsize=.3\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.3\hsize\noindent\strut #\strut \hfil}
& \vrule\hskip2pt \vtop{\hsize=.35\hsize\noindent\strut #\strut \hfil} \tabskip=1em \crcr 
% data
Mixed model with ... & \# of trainable parameters of base CNN & dimensionality of features \cr
  \noalign{\hrule}
resnet50 & 23,534,592 & (\textit{n}, 2, 2, 2048) \cr
vgg16 & 21,768,352 & (\textit{n}, 18, 18, 2048) \cr
vgg19 & 20,024,384 & (\textit{n}, 20, 20, 512) \cr
inception v3 & 14,714,688 & (\textit{n}, 20, 20, 512) \cr
xception & 20,806,952 & (\textit{n}, 20, 20, 2048) \cr}
\hrule
}
\vskip5pt
\startcenter
\captiontablehack/f List of models available in Keras framework with the number of parameters the mixed model using it as its base CNN model will have. Note that \textit{n} in the dimension of features signifies the number of images. Dataset used for this measurement contained images of 640x640 pixels.
\stopcenter
\endinsert


\label[ch6_model_versus] 
\secc Model competition - Image vs. OSM vs. Mixed
%------------------

We have proposed three major CNN architecture schemes to build models - OSM only described in \ref[osm_model] and built with Keras by Code \ref[code12_osm_model], Image only described in \ref[img_model] and in Code \ref[code14_feature_cooking_img_model] and finally the Mixed model described in \ref[mix_model] and built by Code \ref[code15_mix_model].

We chose one dataset to compare these three models on. We use the default setting for building these models.

\midinsert
\clabel[38_modelversus_evol]{Model competition, evolution over epochs}
\picw=13cm \cinspic graphs/6.2.2._model_competition-img-vs-osm-vs-mix/fig1_evolution_markable.pdf
\caption/f Models: \textit{OSM}, \textit{Image}, \textit{Mixed}, Dataset: \textit{5556x\_markable\_640x640}. Model competition, evolution over epochs.
\endinsert

\midinsert
\line{\hsize=.5\hsize \vtop{%
      \clabel[38_modelversus_last]{Model competition, last epoch}
      \picw=5.5cm \cinspic graphs/6.2.2._model_competition-img-vs-osm-vs-mix/fig2_last_epoch_markable.pdf
      \caption/f Model competition, last epoch. Dataset: \textit{5556x\_markable\_640x640}.
   \par}\vtop{%
      \clabel[38_modelversus_best]{Model competition, best epoch}
      \picw=5.5cm \cinspic graphs/6.2.2._model_competition-img-vs-osm-vs-mix/fig3_best_epoch_markable.pdf
      \caption/f Model competition, best epoch. Dataset: \textit{5556x\_markable\_640x640}.
   \par}}
%\centerline {aaa\hfil\hfil bbb}\nobreak\medskip
\endinsert

See Figure \ref[38_modelversus_evol] for high level view of evolution of each models performance over time of epochs. For detailed view of models state in the final epoch see \ref[38_modelversus_last]. To be fair for evaluating the models, we also look at Figure \ref[38_modelversus_best] comparing the best performance of each model when just looking at its validation error. Note that the exact epoch when the model performed the best can be different for these three models.

The results of "5556x_minlen30_640px" dataset are shown in Appendix \ref[ap_model_versus].

\label[ch6_model_osm_wd]
\secc OSM specific - width and depth
%------------------

This experiment focuses on the performance of OSM model (see \ref[osm_model]) in different settings of its width and depth. See Figure \ref[18_osm_model], where we can see what these two parameters influence. Note that by increasing width, we are setting the same width for all layers and that there are possible unexplored architectures of models with variable width in different depths (imagine for example a model with first layer wide 128 neurons and second and third layer 32 neurons wide).

We are trying values of (1, 2, 3, 4) for depth and (32, 64, 128, 256) for width. Each model was ran for 10 times with k=10 folds of cross-validation. Out of these runs we preserved the score best achieved validation over the evolution of epochs (to get the score before the model started overfitting). We have a grid of 4x4 possible combinations plotted in Figure \ref[39_osm_wd_4x4] each with statistics evaluation of the best achieved validation errors.

\midinsert
\clabel[39_osm_wd_4x4]{OSM model with variable depth and width}
\picw=10cm \cinspic graphs/6.2.3._OSM_model_GRID_4x4/fig1_4x4madness.pdf
\caption/f Model: \textit{OSM}, Dataset: \textit{5556x\_markable\_640x640}. OSM model alterations in depth and width.
\endinsert

We chose to display a cut of this table with fixed value of depth to 2 and variable width in Figure \ref[40_osm_wd_d2cut] which follows also their performance over epochs.

\midinsert
\clabel[40_osm_wd_d2cut]{OSM model with fixed depth and variable width}
\picw=12cm \cinspic graphs/6.2.3._OSM_model_GRID_4x4/fig2_cutOfD2fixed.pdf
\caption/f Model: \textit{OSM}, Dataset: \textit{5556x\_markable\_640x640}. OSM model with depth fixed at value 2 and variable width.
\endinsert

For more combinations see section \ref[ap_model_osm_wd] in Appendix, where are other cuts (with one attribute fixed while the other one variable) available.

%\label[ch6_overall]
%\sec Detailed results of one model and dataset combination
%===================

%\midinsert
%\clabel[TemporaryGraph]{TemporaryGraph}
%\picw=15cm \cinspic graphs/00temporary__handmade_design_test.pdf 
%\caption/f TemporaryGraph.
%\endinsert	

\label[ch7]
\chap Discussion
%==============================================================

\sec Limits of indiscriminate dataset expansion
%===================

There are two main fields of strategies we can analyze when approaching our task – dataset augmentation in \ref[ch7_dataset] and model design in \ref[ch7_model]. As was discussed in section \ref[ch3_available_data], there are some ways to alter the scheme of generating data from edges. We understand data as lists of items each with a triple (image, OSM vector, score). When expanding the initial dataset with imagery information, we chose a scheme of generating six non-overlapping images per segment. We rotate by 120\degrees degrees two sides of each segment. Furthermore as described in \ref[7_edge_breakdown] we chose to split long edges into smaller segments and then generate images from them.

It could be argued that it is in our interest to generate as many images as possible, which would encourage breaking long edges into many very short segments and perhaps also adapting the amount of degrees we rotate around on the same spot. In theory the continuous quality of real world could make us want to sample it in the most detailed way, generating an infinity of distinct images. However the practical reality is very much different. In fact we are working with a discrete representation of real world into a limited set of images. If we ask Google Street View for samples too close to each other, we will in fact obtain the same images from its API. Similarly choosing to rotate for less than 120\degrees degrees would produce images with vastly overlapping content.

Note that increasing number of images inevitably also introduces an increase in time and resource consumption. We should consider the advantages of being economical with resources as well as watching the validation error.

\label[ch7_dataset]
\sec Dataset altering experiments analysis
%===================

In \ref[ch6_dataset] we looked into the ways we can employ to alter the dataset we are using and we have plotted both the progress overview of error evolving over epochs as well as detailed views into the situation in the last iteration.

\label[ch7_dataset_pixelsize]
\secc Pixel size influence
%------------------

We have tested the effect of using datasets with different dimensionality of input images - datasets 299x299 and 640x640. As was already mentioned on Figure \ref[20_diff_in_diff_out] the input image dimension will influence the dimension of intermediate features produced by the base CNN model. Consequent structure of our models has to deal with this variability in size.

As we can see in the case when we used Image only model on Figure \ref[35_pixel_size_image], the use of bigger images is advantageous - in the last epoch the statistics of validation error speak in favor of dataset 640x640.

Somewhat surprising is the result over the Mixed model, where as can be seen on Figure \ref[35_pixel_size_mixed] the validation error of 640x640 dataset is actually worse than that of 299x299 in terms of median, mean and also the spread of lower and upper percentiles. Here we should consider the situation, where the 299x299 dataset produces features of size (1, 1, 2048), whereas the 640x640 dataset produces size (2, 2, 2048). Regardless of chosen dataset, the performance of Mixed model is better than that of Image only model. However the 299x299 dataset seems to gain more from this transition than the 640x640 one. We end up with the Mixed model on 299x299 dataset beating other combinations on its last iteration. Figure \ref[35b_pixel_size_four] confirms this finding also for the best achieved validation error along all epochs for all these model and dataset combinations.

\label[ch7_dataset_longedgesplit]
\secc Influence of splitting long edges
%------------------

In section \ref[ch6_dataset_longedgesplit] we looked into comparison of three datasets where we used segment splitting with variable minimal segment length with the original dataset. We tested the Image and Mixel models.

When looking at Figure \ref[36_splitedge_boxplot_img] which portrays the comparison of Image model over the four datasets, we can notice slight improvement of “minlen30” dataset over the original untouched set. Except for one outlier, the lower and upper quartile of results over the ten tests evaluated in k-fold cross-validation are better. However note that for Image model the mean value remained the same across all datasets. This can be visible on Figure \ref[36_splitedge_valerr_img], where validation error averages converge to the same value in their last iteration. It is also worth noting, that in the initial 100 iterations the Expanded datasets performed better and only overfit towards the same mean average value with their 400th to 500th iteration.

Situation in Figure \ref[36_splitedge_valerr] is similar, however the original model seems to be performing better even in initial iterations. Look at Figure \ref[36_splitedge_boxplot] reveals that the state of the last iteration didn’t bring much improvement with the altered datasets of “minlen10”, “minlen20” and “minlen30”. The spread of results in the non original datasets is wider, however both the median and mean of “minlen30” has improved over the original dataset. For several further experiments we chose dataset “5556x\_minlen30\_640px” to accompany the original “5556x\_markable\_640x640”.

\label[ch7_dataset_augmentation]
\secc Influence of dataset augmentation
%------------------

In this experiment we looked at dataset augmentation by series of random transformations applied to images to generate larger sets of images. We have tested two datasets as the initial source for this generation - the basic "5556x_markable_640x640" and the already expanded dataset "5556x_minlen30_640px", which showed some promise in \ref[ch7_dataset_longedgesplit].

We further tested two augmentation schemes, so called “expanded” and “aggressively expanded” generated by the syntax shown in Code \ref[code7b_data_aug]. The second scheme consists of more transformations and larger intensities of the transformation effects.

Mixed model was used for all of these datasets.

Let us first focus on the effect of data augmentation over the basic "5556x_" "markable_640x640" dataset. When comparing the three used datasets in left plot of Figure \ref[37_augmentation], we see better results in terms of both median and mean an the distribution of errors - both expanded datasets improve the original one. When consulting the overall view on the right side of Figure \ref[37_augmentation] we see, that the average validation error is improved significantly for the expanded datasets over the whole training period.

Situation with dataset "5556x_minlen30_640px" taken as the base for further augmentation hasn’t brought the same amount of performance boost, as can be see in Figure \ref[37b_augmentation]. In fact both the median and mean of validation error is worse in its last iteration in the cases of both expanded datasets. Looking at the values of training error, this might be due to the model overfitting more on the two expanded datasets. Overall view on the right side of Figure \ref[37b_augmentation] underscores that in this case the data augmentation didn’t help.

It is interesting to compare the result of data augmentation on the original dataset, versus on the dataset with more data samples in "5556x_minlen30_640px". When looking at table in Figure \ref[ch6_augmentation_table], we can see the exact numbers of images. It seems that the dual data enhancement, first with long edge splitting followed by augmentation by random image transformations didn’t bring in expected error reduction. The byproduct was in fact vastly increased time needed for training and evaluation of models with more than 4 times the images than the initial dataset "5556x_markable_640x640".


\label[ch7_model]
\sec Model altering experiments analysis
%===================

Whereas in \ref[ch7_dataset] we looked mainly at the difference between various datasets with mostly unchanged model choice, in this section we will go over the results of \ref[ch6_model] which explored variation of the basic models architecture.

\secc Alternative base CNN model
%------------------

Plot of Figure \ref[37b_base_models] shows us the first 50 epochs into training of Mixed models built with different base CNNs from the list which Keras allows us to use. All the models except for "resnet50" have great difficulties to learn on the dataset. Table \ref[ch6_cnns_table] provides us with possible explanation. With chosen dataset of images sized 640x640 pixels, the dimensionality of features we save at the output of used base CNN varies dramatically between model "resnet50" with (2, 2, 2048) and other models with values up to (20, 20, 2048). Subsequently the training process of top model with an unchanged architecture is greatly hindered with too large features. We would need to alter the architecture of top models appropriately using pooling layers to subsample the dimension, or make use of dataset with smaller input images. After all these models were initially used with images of pixels size in range of 224x224 to 299x299.

\label[ch7_model_versus]
\secc Competition between model designs
%------------------

We have tested the three basic model designs suggested in chapter \ref[ch4]. OSM model using only the vector neighborhood data of \ref[osm_model], Image model using gathered Google Street View images of \ref[img_model] and finally the mixed model of \ref[mix_model] combining both data sources.

For this experiment we have used three datasets to test the models performance upon. The basic dataset is again "5556x_markable_640x640" followed by "5556x_minlen30_640px".

We will start by looking at the models performance on the basic "5556x_markable_" "640x640" dataset on Figure \ref[38_modelversus_evol]. We can see that on the basic dataset the OSM model outperforms all other models by quite large margin. In detail we can explore the performance in Figure \ref[38_modelversus_last] in last iteration of the training process, or to be more fair in Figure \ref[38_modelversus_best] which looks at the best result over the whole training. The OSM model has better median and mean of validation error as well as lower and upper quartile spread than the other two models.

When we compare the results of the basic unexpanded dataset "5556x_markable_" "640x640" with other expanded datasets, the performance of OSM model actually suffers in the expanded datasets. This is due to the model using only the neighborhood vector information without taking images into account. By the manner of data generation, when splitting long edges, we are also creating new distinct locations, which will be marked with their own distinct OSM vector. But when we simply apply image transformations mentioned in Code \ref[code7b_data_aug] we are not adding any new unique locations. Therefore the OSM model cannot gain any new information by the data augmentation.

When looking at the model comparison over dataset "5556x_minlen30_640px", the performance of Mixed model is much closed to the OSM model. Looking directly at Figure \ref[ap1_modelversus_best], which shows the best achieved average validation error over the whole training process, we see that the Mixed model has in fact very slightly lower validation error in terms of both lower and upper percentile than the OSM model. We can see similar result of \ref[ap1_modelversus_evol], where the Mixed and OSM models have comparable performance over the evolution of epochs, whereas the Image model is lascking.

We can conclude that the neighborhood information brings in useful performance boost, when comparing both the Mixed and OSM models with the Image model. We also encountered the OSM model outperforming the other two more complicated models, only being caught up to by the Mixed model in cases where we used smart dataset enhancing techniques. It should be noted that the OSM model is more compact in terms of number of parameters of the NN as well as being more time efficient for evaluation.

We have taken more care to tweak the performance of this OSM model design in \ref[ch7_model_osm_wd].

\label[ch7_model_osm_wd]
\secc Analysis of OSM model shape
%------------------

In section \ref[ch6_dataset_augmentation] we experimented with various settings used when building OSM model. In Figure \ref[39_osm_wd_4x4] we generated a grid comparing the performances of models with variable depth on the \textit{x} axis and variable width on the \textit{y} axis. Width influences the number of neurons each layer has and the depth influences the number of layers. Variation between the results of models with different setting is not very pronounced - even the most basic model with \textit{depth = 1} and \textit{width = 32} is viable. In general we can see the models with \textit{depth} set at lower extreme value \textit{1} to be somewhat lacking behind more balanced models. Similarly on the other end of graph models with \textit{depth} set to \textit{4} seem to have slightly worse results than the simpler models.

Figure \ref[40_osm_wd_d2cut] allows us to compare amongst each other the models with fixed depth and variable width. Simple models in terms of width can achieve better results throughout the whole training process. Furthermore the smaller the model is in terms of its trainable parameter size, the faster it is taught.

Note that the best performing model can be considered in various terms. When we look at the average best achieved validation error of the 16 models like in \ref[39_osm_wd_4x4], where average of best results of 10 runs was taken, then model which was built with \textit{depth = 2} and \textit{width = 128} ranks first with mean:  mean 0.076 +/- 0.012. See Table \ref[41a_table_avg_best]. If we instead consider the minimal value of average validation error like in \ref[40_osm_wd_d2cut], then the model with \textit{depth = 2} and \textit{width = 64} ranks first with value: 0.08. See Table \ref[41b_table_min_avg].

\midinsert
\line{\hsize=.5\hsize \vtop{%
      \clabel[41a_table_avg_best]{Model comparison by best error}
      \cinspic tables/6_avg_best_table.pdf
      \captiontablehack/f Model comparison by average best validation error (average over minimal validation error of each fold). Model: \textit{OSM}, Dataset: \textit{5556x\_markable\_640x640}.
   \par}\vtop{%
      \clabel[41b_table_min_avg]{Model comparison by average error}
      \cinspic tables/6_min_avg_table.pdf
      \captiontablehack/f Model comparison by minimal average validation error (minimal of average validation errors over all folds). Model: \textit{OSM}, Dataset: \textit{5556x\_markable\_640x640}.
   \par}}
\endinsert

\label[ch8]
\chap Conclusion
%==============================================================

We have successfully implemented Convolutional Neural Network model for the task of route attractivity score estimation. We have enhanced the initial dataset of scored edges and nodes with downloaded Google Street View images and generated neighborhood vectors aggregated from Open Street Maps. We have experimented with various CNN model designs as well as with methods of generating and augmenting imagery dataset. To overcome the limitations of our dataset size, we followed the method of feature transfer, reusing feature extractor section of complex CNN models while adapting them to our own task with custom classifier sections.

We proposed three CNN model designs according to the type of data they use - Image model using imagery information, OSM model using neighborhood vector representation and Mixed model using both. We have made the model descriptions customizable and experimented with changing parameters influencing their shape, width and depth. We have tested various setting combinations extensively while comparing their results with k-fold cross-validation. We report the best model achieving mean squared error measurement of 0.076 +/- 0.012.

\endinput
%%
%% End of file `ch00.tex'.
